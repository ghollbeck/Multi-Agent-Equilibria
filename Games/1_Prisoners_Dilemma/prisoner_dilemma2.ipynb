{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized LLM-driven Decision Making for Iterated Prisoner's Dilemma\n",
    "\n",
    "\n",
    "\n",
    "This notebook implements an LLM-driven simulation of the Iterated Prisoner's Dilemma using OpenAI's API. \n",
    "\n",
    "The simulation features:\n",
    "\n",
    "- Dynamic strategy generation using GPT-4\n",
    "\n",
    "- Evolutionary agent selection\n",
    "\n",
    "- Asynchronous execution for improved performance\n",
    "\n",
    "- Detailed logging and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, we'll import the necessary libraries and set up our OpenAI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% \n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "from typing import List, Tuple, Dict\n",
    "import aiohttp\n",
    "import sys\n",
    "\n",
    "# Load environment variables and setup OpenAI client\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "async_client = AsyncOpenAI(api_key=api_key)\n",
    "\n",
    "# =============================================================================\n",
    "# Comprehensive List of Prisoner's Dilemma Tactics with Explanations\n",
    "# =============================================================================\n",
    "PD_STRATEGIES = {\n",
    "    \"always_cooperate\": \"Always Cooperate: The agent always chooses to cooperate, regardless of the opponent's actions.\",\n",
    "    \"always_defect\": \"Always Defect: The agent always chooses to defect, regardless of the opponent's actions.\",\n",
    "    \"tit_for_tat\": \"Tit-for-Tat: The agent starts by cooperating, then mirrors the opponent's last move.\",\n",
    "    \"grim_trigger\": \"Grim Trigger: The agent cooperates until the opponent defects once, then always defects thereafter.\",\n",
    "    \"win_stay_lose_shift\": \"Win-Stay, Lose-Shift (Pavlov): The agent repeats its previous move if it resulted in a favorable payoff; otherwise, it switches.\",\n",
    "    \"generous_tit_for_tat\": \"Generous Tit-for-Tat: Similar to Tit-for-Tat, but occasionally forgives a defection (influenced by the agent's cooperation bias) to encourage cooperation.\",\n",
    "    \"suspicious_tit_for_tat\": \"Suspicious Tit-for-Tat: Starts with defection, then mirrors the opponent's last move, expecting defection from the opponent.\",\n",
    "    \"random\": \"Random: The agent randomly selects between cooperation and defection.\",\n",
    "    \"contrite_tit_for_tat\": \"Contrite Tit-for-Tat: Like Tit-for-Tat, but if the agent inadvertently defects, it 'apologizes' by cooperating in subsequent rounds.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Implementation\n",
    "\n",
    "The EnhancedAgent class represents a player in the Prisoner's Dilemma game.\n",
    "\n",
    "Each agent:\n",
    "\n",
    "- Has a unique strategy matrix generated by GPT-4\n",
    "\n",
    "- Maintains a history of interactions\n",
    "\n",
    "- Makes decisions based on past interactions and current game state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% \n",
    "class EnhancedAgent:\n",
    "    def __init__(self, name, model=\"gpt-4-turbo\", \n",
    "                 strategy_tactic=\"tit_for_tat\", \n",
    "                 cooperation_bias=0.5,      # Bias toward cooperation (0 to 1)\n",
    "                 risk_aversion=0.5,         # Tendency to avoid risky moves (0 to 1)\n",
    "                 game_theoretic_prior=None  # Additional prior parameters as a dict\n",
    "                ):\n",
    "        self.name = name\n",
    "        self.model = model  # Track model architecture\n",
    "        self.strategy_tactic = strategy_tactic  # Explicit tactic name (should be a key in PD_STRATEGIES)\n",
    "        self.cooperation_bias = cooperation_bias\n",
    "        self.risk_aversion = risk_aversion\n",
    "        self.game_theoretic_prior = game_theoretic_prior if game_theoretic_prior is not None else {}\n",
    "        \n",
    "        self.total_score = 0\n",
    "        self.history = []  # Each entry: (opponent_name, own_action, opp_action, payoff)\n",
    "        self.strategy_matrix = None\n",
    "        self.strategy_evolution = []  # Track strategy changes over generations\n",
    "        self.cooperation_rate = 0.0\n",
    "        self.reciprocity_index = 0.0  # Measure tit-for-tat behavior\n",
    "\n",
    "    async def initialize(self):\n",
    "        \"\"\"Asynchronously initialize the agent's strategy matrix.\"\"\"\n",
    "        self.strategy_matrix = await self.generate_strategy_matrix()\n",
    "        return self\n",
    "\n",
    "    async def generate_strategy_matrix(self):\n",
    "        prompt = \"\"\"System: You are developing a novel strategy for the Iterated Prisoner's Dilemma. \n",
    "Create a unique approach that considers:\n",
    "- Long-term relationship building\n",
    "- Error correction mechanisms\n",
    "- Adaptive response patterns\n",
    "- Potential for both cooperation and defection\n",
    "\n",
    "Format: JSON structure with:\n",
    "{\n",
    "    \"strategy_rules\": [list of conditional statements],\n",
    "    \"forgiveness_factor\": 0-1,\n",
    "    \"retaliation_threshold\": 0-1,\n",
    "    \"adaptability\": 0-1,\n",
    "    \"rationale\": \"str\"\n",
    "}\"\"\"\n",
    "        \n",
    "        for _ in range(3):  # Retry up to 3 times\n",
    "            try:\n",
    "                response = await async_client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[{\"role\": \"system\", \"content\": \"You are a game theory expert creating novel IPD strategies. Respond ONLY with valid JSON.\"},\n",
    "                              {\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0.8,\n",
    "                    response_format={\"type\": \"json_object\"},\n",
    "                    max_tokens=300\n",
    "                )\n",
    "                json_str = response.choices[0].message.content.strip()\n",
    "                # Basic JSON validation\n",
    "                if not json_str.startswith('{') or not json_str.endswith('}'):\n",
    "                    raise json.JSONDecodeError(\"Missing braces\", json_str, 0)\n",
    "                strategy = json.loads(json_str)\n",
    "                # Validate required fields\n",
    "                if all(k in strategy for k in [\"strategy_rules\", \"forgiveness_factor\", \n",
    "                                                 \"retaliation_threshold\", \"adaptability\"]):\n",
    "                    return strategy\n",
    "            except (json.JSONDecodeError, KeyError) as e:\n",
    "                print(f\"Retrying strategy generation due to error: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Fallback strategy if all retries fail\n",
    "        return {\n",
    "            \"strategy_rules\": [\"CC: C\", \"CD: D\", \"DC: D\", \"DD: C\"],\n",
    "            \"forgiveness_factor\": 0.5,\n",
    "            \"retaliation_threshold\": 0.5,\n",
    "            \"adaptability\": 0.5,\n",
    "            \"rationale\": \"Default fallback strategy\"\n",
    "        }\n",
    "    \n",
    "    def decide_action_explicit(self, opponent) -> Dict:\n",
    "        \"\"\"\n",
    "        Implements an explicit decision based on the chosen strategy tactic.\n",
    "        Returns a dict with keys: action, confidence, rationale, expected_opponent_action, risk_assessment.\n",
    "        \"\"\"\n",
    "        # For explicit strategies, use deterministic rules:\n",
    "        if self.strategy_tactic == \"always_cooperate\":\n",
    "            return {\"action\": \"C\", \"confidence\": 1.0, \"rationale\": \"Always Cooperate strategy\", \"expected_opponent_action\": \"C\", \"risk_assessment\": \"Low\"}\n",
    "        \n",
    "        elif self.strategy_tactic == \"always_defect\":\n",
    "            return {\"action\": \"D\", \"confidence\": 1.0, \"rationale\": \"Always Defect strategy\", \"expected_opponent_action\": \"D\", \"risk_assessment\": \"Low\"}\n",
    "        \n",
    "        elif self.strategy_tactic == \"tit_for_tat\":\n",
    "            if self.history:\n",
    "                last_opponent_action = self.history[-1][2]  # Opponent's last move\n",
    "                return {\"action\": last_opponent_action, \"confidence\": 1.0, \"rationale\": \"Tit-for-Tat: mirroring opponent's last move\", \"expected_opponent_action\": last_opponent_action, \"risk_assessment\": \"Medium\"}\n",
    "            else:\n",
    "                return {\"action\": \"C\", \"confidence\": 1.0, \"rationale\": \"Tit-for-Tat: starting with cooperation\", \"expected_opponent_action\": \"C\", \"risk_assessment\": \"Low\"}\n",
    "        \n",
    "        elif self.strategy_tactic == \"grim_trigger\":\n",
    "            if any(interaction[2] == \"D\" for interaction in self.history):\n",
    "                return {\"action\": \"D\", \"confidence\": 1.0, \"rationale\": \"Grim Trigger: defecting after observed defection\", \"expected_opponent_action\": \"D\", \"risk_assessment\": \"High\"}\n",
    "            else:\n",
    "                return {\"action\": \"C\", \"confidence\": 1.0, \"rationale\": \"Grim Trigger: continuing cooperation\", \"expected_opponent_action\": \"C\", \"risk_assessment\": \"Low\"}\n",
    "        \n",
    "        elif self.strategy_tactic == \"win_stay_lose_shift\":\n",
    "            if self.history:\n",
    "                last_payoff = self.history[-1][3]\n",
    "                last_move = self.history[-1][1]\n",
    "                # Assuming a favorable payoff is >= 3 (from the standard payoff matrix)\n",
    "                if last_payoff >= 3:\n",
    "                    return {\"action\": last_move, \"confidence\": 1.0, \"rationale\": \"Win-Stay, Lose-Shift: repeating successful move\", \"expected_opponent_action\": \"C\", \"risk_assessment\": \"Low\"}\n",
    "                else:\n",
    "                    new_move = \"D\" if last_move == \"C\" else \"C\"\n",
    "                    return {\"action\": new_move, \"confidence\": 1.0, \"rationale\": \"Win-Stay, Lose-Shift: switching due to low payoff\", \"expected_opponent_action\": \"C\", \"risk_assessment\": \"Medium\"}\n",
    "            else:\n",
    "                return {\"action\": \"C\", \"confidence\": 1.0, \"rationale\": \"Win-Stay, Lose-Shift: default cooperation\", \"expected_opponent_action\": \"C\", \"risk_assessment\": \"Low\"}\n",
    "        \n",
    "        elif self.strategy_tactic == \"generous_tit_for_tat\":\n",
    "            if self.history:\n",
    "                last_opponent_action = self.history[-1][2]\n",
    "                # Forgiveness based on cooperation_bias probability\n",
    "                if last_opponent_action == \"D\" and random.random() < self.cooperation_bias:\n",
    "                    return {\"action\": \"C\", \"confidence\": 1.0, \"rationale\": \"Generous Tit-for-Tat: forgiving defection\", \"expected_opponent_action\": \"C\", \"risk_assessment\": \"Medium\"}\n",
    "                else:\n",
    "                    return {\"action\": last_opponent_action, \"confidence\": 1.0, \"rationale\": \"Generous Tit-for-Tat: mirroring last move\", \"expected_opponent_action\": last_opponent_action, \"risk_assessment\": \"Medium\"}\n",
    "            else:\n",
    "                return {\"action\": \"C\", \"confidence\": 1.0, \"rationale\": \"Generous Tit-for-Tat: default cooperation\", \"expected_opponent_action\": \"C\", \"risk_assessment\": \"Low\"}\n",
    "        \n",
    "        elif self.strategy_tactic == \"suspicious_tit_for_tat\":\n",
    "            if self.history:\n",
    "                last_opponent_action = self.history[-1][2]\n",
    "                return {\"action\": last_opponent_action, \"confidence\": 1.0, \"rationale\": \"Suspicious Tit-for-Tat: mirroring opponent's move after initial defection\", \"expected_opponent_action\": last_opponent_action, \"risk_assessment\": \"High\"}\n",
    "            else:\n",
    "                return {\"action\": \"D\", \"confidence\": 1.0, \"rationale\": \"Suspicious Tit-for-Tat: starting with defection\", \"expected_opponent_action\": \"D\", \"risk_assessment\": \"High\"}\n",
    "        \n",
    "        elif self.strategy_tactic == \"random\":\n",
    "            action = random.choice([\"C\", \"D\"])\n",
    "            return {\"action\": action, \"confidence\": 1.0, \"rationale\": \"Random: choosing action at random\", \"expected_opponent_action\": \"C\", \"risk_assessment\": \"Variable\"}\n",
    "        \n",
    "        elif self.strategy_tactic == \"contrite_tit_for_tat\":\n",
    "            if self.history:\n",
    "                last_self_move = self.history[-1][1]\n",
    "                last_opponent_move = self.history[-1][2]\n",
    "                if last_opponent_move == \"D\" and last_self_move == \"D\":\n",
    "                    return {\"action\": \"C\", \"confidence\": 1.0, \"rationale\": \"Contrite Tit-for-Tat: apologizing for unintended defection\", \"expected_opponent_action\": \"C\", \"risk_assessment\": \"Medium\"}\n",
    "                else:\n",
    "                    return {\"action\": last_opponent_move, \"confidence\": 1.0, \"rationale\": \"Contrite Tit-for-Tat: mirroring opponent's last move\", \"expected_opponent_action\": last_opponent_move, \"risk_assessment\": \"Medium\"}\n",
    "            else:\n",
    "                return {\"action\": \"C\", \"confidence\": 1.0, \"rationale\": \"Contrite Tit-for-Tat: default cooperation\", \"expected_opponent_action\": \"C\", \"risk_assessment\": \"Low\"}\n",
    "        \n",
    "        else:\n",
    "            # If an unknown tactic is specified, return None to fallback to LLM decision.\n",
    "            return None\n",
    "\n",
    "    async def decide_action(self, opponent):\n",
    "        \"\"\"\n",
    "        Determine an action using an explicit tactic if available.\n",
    "        Otherwise, fallback to the LLM-based decision approach.\n",
    "        \"\"\"\n",
    "        # First, attempt to use the explicit strategy.\n",
    "        explicit_decision = self.decide_action_explicit(opponent)\n",
    "        if explicit_decision is not None:\n",
    "            return explicit_decision\n",
    "\n",
    "        # If no explicit tactic is matched, fallback to LLM-driven decision.\n",
    "        analysis_prompt = f\"\"\"Analyze this Prisoner's Dilemma interaction history with {opponent.name}:\n",
    "Previous Rounds: {str(self.history[-3:]) if len(self.history) > 0 else 'None'}\n",
    "\n",
    "Your Strategy: {json.dumps(self.strategy_matrix)}\n",
    "Opponent's Model: {opponent.model}\n",
    "Opponent's Cooperation Rate: {opponent.cooperation_rate:.2f}\n",
    "\n",
    "Output MUST be valid JSON with:\n",
    "{{\n",
    "    \"action\": \"C/D\",\n",
    "    \"confidence\": 0-1,\n",
    "    \"rationale\": \"str\",\n",
    "    \"expected_opponent_action\": \"C/D\",\n",
    "    \"risk_assessment\": \"str\"\n",
    "}}\"\"\"\n",
    "        \n",
    "        for _ in range(3):  # Retry up to 3 times\n",
    "            try:\n",
    "                response = await async_client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[{\"role\": \"system\", \"content\": \"You are an AI game theorist. Respond ONLY with valid JSON.\"},\n",
    "                              {\"role\": \"user\", \"content\": analysis_prompt}],\n",
    "                    temperature=0.4,\n",
    "                    response_format={\"type\": \"json_object\"},\n",
    "                    max_tokens=150\n",
    "                )\n",
    "                json_str = response.choices[0].message.content.strip()\n",
    "                decision = json.loads(json_str)\n",
    "                action = decision.get(\"action\", \"C\").upper()\n",
    "                if action not in [\"C\", \"D\"]:\n",
    "                    action = random.choice([\"C\", \"D\"])\n",
    "                return decision\n",
    "            except (json.JSONDecodeError, KeyError) as e:\n",
    "                print(f\"Retrying decision due to error: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Fallback to random choice if all retries fail\n",
    "        return {\n",
    "            \"action\": random.choice([\"C\", \"D\"]),\n",
    "            \"confidence\": 0.5,\n",
    "            \"rationale\": \"Fallback decision\",\n",
    "            \"expected_opponent_action\": \"C\",\n",
    "            \"risk_assessment\": \"Unknown\"\n",
    "        }\n",
    "\n",
    "    def log_interaction(self, opponent, own_action, opp_action, payoff):\n",
    "        self.history.append((opponent, own_action, opp_action, payoff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Configuration\n",
    "\n",
    "Define the payoff matrix for the Prisoner's Dilemma and helper functions for agent creation and interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% \n",
    "# Define the standard Prisoner's Dilemma payoff matrix.\n",
    "payoff_matrix = {\n",
    "    ('C', 'C'): (3, 3),  # Both cooperate\n",
    "    ('C', 'D'): (0, 5),  # Player 1 cooperates, Player 2 defects\n",
    "    ('D', 'C'): (5, 0),  # Player 1 defects, Player 2 cooperates\n",
    "    ('D', 'D'): (1, 1),  # Both defect\n",
    "}\n",
    "\n",
    "async def create_enhanced_agents(n=4) -> List[EnhancedAgent]:\n",
    "    \"\"\"Create and initialize multiple agents concurrently.\"\"\"\n",
    "    # For demonstration, agents can be instantiated with different explicit tactics.\n",
    "    tactics = list(PD_STRATEGIES.keys())\n",
    "    agents = [EnhancedAgent(f\"Agent_{i}\",\n",
    "                            strategy_tactic=random.choice(tactics),\n",
    "                            cooperation_bias=random.uniform(0.3, 0.7),\n",
    "                            risk_aversion=random.uniform(0.3, 0.7))\n",
    "              for i in range(n)]\n",
    "    # Initialize all agents concurrently\n",
    "    agents = await asyncio.gather(*(agent.initialize() for agent in agents))\n",
    "    return agents\n",
    "\n",
    "async def simulate_interaction(agent_a: EnhancedAgent, agent_b: EnhancedAgent) -> Dict:\n",
    "    \"\"\"Simulate an interaction between two agents asynchronously.\"\"\"\n",
    "    # Get decisions concurrently\n",
    "    decision_a, decision_b = await asyncio.gather(\n",
    "        agent_a.decide_action(agent_b),\n",
    "        agent_b.decide_action(agent_a)\n",
    "    )\n",
    "    \n",
    "    # Extract and normalize actions\n",
    "    def normalize_action(decision):\n",
    "        action = str(decision.get(\"action\", \"C\")).upper()\n",
    "        return \"C\" if action == \"C\" else \"D\"\n",
    "    \n",
    "    action_a = normalize_action(decision_a)\n",
    "    action_b = normalize_action(decision_b)\n",
    "    \n",
    "    payoff_a, payoff_b = payoff_matrix[(action_a, action_b)]\n",
    "    agent_a.total_score += payoff_a\n",
    "    agent_b.total_score += payoff_b\n",
    "    agent_a.log_interaction(agent_b.name, action_a, action_b, payoff_a)\n",
    "    agent_b.log_interaction(agent_a.name, action_b, action_a, payoff_b)\n",
    "    \n",
    "    return {\n",
    "        \"Pair\": f\"{agent_a.name}-{agent_b.name}\",\n",
    "        \"Actions\": f\"{action_a}-{action_b}\",\n",
    "        \"Payoffs\": f\"{payoff_a}-{payoff_b}\",\n",
    "        \"Strategy_A\": agent_a.strategy_matrix,\n",
    "        \"Strategy_B\": agent_b.strategy_matrix,\n",
    "        \"Reasoning_A\": decision_a.get(\"rationale\", \"\"),\n",
    "        \"Reasoning_B\": decision_b.get(\"rationale\", \"\"),\n",
    "        \"Score_A\": agent_a.total_score,\n",
    "        \"Score_B\": agent_b.total_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Simulation\n",
    "\n",
    "The main simulation function runs multiple generations of agents, with each generation involving:\n",
    "\n",
    "1. Concurrent agent interactions\n",
    "\n",
    "2. Logging of results\n",
    "\n",
    "3. Evolution (selection of top performers)\n",
    "\n",
    "4. Creation of new agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% \n",
    "async def run_llm_driven_simulation(num_agents=4, num_generations=5, models=[\"gpt-4-turbo\"]):\n",
    "    # Determine the base path for results\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        # Running in a Jupyter notebook\n",
    "        base_path = os.getcwd()\n",
    "    else:\n",
    "        # Running as a script\n",
    "        base_path = os.path.dirname(__file__)\n",
    "    \n",
    "    # Create timestamped results folder\n",
    "    results_folder = os.path.join(base_path, \"simulation_results\")\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    run_folder = os.path.join(results_folder, f\"run_{current_time}\")\n",
    "    os.makedirs(run_folder, exist_ok=True)\n",
    "    \n",
    "    agents = await create_enhanced_agents(num_agents)\n",
    "    all_detailed_logs = []\n",
    "    generation_summary = []\n",
    "    \n",
    "    for gen in range(num_generations):\n",
    "        print(f\"\\n=== Generation {gen+1} ===\")\n",
    "        detailed_logs = []\n",
    "        random.shuffle(agents)\n",
    "    \n",
    "        # Pair agents and simulate interactions concurrently\n",
    "        interaction_tasks = []\n",
    "        for i in range(0, len(agents), 2):\n",
    "            if i + 1 < len(agents):\n",
    "                interaction_tasks.append(simulate_interaction(agents[i], agents[i+1]))\n",
    "        \n",
    "        # Wait for all interactions to complete\n",
    "        interaction_results = await asyncio.gather(*interaction_tasks)\n",
    "        \n",
    "        # Process results\n",
    "        gen_metrics = {\n",
    "            \"mutual_cooperation\": 0,\n",
    "            \"mutual_defection\": 0,\n",
    "            \"temptation_payoffs\": 0,\n",
    "            \"sucker_payoffs\": 0,\n",
    "            \"total_payoffs\": 0\n",
    "        }\n",
    "        \n",
    "        for result in interaction_results:\n",
    "            detailed_logs.append({\n",
    "                \"Generation\": gen+1,\n",
    "                **result\n",
    "            })\n",
    "            print(f\"{result['Pair']}: {result['Actions']}, Payoffs: {result['Payoffs']}\")\n",
    "            \n",
    "            actions = result['Actions'].split('-')\n",
    "            payoffs = [int(p) for p in result['Payoffs'].split('-')]\n",
    "            gen_metrics[\"total_payoffs\"] += sum(payoffs)\n",
    "            \n",
    "            if actions == ['C', 'C']:\n",
    "                gen_metrics[\"mutual_cooperation\"] += 1\n",
    "            elif actions == ['D', 'D']:\n",
    "                gen_metrics[\"mutual_defection\"] += 1\n",
    "            elif 'D' in actions and 'C' in actions:\n",
    "                if actions[0] == 'D': \n",
    "                    gen_metrics[\"temptation_payoffs\"] += 1\n",
    "                else: \n",
    "                    gen_metrics[\"sucker_payoffs\"] += 1\n",
    "        \n",
    "        all_detailed_logs.extend(detailed_logs)\n",
    "        \n",
    "        total_possible = 3 * len(interaction_results) * 2  # Max 3 points per player per interaction\n",
    "        pareto_eff = gen_metrics[\"total_payoffs\"] / total_possible if total_possible > 0 else 0\n",
    "        nash_dev = 1 - (gen_metrics[\"mutual_defection\"] / len(interaction_results)) if len(interaction_results) > 0 else 0\n",
    "        strat_diversity = len(set(hash(json.dumps(a.strategy_matrix)) for a in agents))\n",
    "        \n",
    "        avg_score = sum(a.total_score for a in agents) / len(agents) if len(agents) > 0 else 0\n",
    "        generation_summary.append({\n",
    "            \"Generation\": gen+1,\n",
    "            \"Average_Score\": avg_score,\n",
    "            \"Pareto_Efficiency\": pareto_eff,\n",
    "            \"Nash_Deviation\": nash_dev,\n",
    "            \"Strategy_Diversity\": strat_diversity,\n",
    "            **gen_metrics\n",
    "        })\n",
    "        \n",
    "        # Evolution: select top agents and generate new ones.\n",
    "        agents.sort(key=lambda a: a.total_score, reverse=True)\n",
    "        top_agents = agents[:num_agents // 2]\n",
    "        new_agents = await create_enhanced_agents(num_agents // 2)\n",
    "        agents = top_agents + new_agents\n",
    "        \n",
    "        # Reset scores for next generation\n",
    "        for agent in agents:\n",
    "            agent.total_score = 0\n",
    "    \n",
    "    # Save run parameters\n",
    "    params = {\n",
    "        \"num_agents\": num_agents,\n",
    "        \"num_generations\": num_generations,\n",
    "        \"payoff_matrix\": {\n",
    "            \"CC\": payoff_matrix[('C', 'C')],\n",
    "            \"CD\": payoff_matrix[('C', 'D')],\n",
    "            \"DC\": payoff_matrix[('D', 'C')],\n",
    "            \"DD\": payoff_matrix[('D', 'D')]\n",
    "        },\n",
    "        \"timestamp\": current_time,\n",
    "        \"models\": models\n",
    "    }\n",
    "    with open(os.path.join(run_folder, \"parameters.json\"), 'w') as f:\n",
    "        json.dump(params, f, indent=4)\n",
    "    \n",
    "    # Save detailed logs\n",
    "    detailed_df = pd.DataFrame(all_detailed_logs)\n",
    "    detailed_df.to_csv(os.path.join(run_folder, \"detailed_logs.csv\"), index=False)\n",
    "    detailed_df.to_json(os.path.join(run_folder, \"detailed_logs.json\"), orient=\"records\", indent=4)\n",
    "    \n",
    "    # Save generation summary\n",
    "    summary_df = pd.DataFrame(generation_summary)\n",
    "    summary_df.to_csv(os.path.join(run_folder, \"generation_summary.csv\"), index=False)\n",
    "    summary_df.to_json(os.path.join(run_folder, \"generation_summary.json\"), orient=\"records\", indent=4)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    generations = range(1, num_generations + 1)\n",
    "    avg_scores = [entry[\"Average_Score\"] for entry in generation_summary]\n",
    "    plt.plot(generations, avg_scores, marker='o', linestyle='-', linewidth=2)\n",
    "    plt.title(\"Average Cooperation Score over Generations\")\n",
    "    plt.xlabel(\"Generation\")\n",
    "    plt.ylabel(\"Average Score\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(run_folder, \"cooperation_over_generations.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    def create_research_visualizations():\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.subplot(2,2,1)\n",
    "        plt.plot([m[\"Strategy_Diversity\"] for m in generation_summary], marker='o')\n",
    "        plt.title(\"Strategy Diversity Over Generations\")\n",
    "        \n",
    "        plt.subplot(2,2,2)\n",
    "        plt.bar([\"Mutual C\", \"Mutual D\", \"Temptation\", \"Sucker\"], \n",
    "               [generation_summary[-1][\"mutual_cooperation\"],\n",
    "                generation_summary[-1][\"mutual_defection\"],\n",
    "                generation_summary[-1][\"temptation_payoffs\"],\n",
    "                generation_summary[-1][\"sucker_payoffs\"]])\n",
    "        plt.title(\"Final Generation Outcome Distribution\")\n",
    "        \n",
    "        plt.subplot(2,2,3)\n",
    "        plt.plot([m[\"Pareto_Efficiency\"] for m in generation_summary])\n",
    "        plt.title(\"Pareto Efficiency Progress\")\n",
    "        \n",
    "        plt.subplot(2,2,4)\n",
    "        plt.scatter([m[\"Nash_Deviation\"] for m in generation_summary],\n",
    "                    [m[\"Average_Score\"] for m in generation_summary])\n",
    "        plt.title(\"Nash Deviation vs Average Score\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(run_folder, \"research_metrics.png\"))\n",
    "    \n",
    "    create_research_visualizations()\n",
    "    \n",
    "    print(f\"\\nSimulation completed. Results saved in: {run_folder}\")\n",
    "    return generation_summary, all_detailed_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Simulation\n",
    "\n",
    "Execute the simulation with specified parameters.\n",
    "\n",
    "Note: This will make multiple API calls to OpenAI's GPT-4, so ensure your API key is set up correctly.\n",
    "\n",
    "\n",
    "\n",
    "To run in a Jupyter notebook, first install nest_asyncio:\n",
    "\n",
    "```bash\n",
    "\n",
    "pip install nest_asyncio\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Then run the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Helper function to run async code in Jupyter\n",
    "async def run_simulation():\n",
    "    return await run_llm_driven_simulation(num_agents=10, num_generations=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generation 1 ===\n",
      "Agent_5-Agent_6: D-D, Payoffs: 1-1\n",
      "Agent_1-Agent_4: C-C, Payoffs: 3-3\n",
      "Agent_7-Agent_8: C-C, Payoffs: 3-3\n",
      "Agent_9-Agent_3: D-C, Payoffs: 5-0\n",
      "Agent_0-Agent_2: D-C, Payoffs: 5-0\n",
      "\n",
      "=== Generation 2 ===\n",
      "Agent_0-Agent_3: D-C, Payoffs: 5-0\n",
      "Agent_0-Agent_9: C-C, Payoffs: 3-3\n",
      "Agent_1-Agent_4: C-C, Payoffs: 3-3\n",
      "Agent_2-Agent_1: C-C, Payoffs: 3-3\n",
      "Agent_7-Agent_4: C-C, Payoffs: 3-3\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "# Setup for Jupyter notebook execution\n",
    "if not os.getenv('JUPYTER_RUNNING_IN_SCRIPT'):\n",
    "    try:\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "        import asyncio\n",
    "        # Create event loop and run simulation\n",
    "        loop = asyncio.get_event_loop()\n",
    "        summary, logs = loop.run_until_complete(run_simulation())\n",
    "    except ImportError:\n",
    "        print(\"Please install nest_asyncio: pip install nest_asyncio\")\n",
    "else:\n",
    "    # For running as a script\n",
    "    summary, logs = asyncio.run(run_llm_driven_simulation(num_agents=4, num_generations=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Visualization\n",
    "\n",
    "After running the simulation, you can analyze the results using the returned data:\n",
    "\n",
    "- `summary`: Contains generation-level statistics\n",
    "\n",
    "- `logs`: Contains detailed interaction logs\n",
    "\n",
    "\n",
    "\n",
    "Example analysis:\n",
    "\n",
    "```python\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Convert to DataFrames for analysis\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "\n",
    "logs_df = pd.DataFrame(logs)\n",
    "\n",
    "\n",
    "\n",
    "# Analyze cooperation rates\n",
    "\n",
    "cooperation_rates = logs_df['Actions'].apply(lambda x: x.count('C') / len(x))\n",
    "\n",
    "print(f\"Average cooperation rate: {cooperation_rates.mean():.2%}\")\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
