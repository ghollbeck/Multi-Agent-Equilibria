{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized LLM-driven Decision Making for Iterated Prisoner's Dilemma\n",
    "\n",
    "\n",
    "\n",
    "This notebook implements an LLM-driven simulation of the Iterated Prisoner's Dilemma using OpenAI's API. \n",
    "\n",
    "The simulation features:\n",
    "\n",
    "- Dynamic strategy generation using GPT-4\n",
    "\n",
    "- Evolutionary agent selection\n",
    "\n",
    "- Asynchronous execution for improved performance\n",
    "\n",
    "- Detailed logging and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, we'll import the necessary libraries and set up our OpenAI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% \n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "from typing import List, Tuple, Dict\n",
    "import aiohttp\n",
    "import sys\n",
    "from dataclasses import dataclass, field, asdict\n",
    "\n",
    "# Load environment variables and setup OpenAI client\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "async_client = AsyncOpenAI(api_key=api_key)\n",
    "\n",
    "# =============================================================================\n",
    "# Comprehensive List of Iterated Prisoner's Dilemma (IPD) Strategies\n",
    "# (Arranged from Most Social/Collaborative to Most Antisocial)\n",
    "# =============================================================================\n",
    "# Each tuple contains (strategy_key, description)\n",
    "PD_STRATEGIES_SORTED = [\n",
    "    (\"generous_tit_for_tat\", \"Generous Tit-for-Tat: Highly cooperative and forgiving, it promotes long-term mutual cooperation by occasionally overlooking defections.\", 0.85),\n",
    "    (\"tit_for_tat\", \"Tit-for-Tat: Begins with cooperation and reciprocates the opponent's previous move, fostering reciprocal behavior.\", 0.70),\n",
    "    (\"win_stay_lose_shift\", \"Win-Stay, Lose-Shift (Pavlov): Repeats a move if it yielded a favorable payoff, adapting quickly to both cooperation and defection.\", 0.65),\n",
    "    (\"contrite_tit_for_tat\", \"Contrite Tit-for-Tat: Similar to Tit-for-Tat but offers forgiveness for accidental defections, thereby restoring cooperation.\", 0.75),\n",
    "    (\"always_cooperate\", \"Always Cooperate: Consistently cooperates regardless of the opponent's actionsâ€”very social but vulnerable to exploitation.\", 1.0),\n",
    "    (\"grim_trigger\", \"Grim Trigger: Cooperates until the first defection, then defects forever, enforcing strict punishment against betrayal.\", 0.40),\n",
    "    (\"suspicious_tit_for_tat\", \"Suspicious Tit-for-Tat: Starts with defection to test the opponent before potentially cooperating, less immediately cooperative.\", 0.35),\n",
    "    (\"always_defect\", \"Always Defect: Consistently defects, maximizing short-term gain at the expense of long-term cooperation.\", 0.0),\n",
    "    (\"random\", \"Random: Chooses actions unpredictably, lacking a consistent social or antisocial pattern.\", 0.50)\n",
    "]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Data Classes for Structured Logging\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class InteractionData:\n",
    "    generation: int\n",
    "    pair: str\n",
    "    round_actions: str\n",
    "    payoffs: str\n",
    "    reasoning_A: str\n",
    "    reasoning_B: str\n",
    "    score_A: int\n",
    "    score_B: int\n",
    "\n",
    "@dataclass\n",
    "class SimulationData:\n",
    "    hyperparameters: dict\n",
    "    interactions: List[InteractionData] = field(default_factory=list)\n",
    "\n",
    "    def add_interaction(self, interaction: InteractionData):\n",
    "        self.interactions.append(interaction)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'hyperparameters': self.hyperparameters,\n",
    "            'interactions': [asdict(inter) for inter in self.interactions]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Implementation\n",
    "\n",
    "The EnhancedAgent class represents a player in the Prisoner's Dilemma game.\n",
    "\n",
    "Each agent:\n",
    "\n",
    "- Has a unique strategy matrix generated by GPT-4\n",
    "\n",
    "- Maintains a history of interactions\n",
    "\n",
    "- Makes decisions based on past interactions and current game state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% \n",
    "class EnhancedAgent:\n",
    "    def __init__(self, name, model=\"gpt-4-turbo\", \n",
    "                 strategy_tactic=\"tit_for_tat\", \n",
    "                 cooperation_bias=0.5,      # Bias toward cooperation (0 to 1)\n",
    "                 risk_aversion=0.5,         # Tendency to avoid risky moves (0 to 1)\n",
    "                 game_theoretic_prior=None  # Additional prior parameters as a dict\n",
    "                ):\n",
    "        self.name = name\n",
    "        self.model = model  # Track model architecture\n",
    "        self.strategy_tactic = strategy_tactic  # Must be one of the keys in PD_STRATEGIES\n",
    "        self.cooperation_bias = cooperation_bias\n",
    "        self.risk_aversion = risk_aversion\n",
    "        self.game_theoretic_prior = game_theoretic_prior if game_theoretic_prior is not None else {}\n",
    "        \n",
    "        self.total_score = 0\n",
    "        self.history = []  # Each entry: (opponent_name, own_action, opp_action, payoff)\n",
    "        self.strategy_matrix = None\n",
    "        self.strategy_evolution = []  # Track strategy changes over generations\n",
    "        self.cooperation_rate = 0.0\n",
    "        self.reciprocity_index = 0.0  # Measure tit-for-tat behavior\n",
    "\n",
    "    async def initialize(self):\n",
    "        \"\"\"Asynchronously initialize the agent's strategy matrix.\"\"\"\n",
    "        self.strategy_matrix = await self.generate_strategy_matrix()\n",
    "        return self\n",
    "\n",
    "    async def generate_strategy_matrix(self):\n",
    "        prompt = \"\"\"System: You are developing a novel strategy for the Iterated Prisoner's Dilemma. \n",
    "Create a unique approach that considers:\n",
    "- Long-term relationship building\n",
    "- Error correction mechanisms\n",
    "- Adaptive response patterns\n",
    "- Potential for both cooperation and defection\n",
    "\n",
    "Format: JSON structure with:\n",
    "{\n",
    "    \"strategy_rules\": [list of conditional statements],\n",
    "    \"forgiveness_factor\": 0-1,\n",
    "    \"retaliation_threshold\": 0-1,\n",
    "    \"adaptability\": 0-1,\n",
    "    \"rationale\": \"str\"\n",
    "}\"\"\"\n",
    "        \n",
    "        for _ in range(3):  # Retry up to 3 times\n",
    "            try:\n",
    "                response = await async_client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[{\"role\": \"system\", \"content\": \"You are a game theory expert creating novel IPD strategies. Respond ONLY with valid JSON.\"},\n",
    "                              {\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0.8,\n",
    "                    response_format={\"type\": \"json_object\"},\n",
    "                    max_tokens=300\n",
    "                )\n",
    "                json_str = response.choices[0].message.content.strip()\n",
    "                if not json_str.startswith('{') or not json_str.endswith('}'):\n",
    "                    raise json.JSONDecodeError(\"Missing braces\", json_str, 0)\n",
    "                strategy = json.loads(json_str)\n",
    "                if all(k in strategy for k in [\"strategy_rules\", \"forgiveness_factor\", \"retaliation_threshold\", \"adaptability\"]):\n",
    "                    return strategy\n",
    "            except (json.JSONDecodeError, KeyError) as e:\n",
    "                print(f\"Retrying strategy generation due to error: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return {\n",
    "            \"strategy_rules\": [\"CC: C\", \"CD: D\", \"DC: D\", \"DD: C\"],\n",
    "            \"forgiveness_factor\": 0.5,\n",
    "            \"retaliation_threshold\": 0.5,\n",
    "            \"adaptability\": 0.5,\n",
    "            \"rationale\": \"Default fallback strategy\"\n",
    "        }\n",
    "    \n",
    "    def decide_action_explicit(self, opponent) -> Dict:\n",
    "        \"\"\"\n",
    "        Implements an explicit decision based on the chosen strategy tactic.\n",
    "        Returns a dict with keys: action, confidence, rationale, expected_opponent_action, risk_assessment.\n",
    "        \"\"\"\n",
    "        if self.strategy_tactic == \"always_cooperate\":\n",
    "            return {\"action\": \"C\", \"confidence\": 1.0, \"rationale\": \"Always Cooperate strategy\", \"expected_opponent_action\": \"C\", \"risk_assessment\": \"Low\"}\n",
    "        elif self.strategy_tactic == \"always_defect\":\n",
    "            return {\"action\": \"D\", \"confidence\": 1.0, \"rationale\": \"Always Defect strategy\", \"expected_opponent_action\": \"D\", \"risk_assessment\": \"Low\"}\n",
    "        elif self.strategy_tactic == \"tit_for_tat\":\n",
    "            if self.history:\n",
    "                last_opponent_action = self.history[-1][2]\n",
    "                return {\"action\": last_opponent_action, \"confidence\": 1.0, \"rationale\": \"Tit-for-Tat: mirroring opponent's last move\", \"expected_opponent_action\": last_opponent_action, \"risk_assessment\": \"Medium\"}\n",
    "            else:\n",
    "                return {\"action\": \"C\", \"confidence\": 1.0, \"rationale\": \"Tit-for-Tat: starting with cooperation\", \"expected_opponent_action\": \"C\", \"risk_assessment\": \"Low\"}\n",
    "        elif self.strategy_tactic == \"grim_trigger\":\n",
    "            if any(interaction[2] == \"D\" for interaction in self.history):\n",
    "                return {\"action\": \"D\", \"confidence\": 1.0, \"rationale\": \"Grim Trigger: defecting after observed defection\", \"expected_opponent_action\": \"D\", \"risk_assessment\": \"High\"}\n",
    "            else:\n",
    "                return {\"action\": \"C\", \"confidence\": 1.0, \"rationale\": \"Grim Trigger: continuing cooperation\", \"expected_opponent_action\": \"C\", \"risk_assessment\": \"Low\"}\n",
    "        elif self.strategy_tactic == \"win_stay_lose_shift\":\n",
    "            if self.history:\n",
    "                last_payoff = self.history[-1][3]\n",
    "                last_move = self.history[-1][1]\n",
    "                if last_payoff >= 3:\n",
    "                    return {\"action\": last_move, \"confidence\": 1.0, \"rationale\": \"Win-Stay, Lose-Shift: repeating successful move\", \"expected_opponent_action\": \"C\", \"risk_assessment\": \"Low\"}\n",
    "                else:\n",
    "                    new_move = \"D\" if last_move == \"C\" else \"C\"\n",
    "                    return {\"action\": new_move, \"confidence\": 1.0, \"rationale\": \"Win-Stay, Lose-Shift: switching due to low payoff\", \"expected_opponent_action\": \"C\", \"risk_assessment\": \"Medium\"}\n",
    "            else:\n",
    "                return {\"action\": \"C\", \"confidence\": 1.0, \"rationale\": \"Win-Stay, Lose-Shift: default cooperation\", \"expected_opponent_action\": \"C\", \"risk_assessment\": \"Low\"}\n",
    "        elif self.strategy_tactic == \"generous_tit_for_tat\":\n",
    "            if self.history:\n",
    "                last_opponent_action = self.history[-1][2]\n",
    "                if last_opponent_action == \"D\" and random.random() < self.cooperation_bias:\n",
    "                    return {\"action\": \"C\", \"confidence\": 1.0, \"rationale\": \"Generous Tit-for-Tat: forgiving defection\", \"expected_opponent_action\": \"C\", \"risk_assessment\": \"Medium\"}\n",
    "                else:\n",
    "                    return {\"action\": last_opponent_action, \"confidence\": 1.0, \"rationale\": \"Generous Tit-for-Tat: mirroring last move\", \"expected_opponent_action\": last_opponent_action, \"risk_assessment\": \"Medium\"}\n",
    "            else:\n",
    "                return {\"action\": \"C\", \"confidence\": 1.0, \"rationale\": \"Generous Tit-for-Tat: default cooperation\", \"expected_opponent_action\": \"C\", \"risk_assessment\": \"Low\"}\n",
    "        elif self.strategy_tactic == \"suspicious_tit_for_tat\":\n",
    "            if self.history:\n",
    "                last_opponent_action = self.history[-1][2]\n",
    "                return {\"action\": last_opponent_action, \"confidence\": 1.0, \"rationale\": \"Suspicious Tit-for-Tat: mirroring opponent's move\", \"expected_opponent_action\": last_opponent_action, \"risk_assessment\": \"High\"}\n",
    "            else:\n",
    "                return {\"action\": \"D\", \"confidence\": 1.0, \"rationale\": \"Suspicious Tit-for-Tat: starting with defection\", \"expected_opponent_action\": \"D\", \"risk_assessment\": \"High\"}\n",
    "        elif self.strategy_tactic == \"contrite_tit_for_tat\":\n",
    "            if self.history:\n",
    "                last_self_move = self.history[-1][1]\n",
    "                last_opponent_move = self.history[-1][2]\n",
    "                if last_opponent_move == \"D\" and last_self_move == \"D\":\n",
    "                    return {\"action\": \"C\", \"confidence\": 1.0, \"rationale\": \"Contrite Tit-for-Tat: apologizing for unintended defection\", \"expected_opponent_action\": \"C\", \"risk_assessment\": \"Medium\"}\n",
    "                else:\n",
    "                    return {\"action\": last_opponent_move, \"confidence\": 1.0, \"rationale\": \"Contrite Tit-for-Tat: mirroring opponent's last move\", \"expected_opponent_action\": last_opponent_move, \"risk_assessment\": \"Medium\"}\n",
    "            else:\n",
    "                return {\"action\": \"C\", \"confidence\": 1.0, \"rationale\": \"Contrite Tit-for-Tat: default cooperation\", \"expected_opponent_action\": \"C\", \"risk_assessment\": \"Low\"}\n",
    "        elif self.strategy_tactic == \"always_defect\":\n",
    "            return {\"action\": \"D\", \"confidence\": 1.0, \"rationale\": \"Always Defect strategy\", \"expected_opponent_action\": \"D\", \"risk_assessment\": \"Low\"}\n",
    "        elif self.strategy_tactic == \"always_cooperate\":\n",
    "            return {\"action\": \"C\", \"confidence\": 1.0, \"rationale\": \"Always Cooperate strategy\", \"expected_opponent_action\": \"C\", \"risk_assessment\": \"Low\"}\n",
    "        elif self.strategy_tactic == \"random\":\n",
    "            action = random.choice([\"C\", \"D\"])\n",
    "            return {\"action\": action, \"confidence\": 1.0, \"rationale\": \"Random strategy: unpredictable decision\", \"expected_opponent_action\": \"C\", \"risk_assessment\": \"Variable\"}\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    async def decide_action(self, opponent):\n",
    "        \"\"\"\n",
    "        Determine an action using an explicit tactic if available.\n",
    "        Otherwise, fallback to the LLM-based decision approach.\n",
    "        Only the last three rounds are provided as context (partial visibility).\n",
    "        \"\"\"\n",
    "        explicit_decision = self.decide_action_explicit(opponent)\n",
    "        if explicit_decision is not None:\n",
    "            return explicit_decision\n",
    "\n",
    "        analysis_prompt = f\"\"\"Analyze this Prisoner's Dilemma interaction history with {opponent.name}:\n",
    "Previous Rounds (last 3): {str(self.history[-3:]) if len(self.history) > 0 else 'None'}\n",
    "\n",
    "Your Strategy: {json.dumps(self.strategy_matrix)}\n",
    "Opponent's Model: {opponent.model}\n",
    "Opponent's Cooperation Rate: {opponent.cooperation_rate:.2f}\n",
    "\n",
    "Output MUST be valid JSON with:\n",
    "{{\n",
    "    \"action\": \"C/D\",\n",
    "    \"confidence\": 0-1,\n",
    "    \"rationale\": \"str\",\n",
    "    \"expected_opponent_action\": \"C/D\",\n",
    "    \"risk_assessment\": \"str\"\n",
    "}}\"\"\"\n",
    "        \n",
    "        for _ in range(3):\n",
    "            try:\n",
    "                response = await async_client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[{\"role\": \"system\", \"content\": \"You are an AI game theorist. Respond ONLY with valid JSON.\"},\n",
    "                              {\"role\": \"user\", \"content\": analysis_prompt}],\n",
    "                    temperature=0.4,\n",
    "                    response_format={\"type\": \"json_object\"},\n",
    "                    max_tokens=150\n",
    "                )\n",
    "                json_str = response.choices[0].message.content.strip()\n",
    "                decision = json.loads(json_str)\n",
    "                action = decision.get(\"action\", \"C\").upper()\n",
    "                if action not in [\"C\", \"D\"]:\n",
    "                    action = random.choice([\"C\", \"D\"])\n",
    "                return decision\n",
    "            except (json.JSONDecodeError, KeyError) as e:\n",
    "                print(f\"Retrying decision due to error: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return {\n",
    "            \"action\": random.choice([\"C\", \"D\"]),\n",
    "            \"confidence\": 0.5,\n",
    "            \"rationale\": \"Fallback decision\",\n",
    "            \"expected_opponent_action\": \"C\",\n",
    "            \"risk_assessment\": \"Unknown\"\n",
    "        }\n",
    "\n",
    "    def log_interaction(self, opponent, own_action, opp_action, payoff):\n",
    "        self.history.append((opponent, own_action, opp_action, payoff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Configuration\n",
    "\n",
    "Define the payoff matrix for the Prisoner's Dilemma and helper functions for agent creation and interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% \n",
    "# Define the standard Prisoner's Dilemma payoff matrix.\n",
    "payoff_matrix = {\n",
    "    ('C', 'C'): (3, 3),  # Both cooperate\n",
    "    ('C', 'D'): (0, 5),  # Player 1 cooperates, Player 2 defects\n",
    "    ('D', 'C'): (5, 0),  # Player 1 defects, Player 2 cooperates\n",
    "    ('D', 'D'): (1, 1),  # Both defect\n",
    "}\n",
    "\n",
    "async def create_enhanced_agents(n=4) -> List[EnhancedAgent]:\n",
    "    \"\"\"Create and initialize multiple agents concurrently.\"\"\"\n",
    "    # Choose from our sorted strategies\n",
    "    sorted_keys = [key for key, desc in PD_STRATEGIES_SORTED]\n",
    "    agents = [EnhancedAgent(f\"Agent_{i}\",\n",
    "                            strategy_tactic=random.choice(sorted_keys),\n",
    "                            cooperation_bias=random.uniform(0.3, 0.7),\n",
    "                            risk_aversion=random.uniform(0.3, 0.7))\n",
    "              for i in range(n)]\n",
    "    agents = await asyncio.gather(*(agent.initialize() for agent in agents))\n",
    "    return agents\n",
    "\n",
    "async def simulate_interaction(agent_a: EnhancedAgent, agent_b: EnhancedAgent) -> Dict:\n",
    "    \"\"\"Simulate an interaction between two agents asynchronously.\"\"\"\n",
    "    decision_a, decision_b = await asyncio.gather(\n",
    "        agent_a.decide_action(agent_b),\n",
    "        agent_b.decide_action(agent_a)\n",
    "    )\n",
    "    \n",
    "    def normalize_action(decision):\n",
    "        action = str(decision.get(\"action\", \"C\")).upper()\n",
    "        return \"C\" if action == \"C\" else \"D\"\n",
    "    \n",
    "    action_a = normalize_action(decision_a)\n",
    "    action_b = normalize_action(decision_b)\n",
    "    \n",
    "    payoff_a, payoff_b = payoff_matrix[(action_a, action_b)]\n",
    "    agent_a.total_score += payoff_a\n",
    "    agent_b.total_score += payoff_b\n",
    "    agent_a.log_interaction(agent_b.name, action_a, action_b, payoff_a)\n",
    "    agent_b.log_interaction(agent_a.name, action_b, action_a, payoff_b)\n",
    "    \n",
    "    interaction = InteractionData(\n",
    "        generation=None,  # To be filled in by simulation runner\n",
    "        pair=f\"{agent_a.name}-{agent_b.name}\",\n",
    "        round_actions=f\"{action_a}-{action_b}\",\n",
    "        payoffs=f\"{payoff_a}-{payoff_b}\",\n",
    "        reasoning_A=decision_a.get(\"rationale\", \"\"),\n",
    "        reasoning_B=decision_b.get(\"rationale\", \"\"),\n",
    "        score_A=agent_a.total_score,\n",
    "        score_B=agent_b.total_score\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"interaction\": interaction,\n",
    "        \"pair\": f\"{agent_a.name}-{agent_b.name}\",\n",
    "        \"Actions\": f\"{action_a}-{action_b}\",\n",
    "        \"Payoffs\": f\"{payoff_a}-{payoff_b}\",\n",
    "        \"Strategy_A\": agent_a.strategy_matrix,\n",
    "        \"Strategy_B\": agent_b.strategy_matrix,\n",
    "        \"Reasoning_A\": decision_a.get(\"rationale\", \"\"),\n",
    "        \"Reasoning_B\": decision_b.get(\"rationale\", \"\"),\n",
    "        \"Score_A\": agent_a.total_score,\n",
    "        \"Score_B\": agent_b.total_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Simulation\n",
    "\n",
    "The main simulation function runs multiple generations of agents, with each generation involving:\n",
    "\n",
    "1. Concurrent agent interactions\n",
    "\n",
    "2. Logging of results\n",
    "\n",
    "3. Evolution (selection of top performers)\n",
    "\n",
    "4. Creation of new agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% \n",
    "async def run_llm_driven_simulation(num_agents=4, num_generations=5, models=[\"gpt-4-turbo\"]):\n",
    "    # Determine the base path for results\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        base_path = os.getcwd()\n",
    "    else:\n",
    "        base_path = os.path.dirname(__file__)\n",
    "    \n",
    "    results_folder = os.path.join(base_path, \"simulation_results\")\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    run_folder = os.path.join(results_folder, f\"run_{current_time}\")\n",
    "    os.makedirs(run_folder, exist_ok=True)\n",
    "    \n",
    "    sim_data = SimulationData(hyperparameters={\n",
    "        \"num_agents\": num_agents,\n",
    "        \"num_generations\": num_generations,\n",
    "        \"payoff_matrix\": {\n",
    "            \"CC\": payoff_matrix[('C', 'C')],\n",
    "            \"CD\": payoff_matrix[('C', 'D')],\n",
    "            \"DC\": payoff_matrix[('D', 'C')],\n",
    "            \"DD\": payoff_matrix[('D', 'D')]\n",
    "        },\n",
    "        \"timestamp\": current_time,\n",
    "        \"models\": models\n",
    "    })\n",
    "    \n",
    "    # Create the dictionary for cooperation biases\n",
    "    PD_COOPERATION_BIASES = {key: bias for key, _, bias in PD_STRATEGIES_SORTED}\n",
    "    \n",
    "    # Update the PD_STRATEGIES dictionary for proper lookups\n",
    "    PD_STRATEGIES = {key: desc for key, desc, _ in PD_STRATEGIES_SORTED}\n",
    "    \n",
    "    # Update agent creation to use strategy-specific cooperation biases\n",
    "    async def create_enhanced_agents(n=4) -> List[EnhancedAgent]:\n",
    "        \"\"\"Create and initialize multiple agents concurrently with strategy-specific cooperation biases.\"\"\"\n",
    "        # Choose from our sorted strategies\n",
    "        sorted_keys = [key for key, _, _ in PD_STRATEGIES_SORTED]\n",
    "        agents = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            # Select a random strategy\n",
    "            strategy = random.choice(sorted_keys)\n",
    "            # Get the corresponding cooperation bias\n",
    "            cooperation_bias = PD_COOPERATION_BIASES[strategy]\n",
    "            # Create agent with strategy-appropriate bias\n",
    "            agent = EnhancedAgent(\n",
    "                f\"Agent_{i}\",\n",
    "                strategy_tactic=strategy,\n",
    "                cooperation_bias=cooperation_bias,\n",
    "                risk_aversion=random.uniform(0.3, 0.7)\n",
    "            )\n",
    "            agents.append(agent)\n",
    "        \n",
    "        agents = await asyncio.gather(*(agent.initialize() for agent in agents))\n",
    "        return agents\n",
    "    \n",
    "    agents = await create_enhanced_agents(num_agents)\n",
    "    all_detailed_logs = []\n",
    "    generation_summary = []\n",
    "    \n",
    "    # Track strategy distribution across generations\n",
    "    strategy_distribution = {gen: {strategy: 0 for strategy, _, _ in PD_STRATEGIES_SORTED} \n",
    "                            for gen in range(1, num_generations + 1)}\n",
    "    \n",
    "    for gen in range(num_generations):\n",
    "        print(f\"\\n=== Generation {gen+1} ===\")\n",
    "        detailed_logs = []\n",
    "        random.shuffle(agents)\n",
    "        \n",
    "        # Record strategy distribution for this generation\n",
    "        for agent in agents:\n",
    "            strategy_distribution[gen+1][agent.strategy_tactic] += 1\n",
    "    \n",
    "        interaction_tasks = []\n",
    "        for i in range(0, len(agents), 2):\n",
    "            if i + 1 < len(agents):\n",
    "                interaction_tasks.append(simulate_interaction(agents[i], agents[i+1]))\n",
    "        \n",
    "        interaction_results = await asyncio.gather(*interaction_tasks)\n",
    "        \n",
    "        gen_metrics = {\n",
    "            \"mutual_cooperation\": 0,\n",
    "            \"mutual_defection\": 0,\n",
    "            \"temptation_payoffs\": 0,\n",
    "            \"sucker_payoffs\": 0,\n",
    "            \"total_payoffs\": 0\n",
    "        }\n",
    "        \n",
    "        for result in interaction_results:\n",
    "            result[\"interaction\"].generation = gen + 1\n",
    "            sim_data.add_interaction(result[\"interaction\"])\n",
    "            \n",
    "            detailed_logs.append({\n",
    "                \"Generation\": gen+1,\n",
    "                **result\n",
    "            })\n",
    "            print(f\"{result['pair']}: {result['Actions']}, Payoffs: {result['Payoffs']}\")\n",
    "            \n",
    "            actions = result['Actions'].split('-')\n",
    "            payoffs = [int(p) for p in result['Payoffs'].split('-')]\n",
    "            gen_metrics[\"total_payoffs\"] += sum(payoffs)\n",
    "            \n",
    "            if actions == ['C', 'C']:\n",
    "                gen_metrics[\"mutual_cooperation\"] += 1\n",
    "            elif actions == ['D', 'D']:\n",
    "                gen_metrics[\"mutual_defection\"] += 1\n",
    "            elif 'D' in actions and 'C' in actions:\n",
    "                if actions[0] == 'D': \n",
    "                    gen_metrics[\"temptation_payoffs\"] += 1\n",
    "                else: \n",
    "                    gen_metrics[\"sucker_payoffs\"] += 1\n",
    "        \n",
    "        all_detailed_logs.extend(detailed_logs)\n",
    "        \n",
    "        total_possible = 3 * len(interaction_results) * 2\n",
    "        pareto_eff = gen_metrics[\"total_payoffs\"] / total_possible if total_possible > 0 else 0\n",
    "        nash_dev = 1 - (gen_metrics[\"mutual_defection\"] / len(interaction_results)) if len(interaction_results) > 0 else 0\n",
    "        strat_diversity = len(set(hash(json.dumps(a.strategy_matrix)) for a in agents))\n",
    "        \n",
    "        avg_score = sum(a.total_score for a in agents) / len(agents) if agents else 0\n",
    "        generation_summary.append({\n",
    "            \"Generation\": gen+1,\n",
    "            \"Average_Score\": avg_score,\n",
    "            \"Pareto_Efficiency\": pareto_eff,\n",
    "            \"Nash_Deviation\": nash_dev,\n",
    "            \"Strategy_Diversity\": strat_diversity,\n",
    "            **gen_metrics\n",
    "        })\n",
    "        \n",
    "        # After simulation, add strategy distribution to summary\n",
    "        generation_summary[-1][\"strategy_distribution\"] = strategy_distribution[gen+1].copy()\n",
    "        \n",
    "        agents.sort(key=lambda a: a.total_score, reverse=True)\n",
    "        top_agents = agents[:num_agents // 2]\n",
    "        new_agents = await create_enhanced_agents(num_agents // 2)\n",
    "        agents = top_agents + new_agents\n",
    "        \n",
    "        for agent in agents:\n",
    "            agent.total_score = 0\n",
    "    \n",
    "    with open(os.path.join(run_folder, \"parameters.json\"), 'w') as f:\n",
    "        json.dump(sim_data.hyperparameters, f, indent=4)\n",
    "    \n",
    "    detailed_df = pd.DataFrame([asdict(inter) for inter in sim_data.interactions])\n",
    "    detailed_df.to_csv(os.path.join(run_folder, \"detailed_logs.csv\"), index=False)\n",
    "    detailed_df.to_json(os.path.join(run_folder, \"detailed_logs.json\"), orient=\"records\", indent=4)\n",
    "    \n",
    "    summary_df = pd.DataFrame(generation_summary)\n",
    "    summary_df.to_csv(os.path.join(run_folder, \"generation_summary.csv\"), index=False)\n",
    "    summary_df.to_json(os.path.join(run_folder, \"generation_summary.json\"), orient=\"records\", indent=4)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    generations = range(1, num_generations + 1)\n",
    "    avg_scores = [entry[\"Average_Score\"] for entry in generation_summary]\n",
    "    plt.plot(generations, avg_scores, marker='o', linestyle='-', linewidth=2)\n",
    "    plt.title(\"Average Cooperation Score over Generations\")\n",
    "    plt.xlabel(\"Generation\")\n",
    "    plt.ylabel(\"Average Score\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(run_folder, \"cooperation_over_generations.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    # Create visualization for strategy distribution over generations\n",
    "    create_strategy_distribution_plot(strategy_distribution, run_folder, num_generations)\n",
    "    \n",
    "    # Add this function to count strategy occurrences over generations\n",
    "    def count_strategy_occurrences(agents, num_generations):\n",
    "        strategy_counts = {strategy: [0] * num_generations for strategy, _, _ in PD_STRATEGIES_SORTED}\n",
    "        for gen in range(num_generations):\n",
    "            for agent in agents:\n",
    "                strategy_counts[agent.strategy_tactic][gen] += 1\n",
    "        return strategy_counts\n",
    "    \n",
    "    # Create research visualizations function with proper indentation\n",
    "    def create_research_visualizations():\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Existing plots\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot([m[\"Strategy_Diversity\"] for m in generation_summary], marker='o')\n",
    "        plt.title(\"Strategy Diversity Over Generations\")\n",
    "        \n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.bar([\"Mutual C\", \"Mutual D\", \"Temptation\", \"Sucker\"], \n",
    "                [generation_summary[-1][\"mutual_cooperation\"],\n",
    "                 generation_summary[-1][\"mutual_defection\"],\n",
    "                 generation_summary[-1][\"temptation_payoffs\"],\n",
    "                 generation_summary[-1][\"sucker_payoffs\"]])\n",
    "        plt.title(\"Final Generation Outcome Distribution\")\n",
    "        \n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot([m[\"Pareto_Efficiency\"] for m in generation_summary], color='green')\n",
    "        plt.title(\"Pareto Efficiency Progress\")\n",
    "        \n",
    "        # Get strategy distribution data\n",
    "        strategies = [strategy for strategy, _, _ in PD_STRATEGIES_SORTED]\n",
    "        \n",
    "        # Plot strategy distribution over generations\n",
    "        plt.subplot(2, 2, 4)\n",
    "        for strategy in strategies:\n",
    "            counts = [gen_summary.get(\"strategy_distribution\", {}).get(strategy, 0) \n",
    "                     for gen_summary in generation_summary]\n",
    "            plt.plot(generations, counts, marker='o', linewidth=2, label=strategy)\n",
    "        \n",
    "        plt.title(\"Strategy Distribution\")\n",
    "        plt.xlabel(\"Generation\")\n",
    "        plt.ylabel(\"Number of Agents\")\n",
    "        plt.legend(loc='upper right', fontsize='x-small')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(run_folder, \"research_metrics.png\"))\n",
    "    \n",
    "    create_research_visualizations()\n",
    "    \n",
    "    print(f\"\\nSimulation completed. Results saved in: {run_folder}\")\n",
    "    return generation_summary, sim_data.interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_strategy_distribution_plot(strategy_distribution, run_folder, num_generations):\n",
    "    \"\"\"\n",
    "    Create a line plot showing the distribution of strategies across generations.\n",
    "    Each line represents a different strategy.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Get all unique strategies\n",
    "    strategies = [strategy for strategy, _, _ in PD_STRATEGIES_SORTED]\n",
    "    generations = range(1, num_generations + 1)\n",
    "    \n",
    "    # Create line for each strategy\n",
    "    for strategy in strategies:\n",
    "        counts = [strategy_distribution[gen][strategy] for gen in generations]\n",
    "        plt.plot(generations, counts, marker='o', linewidth=2, label=strategy)\n",
    "    \n",
    "    plt.title(\"Strategy Distribution Across Generations\", fontsize=16)\n",
    "    plt.xlabel(\"Generation\", fontsize=14)\n",
    "    plt.ylabel(\"Number of Agents\", fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(title=\"Strategies\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(os.path.join(run_folder, \"strategy_distribution.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Simulation\n",
    "\n",
    "Execute the simulation with specified parameters.\n",
    "\n",
    "Note: This will make multiple API calls to OpenAI's GPT-4, so ensure your API key is set up correctly.\n",
    "\n",
    "\n",
    "\n",
    "To run in a Jupyter notebook, first install nest_asyncio:\n",
    "\n",
    "```bash\n",
    "\n",
    "pip install nest_asyncio\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Then run the following cells:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Helper function to run async code in Jupyter\n",
    "async def run_simulation():\n",
    "    return await run_llm_driven_simulation(num_agents=5, num_generations=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generation 1 ===\n",
      "Agent_4-Agent_0: C-C, Payoffs: 3-3\n",
      "Agent_2-Agent_1: C-D, Payoffs: 0-5\n",
      "\n",
      "=== Generation 2 ===\n",
      "Agent_0-Agent_4: C-C, Payoffs: 3-3\n",
      "Agent_1-Agent_1: C-D, Payoffs: 0-5\n",
      "\n",
      "=== Generation 3 ===\n",
      "Agent_0-Agent_0: D-C, Payoffs: 5-0\n",
      "Agent_1-Agent_1: C-D, Payoffs: 0-5\n",
      "\n",
      "=== Generation 4 ===\n",
      "Agent_1-Agent_0: C-D, Payoffs: 0-5\n",
      "Agent_0-Agent_1: C-D, Payoffs: 0-5\n",
      "\n",
      "=== Generation 5 ===\n",
      "Agent_0-Agent_1: D-D, Payoffs: 1-1\n",
      "Agent_1-Agent_0: C-D, Payoffs: 0-5\n",
      "\n",
      "=== Generation 6 ===\n",
      "Agent_0-Agent_0: D-C, Payoffs: 5-0\n",
      "Agent_0-Agent_1: D-C, Payoffs: 5-0\n",
      "\n",
      "=== Generation 7 ===\n",
      "Agent_1-Agent_0: D-D, Payoffs: 1-1\n",
      "Agent_0-Agent_0: C-D, Payoffs: 0-5\n",
      "\n",
      "=== Generation 8 ===\n",
      "Agent_0-Agent_1: C-D, Payoffs: 0-5\n",
      "Agent_1-Agent_0: C-D, Payoffs: 0-5\n",
      "\n",
      "=== Generation 9 ===\n",
      "Agent_1-Agent_0: C-D, Payoffs: 0-5\n",
      "Agent_1-Agent_0: D-C, Payoffs: 5-0\n",
      "\n",
      "=== Generation 10 ===\n",
      "Agent_1-Agent_0: D-D, Payoffs: 1-1\n",
      "Agent_1-Agent_0: C-C, Payoffs: 3-3\n",
      "\n",
      "=== Generation 11 ===\n",
      "Agent_1-Agent_1: C-C, Payoffs: 3-3\n",
      "Agent_0-Agent_0: C-C, Payoffs: 3-3\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "# Setup for Jupyter notebook execution\n",
    "if not os.getenv('JUPYTER_RUNNING_IN_SCRIPT'):\n",
    "    try:\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "        import asyncio\n",
    "        # Create event loop and run simulation\n",
    "        loop = asyncio.get_event_loop()\n",
    "        summary, logs = loop.run_until_complete(run_simulation())\n",
    "    except ImportError:\n",
    "        print(\"Please install nest_asyncio: pip install nest_asyncio\")\n",
    "else:\n",
    "    # For running as a script\n",
    "    summary, logs = asyncio.run(run_llm_driven_simulation(num_agents=4, num_generations=15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Visualization\n",
    "\n",
    "After running the simulation, you can analyze the results using the returned data:\n",
    "\n",
    "- `summary`: Contains generation-level statistics\n",
    "\n",
    "- `logs`: Contains detailed interaction logs\n",
    "\n",
    "\n",
    "\n",
    "Example analysis:\n",
    "\n",
    "```python\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Convert to DataFrames for analysis\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "\n",
    "logs_df = pd.DataFrame(logs)\n",
    "\n",
    "\n",
    "\n",
    "# Analyze cooperation rates\n",
    "\n",
    "cooperation_rates = logs_df['Actions'].apply(lambda x: x.count('C') / len(x))\n",
    "\n",
    "print(f\"Average cooperation rate: {cooperation_rates.mean():.2%}\")\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
