# %% [markdown]
# # Optimized LLM-driven Decision Making for Iterated Prisoner's Dilemma
# 
# 
# 
# This notebook implements an LLM-driven simulation of the Iterated Prisoner's Dilemma using OpenAI's API. 
# 
# The simulation features:
# 
# - Dynamic strategy generation using GPT-4
# 
# - Evolutionary agent selection
# 
# - Asynchronous execution for improved performance
# 
# - Detailed logging and visualization

# %% [markdown]
# ## Setup and Imports
# 
# First, we'll import the necessary libraries and set up our OpenAI client.

# %%
#%% 
from openai import OpenAI, AsyncOpenAI
import os
import random
import pandas as pd
import json
import matplotlib.pyplot as plt
import datetime
from dotenv import load_dotenv
import asyncio
from typing import List, Tuple, Dict
import aiohttp
import sys
from dataclasses import dataclass, field, asdict
import numpy as np

import concurrent.futures
from functools import partial
import time
from tqdm import tqdm
from pydantic import BaseModel, Field
import requests
from prompts_free import Prompts1, Prompts2, prompt_first_action

# Load environment variables and setup OpenAI client
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)
async_client = AsyncOpenAI(api_key=api_key)

# =============================================================================
# Comprehensive List of Iterated Prisoner's Dilemma (IPD) Strategies
# (Arranged from Most Social/Collaborative to Most Antisocial)
# =============================================================================
# Each tuple contains (strategy_key, description)

# =============================================================================
# Data Classes for Structured Logging
# =============================================================================
@dataclass
class InteractionData:
    generation: int
    pair: str
    round_actions: str
    payoffs: str
    reasoning_A: str
    reasoning_B: str
    score_A: int
    score_B: int

@dataclass
class SimulationData:
    hyperparameters: dict
    interactions: List[InteractionData] = field(default_factory=list)
    equilibrium_metrics: dict = field(default_factory=dict)

    def add_interaction(self, interaction: InteractionData):
        self.interactions.append(interaction)

    def add_equilibrium_data(self, generation, nash_deviation, best_response_diff):
        self.equilibrium_metrics[generation] = {
            'nash_deviation': nash_deviation,
            'best_response_diff': best_response_diff
        }

    def to_dict(self):
        return {
            'hyperparameters': self.hyperparameters,
            'interactions': [asdict(inter) for inter in self.interactions],
            'equilibrium_metrics': self.equilibrium_metrics
        }

# %% [markdown]
# ## Agent Implementation
# 
# The EnhancedAgent class represents a player in the Prisoner's Dilemma game.
# 
# Each agent:
# 
# - Has a unique strategy matrix generated by GPT-4
# 
# - Maintains a history of interactions
# 
# - Makes decisions based on past interactions and current game state

# %%
#%% 
class EnhancedAgent(BaseModel):
    name: str
    model: str = Field(default="gpt-4o-mini")
    llm_provider: str = Field(default="openai")  # LLM provider: 'openai' or 'litellm'
    llm_model: str = Field(default="gpt-4o")  # Model to use with the LLM provider
    
    total_score: int = 0
    history: list = Field(default_factory=list)  # Each entry: (opponent_name, own_action, opp_action, payoff)
    cooperation_rate: float = 0.0
    fixed_opponent: str = None  # (Optional) store fixed opponent name for reference
    first_action: str = None  # The first action (C or D) decided at initialization
    prompt_class: type = Prompts1  # Default to Prompts1

    class Config:
        arbitrary_types_allowed = True

    async def initialize(self, temperature):
        """Asynchronously initialize the agent's first action."""
        self.first_action = await self.decide_first_action(temperature)
        return self

    async def decide_first_action(self, temperature):
        """Determine the first action (C or D) using the LLM."""

        
        if self.llm_provider == "openai":
            return await self._decide_with_openai(prompt_first_action, temperature)
        elif self.llm_provider == "litellm":
            return self._decide_with_litellm(prompt_first_action, temperature)
        else:
            raise ValueError("Unsupported LLM provider")

    async def _decide_with_openai(self, prompt, temperature):
        """Use OpenAI to make a decision."""
        for _ in range(3):  # Retry up to 3 times
            try:
                response = await async_client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "system", "content": "You are playing the Prisoner's Dilemma. Respond ONLY with valid JSON."},
                              {"role": "user", "content": prompt}],
                    temperature=temperature,
                    response_format={"type": "json_object"},
                    max_tokens=150
                )
                json_str = response.choices[0].message.content.strip()
                decision = json.loads(json_str)
                action = decision.get("action", "C").upper()
                if action not in ["C", "D"]:
                    print(f"Invalid action '{action}' received, defaulting to C")
                    action = "C"
                return action
            except (json.JSONDecodeError, KeyError) as e:
                print(f"Retrying decision due to error: {str(e)}")
                continue
        
        # Fallback if all retries fail
        print(f"Using fallback decision for {self.name}")
        return "C"
    
    def _decide_with_litellm(self, prompt, temperature):
        """Use LiteLLM to make a decision."""
        messages = [
            {"role": "system", "content": "You are playing the Prisoner's Dilemma. Respond ONLY with valid JSON."},
            {"role": "user", "content": prompt}
        ]
        
        for _ in range(3):  # Retry up to 3 times
            try:
                response = self._call_litellm(messages, model=self.llm_model, temperature=temperature)
                json_str = response.strip()
                decision = json.loads(json_str)
                action = decision.get("action", "C").upper()
                if action not in ["C", "D"]:
                    print(f"Invalid action '{action}' received, defaulting to C")
                    action = "C"
                return action
            except (json.JSONDecodeError, KeyError) as e:
                print(f"Retrying decision due to error: {str(e)}")
                continue
        
        # Fallback if all retries fail
        print(f"Using fallback decision for {self.name}")
        return "C"

    def _call_litellm(self, messages, model="gpt-4o", temperature=0.8):
        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
        }
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {os.getenv('LITELLM_API_KEY')}",
        }

        response = requests.post("https://litellm.sph-prod.ethz.ch/chat/completions", json=payload, headers=headers)
        if not response.ok:
            raise Exception(f"LiteLLM API error: {response.status_code} {response.reason}")

        data = response.json()
        return data.get("choices", [{}])[0].get("message", {}).get("content", "No response from LiteLLM.")

    async def decide_action(self, opponent, temperature, is_first_round=False):
        """
        Determine an action using the LLM based on previous interactions.
        For the first round, use the pre-determined first action.
        """
        if is_first_round:
            return {
                "action": self.first_action,
                "rationale": "This is my pre-determined first action."
            }
        
        # For subsequent rounds, make decisions based on previous interaction only
        last_own_action = self.history[-1][1] if self.history else None
        last_opp_action = self.history[-1][2] if self.history else None
        
        if last_own_action is None or last_opp_action is None:
            # Fallback if history is empty for some reason
            return {
                "action": self.first_action or "C",
                "rationale": "Using initial action as fallback due to missing history."
            }
        
        decision_prompt = self.prompt_class.get_next_round_decision_prompt(
            agent_name=self.name,
            own_last_action=last_own_action,
            opponent_name=opponent.name,
            opponent_last_action=last_opp_action
        )
        
        if self.llm_provider == "openai":
            return await self._decide_next_with_openai(decision_prompt, temperature)
        elif self.llm_provider == "litellm":
            return self._decide_next_with_litellm(decision_prompt, temperature)
        else:
            raise ValueError(f"Unsupported LLM provider: {self.llm_provider}")
    
    async def _decide_next_with_openai(self, prompt, temperature):
        """Use OpenAI to decide the next action."""
        for _ in range(3):  # Retry up to 3 times
            try:
                response = await async_client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "system", "content": "You are playing the Prisoner's Dilemma. Respond ONLY with valid JSON."},
                              {"role": "user", "content": prompt}],
                    temperature=temperature,
                    response_format={"type": "json_object"},
                    max_tokens=150
                )
                json_str = response.choices[0].message.content.strip()
                decision = json.loads(json_str)
                action = decision.get("action", "C").upper()
                if action not in ["C", "D"]:
                    print(f"Invalid action '{action}' received, defaulting to C")
                    decision["action"] = "C"
                return decision
            except (json.JSONDecodeError, KeyError) as e:
                print(f"Retrying decision due to error: {str(e)}")
                continue
        
        # Fallback if all retries fail
        return {
            "action": "C",
            "rationale": "Fallback decision after multiple LLM failures"
        }
    
    def _decide_next_with_litellm(self, prompt, temperature):
        """Use LiteLLM to decide the next action."""
        messages = [
            {"role": "system", "content": "You are playing the Prisoner's Dilemma. Respond ONLY with valid JSON."},
            {"role": "user", "content": prompt}
        ]
        
        for _ in range(3):  # Retry up to 3 times
            try:
                response = self._call_litellm(messages, model=self.llm_model, temperature=temperature)
                json_str = response.strip()
                decision = json.loads(json_str)
                action = decision.get("action", "C").upper()
                if action not in ["C", "D"]:
                    print(f"Invalid action '{action}' received, defaulting to C")
                    decision["action"] = "C"
                return decision
            except (json.JSONDecodeError, KeyError) as e:
                print(f"Retrying decision due to error: {str(e)}")
                continue
        
        # Fallback if all retries fail
        return {
            "action": "C",
            "rationale": "Fallback decision after multiple LLM failures"
        }

    def log_interaction(self, opponent, own_action, opp_action, payoff):
        self.history.append((opponent, own_action, opp_action, payoff))
        # Update cooperation rate
        self.cooperation_rate = sum(1 for h in self.history if h[1] == "C") / len(self.history)

# %% [markdown]
# ## Game Configuration
# 
# Define the payoff matrix for the Prisoner's Dilemma and helper functions for agent creation and interaction.

# %%
#%% 
# Define the standard Prisoner's Dilemma payoff matrix.
payoff_matrix = {
    ('C', 'C'): (3, 3),  # Both cooperate
    ('C', 'D'): (0, 5),  # Player 1 cooperates, Player 2 defects
    ('D', 'C'): (5, 0),  # Player 1 defects, Player 2 cooperates
    ('D', 'D'): (1, 1),  # Both defect
}

# async def create_enhanced_agents(n=4, temperature=0.8, provider="openai", model="gpt-4o-mini", prompt_class=Prompts1) -> List[EnhancedAgent]:
#     """Create and initialize multiple agents concurrently with unique first actions."""
#     agents = []
#     for i in range(n):
#         agent = EnhancedAgent(
#             name=f"Agent_{i+1}",
#             model=model,
#             llm_provider=provider,
#             llm_model=model,
#             prompt_class=prompt_class
#         )
#         agents.append(agent)

#     # Initialize all agents concurrently (each decides their first action)
#     agents = await asyncio.gather(*(agent.initialize(temperature) for agent in agents))
#     return agents

async def simulate_interaction(agent_a: EnhancedAgent, agent_b: EnhancedAgent, temperature=0.8, is_first_round=False) -> Dict:
    """Simulate an interaction between two agents asynchronously."""
    decision_a, decision_b = await asyncio.gather(
        agent_a.decide_action(agent_b, temperature, is_first_round),
        agent_b.decide_action(agent_a, temperature, is_first_round)
    )
    
    def normalize_action(decision):
        action = str(decision.get("action", "C")).upper()
        return "C" if action == "C" else "D"
    
    action_a = normalize_action(decision_a)
    action_b = normalize_action(decision_b)
    
    payoff_a, payoff_b = payoff_matrix[(action_a, action_b)]
    agent_a.total_score += payoff_a
    agent_b.total_score += payoff_b
    agent_a.log_interaction(agent_b.name, action_a, action_b, payoff_a)
    agent_b.log_interaction(agent_a.name, action_b, action_a, payoff_b)
    
    interaction = InteractionData(
        generation=None,  # To be filled in by simulation runner
        pair=f"{agent_a.name}-{agent_b.name}",
        round_actions=f"{action_a}-{action_b}",
        payoffs=f"{payoff_a}-{payoff_b}",
        reasoning_A=decision_a.get("rationale", ""),
        reasoning_B=decision_b.get("rationale", ""),
        score_A=agent_a.total_score,
        score_B=agent_b.total_score
    )
    
    return {
        "interaction": interaction,
        "pair": f"{agent_a.name}-{agent_b.name}",
        "Actions": f"{action_a}-{action_b}",
        "Payoffs": f"{payoff_a}-{payoff_b}",
        "Reasoning_A": decision_a.get("rationale", ""),
        "Reasoning_B": decision_b.get("rationale", ""),
        "Score_A": agent_a.total_score,
        "Score_B": agent_b.total_score
    }

# %% [markdown]
# ## Main Simulation
# 
# The main simulation function runs multiple generations of agents, with each generation involving:
# 
# 1. Concurrent agent interactions
# 
# 2. Logging of results
# 
# 3. Evolution (selection of top performers)
# 
# 4. Creation of new agents

# %%

# %%
#%% 
async def run_llm_driven_simulation(num_agents=4, num_generations=5, models=["gpt-4o-mini"], prompt_choice="prompts1", llm_provider="openai", temperature=0.8):
    start_time = time.time()  # Start the timer
    print(f"\n[{time.strftime('%H:%M:%S')}] Starting simulation preparation...")
    try:
        # Determine the base path for results
        print(f"[{time.strftime('%H:%M:%S')}] Setting up results directory...")
        if 'ipykernel' in sys.modules:
            base_path = os.getcwd()
        else:
            base_path = os.path.dirname(__file__)

        results_folder = os.path.join(base_path, "simulations_results_free")
        current_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        run_folder = os.path.join(results_folder, f"run_{current_time}")
        os.makedirs(run_folder, exist_ok=True)
        print(f"[{time.strftime('%H:%M:%S')}] Created folder: {run_folder}")

        # Determine which prompt class to use
        print(f"[{time.strftime('%H:%M:%S')}] Setting up prompt class: {prompt_choice}...")
        prompt_class = Prompts1 if prompt_choice == "prompts1" else Prompts2

        # Set up model to use
        model = models[0] if models else "gpt-4o-mini"
        print(f"[{time.strftime('%H:%M:%S')}] Using model: {model}")

        # For our 1-1 competitions, we'll create 2*num_agents
        actual_num_agents = 2 * num_agents
        print(f"[{time.strftime('%H:%M:%S')}] Will create {actual_num_agents} agents for {num_agents} 1-1 competitions")

        sim_data = SimulationData(hyperparameters={
            "num_agents": actual_num_agents,
            "num_pairs": num_agents,
            "num_generations": num_generations,
            "payoff_matrix": {
                "CC": payoff_matrix[('C', 'C')],
                "CD": payoff_matrix[('C', 'D')],
                "DC": payoff_matrix[('D', 'C')],
                "DD": payoff_matrix[('D', 'D')]
            },
            "timestamp": current_time,
            "llm_provider": llm_provider,
            "model": model,
            "prompt_choice": prompt_choice
        })
            
        # Initialize agents with unique names
        print(f"[{time.strftime('%H:%M:%S')}] Starting agent initialization. This may take some time...")
        print(f"[{time.strftime('%H:%M:%S')}] Initializing {actual_num_agents} agents with {prompt_choice} prompts...")
        
        # Will be used to track agent creation progress
        init_progress = tqdm(total=actual_num_agents, desc="Creating agents", unit="agent")
        
        # Create a list to store initialization tasks
        init_tasks = []
        for i in range(actual_num_agents):
            agent = EnhancedAgent(
                name=f"Agent_{i+1}",
                model=model,
                llm_provider=llm_provider,
                llm_model=model,
                prompt_class=prompt_class
            )
            # Add the initialization task to our list
            init_tasks.append(agent.initialize(temperature))
        
        # Initialize all agents concurrently (each decides their first action)
        print(f"[{time.strftime('%H:%M:%S')}] Waiting for API responses to determine first actions...")
        agents = []
        for init_task in asyncio.as_completed(init_tasks):
            agent = await init_task
            agents.append(agent)
            init_progress.update(1)
            print(f"[{time.strftime('%H:%M:%S')}] {agent.name} initialized with first action: {agent.first_action}")
        
        init_progress.close()
        print(f"[{time.strftime('%H:%M:%S')}] All agents initialized successfully.")
        
        # Log first action for each agent
        print(f"Log of first decisions: \n")
        for agent in agents:
            print(f"[{time.strftime('%H:%M:%S')}] {agent.name} first action: {agent.first_action}")
    
        all_detailed_logs = []
        generation_summary = []

        # Create fixed pairs of agents - every 2 consecutive agents form a pair
        agent_pairs = []
        for i in range(0, len(agents), 2):
            if i + 1 < len(agents):
                agent_pairs.append((agents[i], agents[i+1]))
        
        print(f"\n")

        print(f"[{time.strftime('%H:%M:%S')}] Created {len(agent_pairs)} agent pairs for competition")
        print(f"\n")

        # Store opponent history for each agent
        for agent in agents:
            agent.fixed_opponent = None
        
        # Assign fixed opponents
        for agent_a, agent_b in agent_pairs:
            agent_a.fixed_opponent = agent_b.name
            agent_b.fixed_opponent = agent_a.name

        # Lists to track cooperation rates over generations for means and variances
        cooperation_rates_by_gen = []
        
        for gen in tqdm(range(num_generations), desc="Simulating Generations", unit="generation"):
            print(f"\n[{time.strftime('%H:%M:%S')}] === Generation {gen+1} ===")
            detailed_logs = []
            
            # Track cooperation vs defection for metrics
            gen_metrics = {
                "mutual_cooperation": 0,
                "mutual_defection": 0,
                "temptation_payoffs": 0,
                "sucker_payoffs": 0,
                "total_payoffs": 0
            }
            
            # Track cooperation rates for this generation
            gen_coop_rates = []
            
            print(f"[{time.strftime('%H:%M:%S')}] Executing {len(agent_pairs)} interactions concurrently...")
            
            interaction_tasks = []
            for agent_a, agent_b in agent_pairs:
                # For the first generation (gen=0), use the initial actions
                is_first_round = (gen == 0)
                interaction_tasks.append(simulate_interaction(agent_a, agent_b, temperature, is_first_round))

            # Run all interactions concurrently
            interaction_results = await asyncio.gather(*interaction_tasks)
            
            print(f"[{time.strftime('%H:%M:%S')}] Processing results for generation {gen+1}...")
            
            # Process results
            for result in interaction_results:
                result["interaction"].generation = gen + 1
                sim_data.add_interaction(result["interaction"])

                detailed_logs.append({
                    "Generation": gen+1,
                    **result
                })
                print(f"[{time.strftime('%H:%M:%S')}] {result['pair']}: {result['Actions']}, Payoffs: {result['Payoffs']}")

                actions = result['Actions'].split('-')
                payoffs = [int(p) for p in result['Payoffs'].split('-')]
                
                # Update metrics
                gen_metrics["total_payoffs"] += sum(payoffs)

                # Track cooperation rates for agent A and B
                a_cooperated = 1 if actions[0] == 'C' else 0
                b_cooperated = 1 if actions[1] == 'C' else 0
                gen_coop_rates.extend([a_cooperated, b_cooperated])
                
                if actions == ['C', 'C']:
                    gen_metrics["mutual_cooperation"] += 1
                elif actions == ['D', 'D']:
                    gen_metrics["mutual_defection"] += 1
                elif actions[0] == 'D' and actions[1] == 'C':
                    gen_metrics["temptation_payoffs"] += 1
                elif actions[0] == 'C' and actions[1] == 'D':
                    gen_metrics["sucker_payoffs"] += 1

            # Store cooperation rates for this generation
            cooperation_rates_by_gen.append(gen_coop_rates)
            
            all_detailed_logs.extend(detailed_logs)
            

            
            # Calculate Nash Equilibrium metrics
            br_diffs = []
            nash_regrets = []

            for result in interaction_results:
                action_a, action_b = result['Actions'].split('-')
                payoff_a, payoff_b = map(int, result['Payoffs'].split('-'))

                # Correct standard PD best response is always Defect
                br_a = 'D'
                br_b = 'D'

                # Then compute payoff differences using the payoff matrix
                br_diff_a = payoff_matrix[(br_a, action_b)][0] - payoff_a
                br_diff_b = payoff_matrix[(action_a, br_b)][1] - payoff_b

                br_diffs.extend([br_diff_a, br_diff_b])
                nash_regrets.extend([br_diff_a > 0, br_diff_b > 0])

            # Calculate equilibrium metrics
            avg_br_diff = np.mean(br_diffs) if br_diffs else 0
            nash_dev = np.mean(nash_regrets) if nash_regrets else 0

            # Store equilibrium data
            sim_data.add_equilibrium_data(gen+1, nash_dev, avg_br_diff)

            # Calculate other metrics for the summary
            total_possible = 3 * len(interaction_results) * 2
            pareto_eff = gen_metrics["total_payoffs"] / total_possible if total_possible > 0 else 0
            
            # Track cooperation rates for each agent
            agent_actions = {}
            for agent in agents:
                agent_actions[agent.name] = "C" if agent.cooperation_rate >= 0.5 else "D"
            
            # Calculate average cooperation rate for this generation
            avg_coop_rate = np.mean(gen_coop_rates) if gen_coop_rates else 0
            
            # Calculate variance of cooperation rate
            var_coop_rate = np.var(gen_coop_rates) if gen_coop_rates else 0
                
            avg_score = sum(a.total_score for a in agents) / len(agents) if agents else 0
            generation_summary.append({
                "Generation": gen+1,
                "Average_Score": avg_score,
                "Pareto_Efficiency": pareto_eff,
                "Nash_Deviation": nash_dev,
                "best_response_diff": avg_br_diff,
                "agent_actions": agent_actions,
                "average_cooperation_rate": avg_coop_rate,
                "variance_cooperation_rate": var_coop_rate,
                "run_folder": run_folder,
                **gen_metrics
            })

            # Reset scores for the next generation but keep history
            for agent in agents:
                agent.cooperation_rate = sum(1 for h in agent.history if h[1] == "C") / len(agent.history) if agent.history else 0
                agent.total_score = 0

        print(f"[{time.strftime('%H:%M:%S')}] Simulation completed. Saving results...")
        
        with open(os.path.join(run_folder, "parameters.json"), 'w') as f:
            json.dump(sim_data.hyperparameters, f, indent=4)

        # Save detailed logs
        detailed_df = pd.DataFrame([asdict(inter) for inter in sim_data.interactions])
        detailed_df.to_csv(os.path.join(run_folder, "detailed_logs.csv"), index=False)
        detailed_df.to_json(os.path.join(run_folder, "detailed_logs.json"), orient="records", indent=4)

        # Save generation summary
        summary_df = pd.DataFrame(generation_summary)
        summary_df.to_csv(os.path.join(run_folder, "generation_summary.csv"), index=False)
        summary_df.to_json(os.path.join(run_folder, "generation_summary.json"), orient="records", indent=4)

        # Calculate mean and variance for all generations
        print(f"[{time.strftime('%H:%M:%S')}] Creating visualizations with mean and variance...")
        
        # Convert to numpy arrays for easier calculations
        all_coop_rates = np.array(cooperation_rates_by_gen)
        
        # Calculate mean cooperation rate for each generation
        mean_coop_rates = np.mean(all_coop_rates, axis=1)
        
        # Calculate variance for each generation
        var_coop_rates = np.var(all_coop_rates, axis=1)
        
        # Calculate standard deviation for visualization
        std_coop_rates = np.sqrt(var_coop_rates)
        
        # Create upper and lower bounds for the "tunnel" visualization
        upper_bound = np.minimum(mean_coop_rates + std_coop_rates, 1.0)  # Cap at 1.0
        lower_bound = np.maximum(mean_coop_rates - std_coop_rates, 0.0)  # Floor at 0.0
        
        # Plot cooperation rates with variance tunnel
        plt.figure(figsize=(12, 8), facecolor='white')
        generations = range(1, num_generations + 1)
        
        # Plot the mean line
        plt.plot(generations, mean_coop_rates, 'b-', linewidth=3, label='Mean Cooperation Rate')
        
        # Plot the variance tunnel
        plt.fill_between(generations, lower_bound, upper_bound, color='lightblue', alpha=0.5, label='±1 Std Dev')
        
        plt.title(f"Cooperation Rate Over Generations with Variance - {prompt_choice}", fontsize=16)
        plt.xlabel("Generation", fontsize=14)
        plt.ylabel("Cooperation Rate", fontsize=14)
        plt.ylim(-0.05, 1.05)  # Add a little padding
        plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='50% Cooperation')
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.legend(loc='best', fontsize=12)
        plt.tight_layout()
        
        # Ensure white background
        ax = plt.gca()
        ax.set_facecolor('white')
        
        plt.savefig(os.path.join(run_folder, "cooperation_rate_with_variance.png"), 
                    dpi=300, facecolor='white', bbox_inches='tight')
        plt.close()
        
        # Plot average score with variance
        plt.figure(figsize=(12, 8), facecolor='white')
        
        # Get average scores from generation summary
        avg_scores = np.array([entry["Average_Score"] for entry in generation_summary])
        
        # Plot the mean score line
        plt.plot(generations, avg_scores, 'g-', linewidth=3, label='Average Score')
        
        plt.title(f"Average Score Over Generations - {prompt_choice}", fontsize=16)
        plt.xlabel("Generation", fontsize=14)
        plt.ylabel("Average Score", fontsize=14)
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.legend(loc='best', fontsize=12)
        plt.tight_layout()
        
        # Ensure white background
        ax = plt.gca()
        ax.set_facecolor('white')
        
        plt.savefig(os.path.join(run_folder, "average_score_over_generations.png"), 
                    dpi=300, facecolor='white', bbox_inches='tight')
        plt.close()
        
        # Create comprehensive research metrics visualization
        plt.figure(figsize=(16, 12), facecolor='white')
        
        # Add a main title with the requested information
        main_title = (
            f"Prisoner's Dilemma Research Metrics\n"
            f"Provider: {sim_data.hyperparameters['llm_provider']}, "
            f"Model: {sim_data.hyperparameters['model']}, "
            f"Prompt: {prompt_choice}\n"
            f"Date & Time: {sim_data.hyperparameters['timestamp']}\n"
            f"Agents: {sim_data.hyperparameters['num_agents']} "
            f"({sim_data.hyperparameters['num_pairs']} pairs), "
            f"Generations: {sim_data.hyperparameters['num_generations']}"
        )
        plt.suptitle(main_title, fontsize=16, y=0.98)
        
        # 1. Plot cooperation rate with variance (top left)
        plt.subplot(2, 2, 1)
        plt.plot(generations, mean_coop_rates, 'b-', linewidth=3)
        plt.fill_between(generations, lower_bound, upper_bound, color='lightblue', alpha=0.5)
        plt.title('Cooperation Rate with Variance')
        plt.xlabel('Generation')
        plt.ylabel('Rate')
        plt.ylim(-0.05, 1.05)
        plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.7)
        plt.grid(True, alpha=0.5)
        
        # Set white background
        ax = plt.gca()
        ax.set_facecolor('white')
        
        # 2. Plot outcome distribution for final generation (top right)
        plt.subplot(2, 2, 2)
        bar_colors = ['#4CAF50', '#F44336', '#2196F3', '#FF9800']  # Green, Red, Blue, Orange
        plt.bar(["Mutual C", "Mutual D", "Temptation", "Sucker"], 
                [generation_summary[-1]["mutual_cooperation"],
                generation_summary[-1]["mutual_defection"],
                generation_summary[-1]["temptation_payoffs"],
                generation_summary[-1]["sucker_payoffs"]],
                color=bar_colors)
        plt.title("Final Generation Outcome Distribution")
        
        # Set white background
        ax = plt.gca()
        ax.set_facecolor('white')
        
        # 3. Plot average score over generations (bottom left)
        plt.subplot(2, 2, 3)
        plt.plot(generations, avg_scores, marker='o', color='green', linewidth=3, markersize=8)
        plt.title('Average Score')
        plt.xlabel('Generation')
        plt.ylabel('Score')
        plt.grid(True, alpha=0.5)
        
        # Set white background
        ax = plt.gca()
        ax.set_facecolor('white')
        
        # 4. Plot variances over generations (bottom right)
        plt.subplot(2, 2, 4)
        plt.plot(generations, var_coop_rates, marker='o', color='purple', linewidth=3, markersize=8, label='Cooperation Variance')
        plt.title('Cooperation Rate Variance')
        plt.xlabel('Generation')
        plt.ylabel('Variance')
        plt.grid(True, alpha=0.5)
        
        # Set white background
        ax = plt.gca()
        ax.set_facecolor('white')
        
        plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit the main title
        plt.savefig(os.path.join(run_folder, "comprehensive_metrics.png"), 
                    dpi=300, facecolor='white', bbox_inches='tight')
        plt.close()
        
        # Create a visualization comparing individual agent strategies over time
        plt.figure(figsize=(15, 10), facecolor='white')
        
        # Group agents by pairs for visualization
        for i, (agent_a, agent_b) in enumerate(agent_pairs):
            if i >= 4:  # Limit to first 4 pairs to avoid overcrowding
                break
                
            plt.subplot(2, 2, i+1)
            
            # Convert actions to numerical values (C=1, D=0)
            actions_a = [1 if h[1] == "C" else 0 for h in agent_a.history]
            actions_b = [1 if h[1] == "C" else 0 for h in agent_b.history]
            rounds = range(1, len(actions_a) + 1)
            
            # Plot this pair's actions with improved visibility
            plt.plot(rounds, actions_a, 'b-', marker='o', linewidth=2.5, markersize=8, 
                    label=f"{agent_a.name}")
            plt.plot(rounds, actions_b, 'r-', marker='x', linewidth=2.5, markersize=8,
                    label=f"{agent_b.name}")
            
            plt.title(f"Pair {i+1}: {agent_a.name} vs {agent_b.name}")
            plt.xlabel("Round")
            plt.ylabel("Action (1=C, 0=D)")
            plt.yticks([0, 1], ["Defect", "Cooperate"])
            plt.grid(True, linestyle='--', alpha=0.7)
            plt.legend()
            
            # Set white background
            ax = plt.gca()
            ax.set_facecolor('white')
        
        plt.tight_layout()
        plt.savefig(os.path.join(run_folder, "agent_pair_strategies.png"), 
                    dpi=300, facecolor='white', bbox_inches='tight')
        
        print(f"[{time.strftime('%H:%M:%S')}] All visualizations created successfully.")
        
        # After saving logs, format them with Prettier
        format_logs_with_prettier(run_folder)
        
        # Return summaries for further analysis if needed
        return generation_summary, all_detailed_logs
    
    except Exception as e:
        print(f"[{time.strftime('%H:%M:%S')}] An error occurred during the simulation: {e}")
        import traceback
        traceback.print_exc()
        return None, None

# %%

# Add this new function toward the end of the file (e.g. just before the final __main__ block)
import matplotlib.pyplot as plt
import numpy as np
import os
import subprocess

def format_logs_with_prettier(run_folder):
    """Format all CSV and JSON files in the run folder using Prettier."""
    for file_name in os.listdir(run_folder):
        if file_name.endswith('.csv') or file_name.endswith('.json'):
            file_path = os.path.join(run_folder, file_name)
            try:
                subprocess.run(['prettier', '--write', file_path], check=True)
                print(f"Formatted {file_name} with Prettier.")
            except subprocess.CalledProcessError as e:
                print(f"Failed to format {file_name} with Prettier: {e}")

    

# %%

# %% [markdown]
# ## Run the Simulation
# 
# Execute the simulation with specified parameters.
# 
# Note: This will make multiple API calls to OpenAI's GPT-4, so ensure your API key is set up correctly.
# 
# 
# 
# To run in a Jupyter notebook, first install nest_asyncio:
# 
# bash
# 
# pip install nest_asyncio
# 
# 
# 
# 
# 
# Then run the following cells:

# %% [markdown]
# 

# %%
#%%
# Helper function to run async code in Jupyter
async def run_simulation(num_agents=8, num_generations=3, model="gpt-4o-mini", llm_provider="openai", prompt_choice="prompts1", temperature=0.8):
    """Run the simulation with configurable parameters."""
    return await run_llm_driven_simulation(
        num_agents=num_agents, 
        num_generations=num_generations,
        models=[model],
        prompt_choice=prompt_choice,
        llm_provider=llm_provider,
        temperature=temperature
    )

# %%

def main(num_agents=8, num_generations=4, llm_provider="openai", llm_model="gpt-4o", prompt_choice="prompts1", temperature=0.8):
    """
    Main function to run the simulation when script is executed directly.
    
    Parameters:
    -----------
    num_agents : int
        Number of agent pairs in the simulation (will create 2*num_agents actual agents)
    num_generations : int
        Number of generations to run
    llm_provider : str
        The LLM provider to use ('openai' or 'litellm')
    llm_model : str
        The model to use with the LLM provider
    prompt_choice : str
        The prompt choice to use ('prompts1' for long-term focus or 'prompts2' for short-term focus)
    temperature : float
        The temperature to use for LLM calls
        
    Notes:
    ------
    This enhanced version of the simulation:
    1. Creates twice as many agents as specified (for 1-1 competitions)
    2. Calculates mean and variance of cooperation rates across all agents
    3. Visualizes the variance as a "tunnel" around the mean values
    4. Generates comprehensive research visualizations with statistical metrics
    5. Provides detailed logging throughout the simulation process
    
    Generated visualizations include:
    - cooperation_rate_with_variance.png - Mean cooperation rate with variance tunnel
    - average_score_over_generations.png - Average score progression
    - comprehensive_metrics.png - Combined metrics dashboard
    - agent_pair_strategies.png - Individual agent pair strategies
    """
    try:
        # Print a nice header
        print("\n\n\n\n\n\n\n\n\n\n" + "="*80)
        print(" "*30 + "PRISONER'S DILEMMA SIMULATION")
        print("="*80 + "\n")
        
        # Print simulation parameters
        print("SIMULATION PARAMETERS:")
        print("-"*50)
        print(f"Number of agents:      {num_agents}")
        print(f"Number of generations: {num_generations}")
        print(f"LLM Provider:          {llm_provider}")
        print(f"Model:                 {llm_model}")
        print(f"Prompt choice:         {prompt_choice}")
        print(f"Temperature:           {temperature}")
        print("-"*50 + "\n")
        
        print("Starting simulation...\n")
        
        import nest_asyncio
        nest_asyncio.apply()
        import asyncio
        import os
        
        # Ensure we're using relative paths for saving results
        original_cwd = os.getcwd()
        script_dir = os.path.dirname(os.path.abspath(__file__))
        os.chdir(script_dir)
        
        # Create event loop and run simulation
        loop = asyncio.get_event_loop()
        summary, logs = loop.run_until_complete(run_simulation(
            num_agents=num_agents,
            num_generations=num_generations,
            model=llm_model,
            llm_provider=llm_provider,
            prompt_choice=prompt_choice,
            temperature=temperature
        ))
        
        # Print initial agent actions
        if summary and len(summary) > 0:
            print("\nINITIAL AGENT ACTIONS:")
            print("-"*50)
            
            # Extract agent actions from the agent_actions field
            first_gen = summary[0]
            if 'agent_actions' in first_gen:
                for agent, action in first_gen['agent_actions'].items():
                    print(f"{agent}: {action}")
            
            print("-"*50 + "\n")
        
        # Get the run folder from the summary if available
        run_folder = None
        if summary and len(summary) > 0:
            run_folder = summary[0].get('run_folder', None)
        
        # If we found the run folder, list the plots that were saved
        if run_folder and os.path.exists(run_folder):
            print("\nPLOTS GENERATED:")
            print("-"*50)
            plot_files = [
                "cooperation_rate_with_variance.png",
                "average_score_over_generations.png", 
                "comprehensive_metrics.png",
                "agent_pair_strategies.png"
            ]
            
            print("Click on any of these paths to open the image:")
            for plot_file in plot_files:
                file_path = os.path.join(run_folder, plot_file)
                if os.path.exists(file_path):
                    # Get absolute path for easier access
                    abs_path = os.path.abspath(file_path)
                    # Format as a clickable link (works in many terminals)
                    print(f"✓ {plot_file}:")
                    print(f"  file://{abs_path}")
                else:
                    print(f"✗ {plot_file} (not generated)")
            
            print("\nNote: You can copy-paste these paths into your browser if clicking doesn't work.")
            print("-"*50 + "\n")
        
        # Restore original working directory
        os.chdir(original_cwd)
        
        print("\n" + "="*80)
        print(" "*30 + "SIMULATION COMPLETED")
        print("="*80 + "\n")
        
        return summary, logs
    except ImportError:
        print("\nERROR: Please install nest_asyncio: pip install nest_asyncio")
        return None, None
    except Exception as e:
        print(f"\nERROR: An unexpected error occurred: {e}")
        import traceback
        traceback.print_exc()
        return None, None

# This ensures the main() function only runs when the script is executed directly
if __name__ == "__main__":
    main(
        num_agents=1,
        num_generations=5,
        llm_provider="litellm",  # Options: "openai", "litellm"
        llm_model="gpt-4o-mini",
        prompt_choice="prompts1",  # Options: "prompts1", "prompts2"
        temperature=0.8
    )

