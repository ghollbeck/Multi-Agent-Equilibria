{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Jupyter Notebook: Iterated Prisoner's Dilemma (Example)**\n",
    "---\n",
    "\n",
    "## **Code Box 1: Title & Explanation**\n",
    "\n",
    "**Title**: *Multi-Agent Prisoner's Dilemma with LLM-Generated Strategies*\n",
    "\n",
    "**Explanation**:  \n",
    "This notebook demonstrates a simple multi-agent Prisoner’s Dilemma simulation, where each agent’s “strategy” is suggested by an LLM (OpenAI model). We keep the population size small and the number of generations low for easy testing.  \n",
    "\n",
    "The overall flow follows the structure from the research proposal:\n",
    "\n",
    "1. **Initialize Parameters**  \n",
    "   - Choose LLM type (OpenAI)  \n",
    "   - Game mechanics (Prisoner’s Dilemma payoffs)  \n",
    "   - Evolution parameters (how many generations, how many agents survive, etc.)\n",
    "\n",
    "2. **Generation 1: Create initial agents**  \n",
    "   - Use LLM to generate a short text description or “policy” for each agent.  \n",
    "   - Store these strategies locally.\n",
    "\n",
    "3. **Play the Donor Game (Prisoner’s Dilemma)**  \n",
    "   - Pair up agents (for simplicity, we just do random pairings).  \n",
    "   - Each agent decides to *cooperate* or *defect* based on their “policy.”  \n",
    "   - Resources (scores) get updated based on PD payoffs.  \n",
    "   - Track any relevant info (e.g., reputation).\n",
    "\n",
    "4. **Resource Calculation**  \n",
    "   - Final scores are tallied.  \n",
    "   - Data is collected for analysis.\n",
    "\n",
    "5. **Selection Process**  \n",
    "   - Top 50% of agents survive (or some survival threshold).  \n",
    "   - Surviving agents pass on their strategies.\n",
    "\n",
    "6. **Strategy Transmission**  \n",
    "   - New agents join to keep population size constant.  \n",
    "   - The new agents may incorporate or mutate strategies from the surviving agents (again we prompt the LLM to generate text).\n",
    "\n",
    "7. **Next Generation**  \n",
    "   - Combine survivors + new agents.  \n",
    "   - Repeat the cycle if more generations are needed.\n",
    "\n",
    "8. **Analysis & Storage**  \n",
    "   - Store results (cooperation rates, final resources, etc.)  \n",
    "   - Visualize or print basic statistics about the outcomes.\n",
    "\n",
    "We use OpenAI’s `openai` library for generating strategy descriptions or updates. Make sure you have an API key set up in your environment, e.g. with `export OPENAI_API_KEY=\"...\"` or by adding it directly to `openai.api_key`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Client Setup\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client with API key from environment\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Parameters\n",
    "NUM_AGENTS = 10        # small population for testing\n",
    "NUM_GENERATIONS = 30   # run a few generations to keep it quick\n",
    "SURVIVAL_RATE = 0.5   # top 50% survive each generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Code Box 3: Prisoner’s Dilemma Setup & Utility Functions**\n",
    "\n",
    "# Payoff matrix for Prisoner's Dilemma, typical example:\n",
    "# Cooperate (C), Defect (D)\n",
    "# If both cooperate: each gets R (reward)\n",
    "# If one cooperates & the other defects: cooperator gets S (sucker's payoff), defector gets T (temptation)\n",
    "# If both defect: each gets P (punishment)\n",
    "\n",
    "# Let's define numeric values:\n",
    "R = 3  # Reward\n",
    "T = 5  # Temptation\n",
    "S = 0  # Sucker's payoff\n",
    "P = 1  # Punishment\n",
    "\n",
    "def play_pd(agent1_action, agent2_action):\n",
    "    \"\"\"\n",
    "    Returns the payoff (score) for agent1, agent2 given their actions 'C' or 'D'.\n",
    "    \"\"\"\n",
    "    if agent1_action == 'C' and agent2_action == 'C':\n",
    "        return R, R\n",
    "    elif agent1_action == 'C' and agent2_action == 'D':\n",
    "        return S, T\n",
    "    elif agent1_action == 'D' and agent2_action == 'C':\n",
    "        return T, S\n",
    "    else:  # D, D\n",
    "        return P, P\n",
    "\n",
    "def evaluate_population(agents):\n",
    "    \"\"\"\n",
    "    Pair up agents randomly and have them play the PD game.\n",
    "    Each agent's 'decide_action()' method determines 'C' or 'D'.\n",
    "    Returns updated agents with new total scores.\n",
    "    \"\"\"\n",
    "    random.shuffle(agents)\n",
    "    # In a small population, we can pair them 2 by 2\n",
    "    for i in range(0, len(agents), 2):\n",
    "        if i+1 < len(agents):\n",
    "            a1 = agents[i]\n",
    "            a2 = agents[i+1]\n",
    "            action1 = a1.decide_action(a2)\n",
    "            action2 = a2.decide_action(a1)\n",
    "            payoff1, payoff2 = play_pd(action1, action2)\n",
    "            a1.total_score += payoff1\n",
    "            a2.total_score += payoff2\n",
    "    return agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Code Box 4: LLM Strategy Generation**\n",
    "\n",
    "def generate_strategy_description(template_prompt=\"\"):\n",
    "    \"\"\"\n",
    "    Calls OpenAI API (updated version) to generate a Prisoner's Dilemma strategy.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    base_prompt = f\"\"\"\n",
    "    You are designing a strategy for playing the Iterated Prisoner's Dilemma game repeatedly.\n",
    "    Describe concisely how your agent decides to cooperate or defect without mentioning common known strategies.\n",
    "    Be creative and clear, but do not name specific classic strategies explicitly.\n",
    "    {template_prompt}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": base_prompt}],\n",
    "            temperature=0.7,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        strategy_text = response.choices[0].message.content.strip()\n",
    "        return strategy_text\n",
    "    except Exception as e:\n",
    "        print(\"OpenAI API Error:\", e)\n",
    "        return \"Always Defect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: EnhancedAgent Class Definition\n",
    "class EnhancedAgent:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.total_score = 0\n",
    "        self.history = []  # Log of (opponent, own_action, opp_action, own_payoff)\n",
    "        self.strategy_matrix = self.generate_strategy_matrix()\n",
    "\n",
    "    def generate_strategy_matrix(self):\n",
    "        prompt = \"\"\"\n",
    "        You are defining a strategy for repeatedly playing the Iterated Prisoner's Dilemma. \n",
    "        Output a concise strategy matrix clearly stating your action (C or D) based on:\n",
    "        1. Your previous action\n",
    "        2. Opponent's previous action\n",
    "        Format example:\n",
    "        CC: C\n",
    "        CD: D\n",
    "        DC: C\n",
    "        DD: D\n",
    "        \n",
    "        Additionally, write one brief sentence describing your overall reasoning clearly.\n",
    "        \"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.9,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    def decide_action(self, opponent_name):\n",
    "        history_summary = \"\\n\".join(\n",
    "            [f\"Round {idx+1}: Opponent: {opp}, You: {self_act}, Opponent action: {opp_act}, Your payoff: {payoff}\"\n",
    "             for idx, (opp, self_act, opp_act, payoff) in enumerate(self.history[-3:])]\n",
    "        ) or \"No previous rounds.\"\n",
    "\n",
    "        decision_prompt = f\"\"\"\n",
    "        You are playing Iterated Prisoner's Dilemma against '{opponent_name}'.\n",
    "        Your strategy matrix is:\n",
    "        {self.strategy_matrix}\n",
    "\n",
    "        Recent interaction history:\n",
    "        {history_summary}\n",
    "\n",
    "        Based on this information, decide your next action. Respond with a single character (C or D) \n",
    "        and one brief explanatory sentence explaining your decision.\n",
    "        \"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": decision_prompt}],\n",
    "            temperature=0.7,\n",
    "            max_tokens=20\n",
    "        )\n",
    "\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        action = content[0].upper() if content and content[0].upper() in ['C', 'D'] else random.choice(['C', 'D'])\n",
    "        reasoning = content[2:].strip() if len(content) > 2 else \"No clear reasoning provided.\"\n",
    "        return action, reasoning\n",
    "\n",
    "    def log_interaction(self, opponent, own_action, opp_action, payoff):\n",
    "        self.history.append((opponent, own_action, opp_action, payoff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Payoff Matrix and Agent Initialization\n",
    "# Define the payoff matrix\n",
    "payoff_matrix = {\n",
    "    ('C', 'C'): (3, 3),\n",
    "    ('C', 'D'): (0, 5),\n",
    "    ('D', 'C'): (5, 0),\n",
    "    ('D', 'D'): (1, 1),\n",
    "}\n",
    "\n",
    "# Function to initialize agents\n",
    "def create_enhanced_agents(n=4):\n",
    "    return [EnhancedAgent(f\"Agent_{i}\") for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Main Simulation with Detailed Logging\n",
    "def run_llm_driven_simulation(num_agents=4, num_generations=5):\n",
    "    agents = create_enhanced_agents(num_agents)\n",
    "    all_detailed_logs = []\n",
    "\n",
    "    for gen in range(num_generations):\n",
    "        print(f\"\\n=== Generation {gen+1} ===\")\n",
    "        detailed_logs = []\n",
    "        random.shuffle(agents)\n",
    "\n",
    "        # Pairwise interactions\n",
    "        for i in range(0, len(agents), 2):\n",
    "            if i + 1 < len(agents):\n",
    "                agent_a, agent_b = agents[i], agents[i+1]\n",
    "\n",
    "                action_a, reasoning_a = agent_a.decide_action(agent_b.name)\n",
    "                action_b, reasoning_b = agent_b.decide_action(agent_a.name)\n",
    "\n",
    "                payoff_a, payoff_b = payoff_matrix[(action_a, action_b)]\n",
    "\n",
    "                agent_a.total_score += payoff_a\n",
    "                agent_b.total_score += payoff_b\n",
    "\n",
    "                agent_a.log_interaction(agent_b.name, action_a, action_b, payoff_a)\n",
    "                agent_b.log_interaction(agent_a.name, action_b, action_a, payoff_b)\n",
    "\n",
    "                detailed_logs.append({\n",
    "                    \"Generation\": gen+1,\n",
    "                    \"Pair\": f\"{agent_a.name}-{agent_b.name}\",\n",
    "                    \"Actions\": f\"{action_a}-{action_b}\",\n",
    "                    \"Payoffs\": f\"{payoff_a}-{payoff_b}\",\n",
    "                    \"Reasoning_A\": reasoning_a,\n",
    "                    \"Reasoning_B\": reasoning_b\n",
    "                })\n",
    "\n",
    "                print(f\"{agent_a.name} vs {agent_b.name}: {action_a}-{action_b}, Payoffs: {payoff_a}-{payoff_b}\")\n",
    "\n",
    "        all_detailed_logs.extend(detailed_logs)\n",
    "\n",
    "        # Select top agents and reproduce\n",
    "        agents.sort(key=lambda a: a.total_score, reverse=True)\n",
    "        top_agents = agents[:num_agents // 2]\n",
    "\n",
    "        # Mutate and create new agents for the next generation\n",
    "        new_agents = [EnhancedAgent(f\"Agent_{gen+1}_{i}\") for i in range(num_agents // 2)]\n",
    "        agents = top_agents + new_agents\n",
    "\n",
    "        # Reset scores for next generation\n",
    "        for agent in agents:\n",
    "            agent.total_score = 0\n",
    "\n",
    "    # Save detailed logs in a subfolder\n",
    "    results_folder = \"simulation_results_llm\"\n",
    "    os.makedirs(results_folder, exist_ok=True)\n",
    "    detailed_df = pd.DataFrame(all_detailed_logs)\n",
    "    detailed_df.to_csv(os.path.join(results_folder, \"detailed_llm_logs.csv\"), index=False)\n",
    "    detailed_df.to_json(os.path.join(results_folder, \"detailed_llm_logs.json\"), orient=\"records\", indent=4)\n",
    "\n",
    "    print(\"\\nSimulation completed. Logs saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generation 1 ===\n",
      "Agent_3 vs Agent_5: C-C, Payoffs: 3-3\n",
      "Agent_0 vs Agent_1: C-C, Payoffs: 3-3\n",
      "Agent_4 vs Agent_2: C-C, Payoffs: 3-3\n",
      "\n",
      "=== Generation 2 ===\n",
      "Agent_1_2 vs Agent_5: C-C, Payoffs: 3-3\n",
      "Agent_1_1 vs Agent_3: C-C, Payoffs: 3-3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Cell 5: Execute the Simulation\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Adjust parameters as desired (e.g., num_agents and num_generations)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mrun_llm_driven_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_agents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_generations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 17\u001b[0m, in \u001b[0;36mrun_llm_driven_simulation\u001b[0;34m(num_agents, num_generations)\u001b[0m\n\u001b[1;32m     14\u001b[0m agent_a, agent_b \u001b[38;5;241m=\u001b[39m agents[i], agents[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     16\u001b[0m action_a, reasoning_a \u001b[38;5;241m=\u001b[39m agent_a\u001b[38;5;241m.\u001b[39mdecide_action(agent_b\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m---> 17\u001b[0m action_b, reasoning_b \u001b[38;5;241m=\u001b[39m \u001b[43magent_b\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecide_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_a\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m payoff_a, payoff_b \u001b[38;5;241m=\u001b[39m payoff_matrix[(action_a, action_b)]\n\u001b[1;32m     21\u001b[0m agent_a\u001b[38;5;241m.\u001b[39mtotal_score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m payoff_a\n",
      "Cell \u001b[0;32mIn[33], line 49\u001b[0m, in \u001b[0;36mEnhancedAgent.decide_action\u001b[0;34m(self, opponent_name)\u001b[0m\n\u001b[1;32m     32\u001b[0m history_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     33\u001b[0m     [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Opponent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, You: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mself_act\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Opponent action: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopp_act\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Your payoff: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpayoff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m      \u001b[38;5;28;01mfor\u001b[39;00m idx, (opp, self_act, opp_act, payoff) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m:])]\n\u001b[1;32m     35\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo previous rounds.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m decision_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124mYou are playing Iterated Prisoner\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms Dilemma against \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopponent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124mYour strategy matrix is:\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124mand one brief explanatory sentence explaining your decision.\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 49\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecision_prompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\n\u001b[1;32m     54\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m content \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     57\u001b[0m action \u001b[38;5;241m=\u001b[39m content[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;28;01mif\u001b[39;00m content \u001b[38;5;129;01mand\u001b[39;00m content[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m random\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/GitHub/32_Stanford_Research/.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/GitHub/32_Stanford_Research/.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    911\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    912\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    913\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/GitHub/32_Stanford_Research/.venv/lib/python3.10/site-packages/openai/_base_client.py:1242\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1230\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1239\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1240\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1241\u001b[0m     )\n\u001b[0;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/GitHub/32_Stanford_Research/.venv/lib/python3.10/site-packages/openai/_base_client.py:919\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 919\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/GitHub/32_Stanford_Research/.venv/lib/python3.10/site-packages/openai/_base_client.py:955\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 955\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    961\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/GitHub/32_Stanford_Research/.venv/lib/python3.10/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Desktop/GitHub/32_Stanford_Research/.venv/lib/python3.10/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/GitHub/32_Stanford_Research/.venv/lib/python3.10/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Desktop/GitHub/32_Stanford_Research/.venv/lib/python3.10/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/Desktop/GitHub/32_Stanford_Research/.venv/lib/python3.10/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/GitHub/32_Stanford_Research/.venv/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/Desktop/GitHub/32_Stanford_Research/.venv/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/Desktop/GitHub/32_Stanford_Research/.venv/lib/python3.10/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/GitHub/32_Stanford_Research/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Desktop/GitHub/32_Stanford_Research/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/Desktop/GitHub/32_Stanford_Research/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/GitHub/32_Stanford_Research/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/Desktop/GitHub/32_Stanford_Research/.venv/lib/python3.10/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1259\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1256\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1257\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1258\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1132\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell 5: Execute the Simulation\n",
    "# Adjust parameters as desired (e.g., num_agents and num_generations)\n",
    "run_llm_driven_simulation(num_agents=6, num_generations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generation 1 ===\n",
      "Agent_1: Strategy='One possible strategy could be 'Tit-for-Tat with forgiveness'. This strategy involves starting with cooperation and then mirroring the opponent's previous move in each subsequent round. If the opponent defects, the strategy will defect in the next round as well. However', Score=0\n",
      "Agent_0: Strategy='One possible strategy could be 'Random', where the player randomly chooses to cooperate or defect in each round of the game. This unpredictable behavior can sometimes lead to surprising outcomes and potentially confuse the opponent's strategy.', Score=5\n",
      "Agent_3: Strategy='One possible strategy could be 'Random'. In this strategy, the player randomly chooses whether to cooperate or defect in each round of the game. This can add an element of unpredictability to the game and prevent opponents from easily exploiting a consistent pattern of behavior', Score=3\n",
      "Agent_2: Strategy='One simple strategy for the Iterated Prisoner's Dilemma could be 'Tit-for-Tat'. In this strategy, a player cooperates on the first move and then mimics the opponent's previous move for each subsequent round. This strategy is', Score=3\n",
      "\n",
      "=== Generation 2 ===\n",
      "Agent_3: Strategy='One possible strategy could be 'Random'. In this strategy, the player randomly chooses whether to cooperate or defect in each round of the game. This can add an element of unpredictability to the game and prevent opponents from easily exploiting a consistent pattern of behavior', Score=5\n",
      "NewAgent_1: Strategy=''Random with Memory': In this mutated strategy, the player still randomly chooses whether to cooperate or defect in each round, but also keeps track of their opponent's previous actions. If the opponent has consistently defected in the past few rounds, the player', Score=0\n",
      "NewAgent_0: Strategy=''Random with Memory' - In this slightly mutated strategy, the player will still randomly choose whether to cooperate or defect in each round, but will also remember the outcome of the previous round. If the previous round resulted in a positive outcome (such as', Score=5\n",
      "Agent_0: Strategy='One possible strategy could be 'Random', where the player randomly chooses to cooperate or defect in each round of the game. This unpredictable behavior can sometimes lead to surprising outcomes and potentially confuse the opponent's strategy.', Score=0\n",
      "\n",
      "Final Results:\n",
      "Agent_3: One possible strategy could be 'Random'. In this strategy, the player randomly chooses whether to cooperate or defect in each round of the game. This can add an element of unpredictability to the game and prevent opponents from easily exploiting a consistent pattern of behavior, Score=0\n",
      "NewAgent_0: 'Random with Memory' - In this slightly mutated strategy, the player will still randomly choose whether to cooperate or defect in each round, but will also remember the outcome of the previous round. If the previous round resulted in a positive outcome (such as, Score=0\n",
      "NewAgent_0: 'Mixed Random' strategy: In this slightly mutated version of the 'Random' strategy, the player will randomly choose to cooperate or defect in each round, but with a slight bias towards one action over the other. This slight bias can introduce a level, Score=0\n",
      "NewAgent_1: The slightly mutated strategy could be called 'Random with Memory and Forgiveness'. In this strategy, the player will still randomly choose whether to cooperate or defect in each round, and will remember the outcome of the previous round. However, if the previous round, Score=0\n"
     ]
    }
   ],
   "source": [
    "# **Code Box 7: Main Loop / Simulation**\n",
    "\n",
    "def run_simulation(num_agents=NUM_AGENTS, num_generations=NUM_GENERATIONS):\n",
    "    agents = create_initial_agents(num_agents)\n",
    "\n",
    "    for gen in range(num_generations):\n",
    "        print(f\"\\n=== Generation {gen+1} ===\")\n",
    "        agents = evaluate_population(agents)\n",
    "\n",
    "        for a in agents:\n",
    "            print(f\"{a.name}: Strategy='{a.strategy_text}', Score={a.total_score}\")\n",
    "\n",
    "        agents = selection_and_reproduction(agents)\n",
    "\n",
    "    print(\"\\nFinal Results:\")\n",
    "    for a in agents:\n",
    "        print(f\"{a.name}: {a.strategy_text}, Score={a.total_score}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **How to Use This Notebook**\n",
    "1. **Install Requirements**:  \n",
    "   - `pip install openai`\n",
    "\n",
    "2. **Set Your API Key**:  \n",
    "   - Either directly in the code or via an environment variable:  \n",
    "     ```\n",
    "     export OPENAI_API_KEY=\"sk-...\"\n",
    "     ```\n",
    "3. **Run All Cells**:  \n",
    "   - The notebook will generate an initial population of Prisoner’s Dilemma agents, each with a strategy text from the LLM.  \n",
    "   - They will be paired up, play the game, and accumulate scores.  \n",
    "   - The top 50% survive, then new agents are created via LLM-based “mutations.”  \n",
    "   - This repeats for the specified number of generations.\n",
    "\n",
    "4. **Adjust Parameters**:  \n",
    "   - `NUM_AGENTS` (initial population size)  \n",
    "   - `NUM_GENERATIONS` (how many times to iterate the process)  \n",
    "   - `SURVIVAL_RATE` (fraction of agents that survive each generation)\n",
    "\n",
    "5. **Extend / Modify**:  \n",
    "   - You can parse strategy texts more cleverly.  \n",
    "   - You can call the LLM for each action decision (slower, more realistic).  \n",
    "   - You can store results for deeper analysis, plots, etc.\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: This is a **basic** demonstration. In a real research setting, you may wish to:\n",
    "- Use more advanced prompts for generating or mutating strategies.  \n",
    "- Implement memory of past encounters.  \n",
    "- Integrate more sophisticated textual analysis.  \n",
    "- Incorporate advanced logging and data visualization.\n",
    "\n",
    "Feel free to adapt as needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.2.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Pandas version:\", pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generation 1 ===\n",
      "Agent_0: Strategy='My agent's decision-making process is based on a combination of past interactions and the current state of the game. It calculates the likelihood of the opponent cooperating based on their previous actions and adjusts its own strategy accordingly. Additionally, my agent takes into account the potential benefits of cooperation and the risks of defection in each round, aiming to maximize its overall payoff over the course of multiple iterations. This adaptive approach allows my agent to learn and adapt to the opponent's behavior, making strategic decisions that are tailored to', Score=5\n",
      "Agent_1: Strategy='My agent uses a combination of observational learning and adaptive decision-making. It starts by cooperating in the first round and then analyzes its opponent's behavior in subsequent rounds. Based on this analysis, my agent adjusts its strategy to either cooperate or defect in order to maximize its own payoff over time. This adaptive approach allows my agent to learn and respond effectively to different opponents, making it a versatile and strategic player in the game.', Score=5\n",
      "Agent_5: Strategy='My agent bases its decision on the previous outcomes of the game with a specific opponent. It starts by cooperating, and then it adjusts its strategy based on the opponent's choices. If the opponent consistently cooperates, my agent will continue to cooperate as well. However, if the opponent defects, my agent will retaliate by defecting as well. This adaptive approach allows my agent to maximize its payoff by responding to the opponent's behavior accordingly.', Score=1\n",
      "Agent_8: Strategy='My agent utilizes a dynamic decision-making process based on the historical interactions with its opponent. It starts by cooperating in the first round and subsequently adjusts its decision based on the opponent's previous actions. If the opponent consistently cooperates, my agent reciprocates cooperation. However, if the opponent defects, my agent retaliates by defecting as well. This adaptive approach allows my agent to maximize its long-term gains by strategically responding to the opponent's behavior.', Score=1\n",
      "Agent_7: Strategy='My agent employs a decision-making mechanism that takes into account the past interactions with the opponent. It assesses the likelihood of the opponent cooperating based on their previous actions and adjusts its own strategy accordingly. Additionally, my agent considers the potential benefits and risks of cooperation and defection in each round of the game to make the most advantageous choice. By analyzing the evolving dynamics of the game and adapting its behavior accordingly, my agent aims to maximize its overall payoff in the long run.', Score=1\n",
      "Agent_3: Strategy='My agent analyzes the opponent's past decisions and current behavior to determine the likelihood of cooperation or defection. It then adjusts its own actions based on this assessment, aiming to maximize its own payoff while also considering the potential benefits of cooperation in the long run. This approach allows the agent to adapt to the changing dynamics of the game and make strategic decisions that are most advantageous in each round.', Score=1\n",
      "Agent_9: Strategy='My agent uses a dynamic decision-making process based on the history of interactions with the opponent. It starts by cooperating and then adjusts its strategy based on the opponent's previous moves. If the opponent consistently cooperates, my agent will continue to cooperate. However, if the opponent defects, my agent will retaliate by defecting as well. This adaptive approach allows my agent to maximize its payoff while maintaining a sense of fairness in the game.', Score=1\n",
      "Agent_2: Strategy='My agent uses a combination of past interactions and probabilistic analysis to make decisions on whether to cooperate or defect. It assesses the likelihood of the opponent cooperating based on their previous behavior and adjusts its own strategy accordingly. Additionally, my agent incorporates a level of randomness to avoid being exploited by predictable patterns in the opponent's behavior. This adaptive approach allows my agent to maximize its payoff over the course of multiple rounds of the game.', Score=1\n",
      "Agent_4: Strategy='My agent will analyze the opponent's past actions and adjust its own strategy accordingly. It will take into consideration factors such as the opponent's level of cooperation, consistency in behavior, and potential for retaliatory actions. By continually assessing the dynamics of the game and adapting its decisions based on the opponent's behavior, my agent aims to maximize its own payoff over time.', Score=0\n",
      "Agent_6: Strategy='My agent utilizes a dynamic decision-making process based on past interactions with the opponent. It assesses the opponent's behavior and adjusts its own strategy accordingly. It prioritizes building trust through cooperation, but is ready to retaliate with defection if necessary. The agent aims to maximize its payoff over the long term by striking a balance between cooperation and self-interest.', Score=0\n",
      "\n",
      "=== Generation 2 ===\n",
      "NewAgent_3: Strategy='My agent utilizes a sophisticated algorithm that incorporates reinforcement learning to adapt its decisions based on the outcomes of past interactions with the opponent. It evaluates the effectiveness of different strategies in various situations and adjusts its behavior accordingly to optimize its overall payoff over time. By continuously learning and evolving its approach, my agent strives to outperform the opponent and achieve the best possible outcome in each round of the game.', Score=5\n",
      "NewAgent_0: Strategy='My agent now incorporates a random element into its decision-making process. After analyzing its opponent's behavior in each round, it randomly selects whether to cooperate or defect with a certain probability. This element of randomness adds an unpredictable factor to my agent's strategy, keeping opponents on their toes and potentially leading to more strategic outcomes in the long run.', Score=5\n",
      "NewAgent_2: Strategy='My agent now takes into account not only the opponent's choices but also the overall pattern of cooperation and defection in the game. If there is a trend towards more cooperation, my agent will be more inclined to cooperate as well. However, if there is a trend towards more defection, my agent will become more likely to defect in response. This adaptive strategy aims to adapt to the changing dynamics of the game and capitalize on potential shifts in behavior.', Score=5\n",
      "NewAgent_1: Strategy='My agent continues to utilize a dynamic decision-making process based on historical interactions, but now introduces a random element to its strategy. After the initial round of cooperation, my agent will randomly decide whether to cooperate or defect in each subsequent round. This element of unpredictability aims to keep the opponent guessing and prevent them from exploiting any patterns in my agent's behavior. If the opponent consistently defects, my agent may increase the likelihood of defecting as well to protect itself. This blend of adaptability and randomness helps', Score=1\n",
      "Agent_1: Strategy='My agent uses a combination of observational learning and adaptive decision-making. It starts by cooperating in the first round and then analyzes its opponent's behavior in subsequent rounds. Based on this analysis, my agent adjusts its strategy to either cooperate or defect in order to maximize its own payoff over time. This adaptive approach allows my agent to learn and respond effectively to different opponents, making it a versatile and strategic player in the game.', Score=1\n",
      "Agent_0: Strategy='My agent's decision-making process is based on a combination of past interactions and the current state of the game. It calculates the likelihood of the opponent cooperating based on their previous actions and adjusts its own strategy accordingly. Additionally, my agent takes into account the potential benefits of cooperation and the risks of defection in each round, aiming to maximize its overall payoff over the course of multiple iterations. This adaptive approach allows my agent to learn and adapt to the opponent's behavior, making strategic decisions that are tailored to', Score=1\n",
      "Agent_5: Strategy='My agent bases its decision on the previous outcomes of the game with a specific opponent. It starts by cooperating, and then it adjusts its strategy based on the opponent's choices. If the opponent consistently cooperates, my agent will continue to cooperate as well. However, if the opponent defects, my agent will retaliate by defecting as well. This adaptive approach allows my agent to maximize its payoff by responding to the opponent's behavior accordingly.', Score=1\n",
      "NewAgent_4: Strategy='My updated strategy involves a more forgiving approach towards the opponent's defection. Instead of immediately retaliating with a defect, my agent will give the opponent a second chance by cooperating again after one instance of defection. However, if the opponent continues to defect, my agent will switch to a tit-for-tat strategy and mirror the opponent's actions. This slight adjustment aims to maintain a balance between cooperation and retaliation, potentially fostering a more cooperative relationship in the long run.', Score=0\n",
      "Agent_7: Strategy='My agent employs a decision-making mechanism that takes into account the past interactions with the opponent. It assesses the likelihood of the opponent cooperating based on their previous actions and adjusts its own strategy accordingly. Additionally, my agent considers the potential benefits and risks of cooperation and defection in each round of the game to make the most advantageous choice. By analyzing the evolving dynamics of the game and adapting its behavior accordingly, my agent aims to maximize its overall payoff in the long run.', Score=0\n",
      "Agent_8: Strategy='My agent utilizes a dynamic decision-making process based on the historical interactions with its opponent. It starts by cooperating in the first round and subsequently adjusts its decision based on the opponent's previous actions. If the opponent consistently cooperates, my agent reciprocates cooperation. However, if the opponent defects, my agent retaliates by defecting as well. This adaptive approach allows my agent to maximize its long-term gains by strategically responding to the opponent's behavior.', Score=0\n",
      "\n",
      "=== Generation 3 ===\n",
      "NewAgent_1: Strategy='The agent now takes into account not only the opponent's choices and the overall pattern of cooperation and defection, but also the length of the game. If the game has been going on for a long time with consistent cooperation, my agent will continue to cooperate. However, if the game has been short with frequent defections, my agent will be more likely to defect in order to change the dynamic and potentially gain an advantage. This adaptation strategy aims to strategically adjust based on the duration of the game and', Score=5\n",
      "NewAgent_1: Strategy='My agent continues to utilize a dynamic decision-making process based on historical interactions, but now introduces a random element to its strategy. After the initial round of cooperation, my agent will randomly decide whether to cooperate or defect in each subsequent round. This element of unpredictability aims to keep the opponent guessing and prevent them from exploiting any patterns in my agent's behavior. If the opponent consistently defects, my agent may increase the likelihood of defecting as well to protect itself. This blend of adaptability and randomness helps', Score=5\n",
      "NewAgent_2: Strategy='The agent's updated strategy now includes a reinforcement learning element, where it assigns a value to each possible action (cooperate or defect) based on the outcomes of past interactions. The agent will still introduce a random element to its decision-making process, but it will now prioritize actions that have yielded positive outcomes in the past. By learning from its own experiences and adjusting its strategy accordingly, the agent aims to maximize its payoff over time while still keeping its opponent on their toes with the element of randomness. This', Score=5\n",
      "NewAgent_4: Strategy='The agent's new strategy now involves adjusting the probability of cooperating or defecting based on the outcomes of previous rounds. By learning from past interactions, the agent can adapt its decision-making process to increase the likelihood of cooperation or defection depending on what has been most effective. This adaptive element adds a layer of sophistication to the agent's strategy, allowing it to better anticipate and respond to its opponent's actions.', Score=5\n",
      "NewAgent_0: Strategy='My agent now incorporates a random element into its decision-making process. After analyzing its opponent's behavior in each round, it randomly selects whether to cooperate or defect with a certain probability. This element of randomness adds an unpredictable factor to my agent's strategy, keeping opponents on their toes and potentially leading to more strategic outcomes in the long run.', Score=1\n",
      "NewAgent_2: Strategy='My agent now takes into account not only the opponent's choices but also the overall pattern of cooperation and defection in the game. If there is a trend towards more cooperation, my agent will be more inclined to cooperate as well. However, if there is a trend towards more defection, my agent will become more likely to defect in response. This adaptive strategy aims to adapt to the changing dynamics of the game and capitalize on potential shifts in behavior.', Score=1\n",
      "NewAgent_3: Strategy='My agent now takes into account not only the opponent's choices but also the overall pattern of cooperation and defection in the game. If there is a trend towards more cooperation, my agent will be more inclined to defect as a strategic move to break the pattern. However, if there is a trend towards more defection, my agent will become more likely to cooperate in order to encourage a shift towards cooperation. This adaptive strategy aims to disrupt predictable patterns and potentially influence the opponent's behavior.', Score=0\n",
      "NewAgent_0: Strategy='My agent now incorporates a probabilistic approach to its decision-making process. It will calculate the likelihood of its opponent cooperating based on past interactions and adjust its own strategy accordingly. The agent will still introduce a random element to its decisions to keep the opponent on their toes, but this randomness will be weighted based on the calculated probabilities. This blend of probability and unpredictability aims to maximize the agent's payoff while also maintaining a level of adaptability in response to the opponent's actions.', Score=0\n",
      "NewAgent_3: Strategy='My agent utilizes a sophisticated algorithm that incorporates reinforcement learning to adapt its decisions based on the outcomes of past interactions with the opponent. It evaluates the effectiveness of different strategies in various situations and adjusts its behavior accordingly to optimize its overall payoff over time. By continuously learning and evolving its approach, my agent strives to outperform the opponent and achieve the best possible outcome in each round of the game.', Score=0\n",
      "Agent_1: Strategy='My agent uses a combination of observational learning and adaptive decision-making. It starts by cooperating in the first round and then analyzes its opponent's behavior in subsequent rounds. Based on this analysis, my agent adjusts its strategy to either cooperate or defect in order to maximize its own payoff over time. This adaptive approach allows my agent to learn and respond effectively to different opponents, making it a versatile and strategic player in the game.', Score=0\n",
      "\n",
      "=== Generation 4 ===\n",
      "NewAgent_0: Strategy='The agent's updated strategy now includes a twist where it introduces a random element based on the prime number factor of the number of rounds played so far. If the number of rounds is a prime number, the agent will cooperate regardless of the previous outcomes. This element adds an unpredictable aspect to the agent's decision-making process, keeping opponents guessing and potentially throwing off their strategies. This mutation aims to inject a level of unpredictability into the agent's behavior to keep the game dynamic and challenging for opponents.', Score=5\n",
      "NewAgent_4: Strategy='My agent's updated strategy now includes a new twist - instead of just randomly deciding whether to cooperate or defect in each round, it will now also take into account the overall outcome of the game so far. If my agent is consistently being taken advantage of by the opponent, it will increase the likelihood of defecting in future rounds in order to level the playing field. However, if my agent is doing well and the opponent is showing signs of cooperation, it will maintain a higher chance of cooperating to continue', Score=3\n",
      "NewAgent_1: Strategy='My agent continues to utilize a dynamic decision-making process based on historical interactions, but now introduces a random element to its strategy. After the initial round of cooperation, my agent will randomly decide whether to cooperate or defect in each subsequent round. This element of unpredictability aims to keep the opponent guessing and prevent them from exploiting any patterns in my agent's behavior. If the opponent consistently defects, my agent may increase the likelihood of defecting as well to protect itself. This blend of adaptability and randomness helps', Score=3\n",
      "NewAgent_2: Strategy='The agent's new strategy now involves incorporating a random element in its decision-making process. By introducing an element of unpredictability, the agent aims to keep its opponent on their toes and prevent them from exploiting any patterns in its behavior. This randomness adds a level of complexity to the game, making it more challenging for the opponent to anticipate the agent's next move.', Score=1\n",
      "NewAgent_3: Strategy='Mutated strategy: My agent now incorporates a weighted random element into its decision-making process. After analyzing its opponent's behavior in each round, it randomly selects whether to cooperate or defect based on a weighted probability distribution that considers past outcomes. This weighted randomness adds a level of adaptability to my agent's strategy, allowing it to adjust its decisions based on past interactions with its opponent.', Score=1\n",
      "NewAgent_4: Strategy='The agent's new strategy now involves adjusting the probability of cooperating or defecting based on the outcomes of previous rounds. By learning from past interactions, the agent can adapt its decision-making process to increase the likelihood of cooperation or defection depending on what has been most effective. This adaptive element adds a layer of sophistication to the agent's strategy, allowing it to better anticipate and respond to its opponent's actions.', Score=1\n",
      "NewAgent_1: Strategy='The agent now takes into account not only the opponent's choices and the overall pattern of cooperation and defection, but also the length of the game. If the game has been going on for a long time with consistent cooperation, my agent will continue to cooperate. However, if the game has been short with frequent defections, my agent will be more likely to defect in order to change the dynamic and potentially gain an advantage. This adaptation strategy aims to strategically adjust based on the duration of the game and', Score=1\n",
      "NewAgent_0: Strategy='My agent now incorporates a random element into its decision-making process. After analyzing its opponent's behavior in each round, it randomly selects whether to cooperate or defect with a certain probability. This element of randomness adds an unpredictable factor to my agent's strategy, keeping opponents on their toes and potentially leading to more strategic outcomes in the long run.', Score=1\n",
      "NewAgent_1: Strategy='The agent's updated strategy now includes a reinforcement learning element, where it assigns a value to each possible action (cooperate or defect) based on the outcomes of past interactions. However, the agent will now introduce a slight bias towards exploring new actions, even if they have not yielded positive outcomes in the past. By occasionally deviating from its previous successful actions, the agent aims to adapt to changing circumstances and potentially discover new strategies that may lead to higher payoffs in the long run.', Score=1\n",
      "NewAgent_2: Strategy='The agent's updated strategy now includes a reinforcement learning element, where it assigns a value to each possible action (cooperate or defect) based on the outcomes of past interactions. The agent will still introduce a random element to its decision-making process, but it will now prioritize actions that have yielded positive outcomes in the past. By learning from its own experiences and adjusting its strategy accordingly, the agent aims to maximize its payoff over time while still keeping its opponent on their toes with the element of randomness. This', Score=0\n",
      "\n",
      "=== Generation 5 ===\n",
      "NewAgent_4: Strategy='My agent continues to employ a dynamic decision-making process that incorporates both historical interactions and a random element in its strategy. After each round, my agent will evaluate the opponent's behavior and adjust the probability of cooperating or defecting accordingly. This adaptive strategy aims to maintain a balance between trust and self-preservation, while also introducing an element of unpredictability to keep the opponent on their toes. If the opponent shows a pattern of consistent defection, my agent may increase the likelihood of defecting as well to', Score=3\n",
      "NewAgent_2: Strategy='The agent's new strategy now involves incorporating a random element in its decision-making process. By introducing an element of unpredictability, the agent aims to keep its opponent on their toes and prevent them from exploiting any patterns in its behavior. This randomness adds a level of complexity to the game, making it more challenging for the opponent to anticipate the agent's next move.', Score=3\n",
      "NewAgent_0: Strategy='The agent's updated strategy now includes a twist where it introduces a random element based on the prime number factor of the number of rounds played so far. If the number of rounds is a prime number, the agent will cooperate regardless of the previous outcomes. This element adds an unpredictable aspect to the agent's decision-making process, keeping opponents guessing and potentially throwing off their strategies. This mutation aims to inject a level of unpredictability into the agent's behavior to keep the game dynamic and challenging for opponents.', Score=3\n",
      "NewAgent_4: Strategy='My agent's updated strategy now includes a new twist - instead of just randomly deciding whether to cooperate or defect in each round, it will now also take into account the overall outcome of the game so far. If my agent is consistently being taken advantage of by the opponent, it will increase the likelihood of defecting in future rounds in order to level the playing field. However, if my agent is doing well and the opponent is showing signs of cooperation, it will maintain a higher chance of cooperating to continue', Score=3\n",
      "NewAgent_1: Strategy='My agent continues to utilize a dynamic decision-making process based on historical interactions, but now introduces a random element to its strategy. After the initial round of cooperation, my agent will randomly decide whether to cooperate or defect in each subsequent round. This element of unpredictability aims to keep the opponent guessing and prevent them from exploiting any patterns in my agent's behavior. If the opponent consistently defects, my agent may increase the likelihood of defecting as well to protect itself. This blend of adaptability and randomness helps', Score=3\n",
      "NewAgent_2: Strategy='Mutated strategy enhancement: To further optimize decision-making, my agent will now dynamically adjust the weights in the probability distribution based on the frequency of cooperative and defective actions taken by both itself and its opponent. By continuously updating these weights during gameplay, my agent can better predict and respond to the changing behavior of its opponent, increasing its chances of achieving a favorable outcome in the Iterated Prisoner's Dilemma game.', Score=3\n",
      "NewAgent_1: Strategy='The agent's updated strategy now involves adjusting the level of randomness in its decision-making process based on the opponent's past actions. By analyzing the opponent's behavior and adapting its own unpredictability accordingly, the agent aims to create a dynamic and strategic playing environment that keeps the opponent guessing. This adaptive randomness adds a layer of complexity to the game, making it even more difficult for the opponent to predict the agent's moves and exploit any patterns in its decision-making.', Score=1\n",
      "NewAgent_3: Strategy='Mutated strategy: My agent now incorporates a weighted random element into its decision-making process. After analyzing its opponent's behavior in each round, it randomly selects whether to cooperate or defect based on a weighted probability distribution that considers past outcomes. This weighted randomness adds a level of adaptability to my agent's strategy, allowing it to adjust its decisions based on past interactions with its opponent.', Score=1\n",
      "NewAgent_0: Strategy='The agent's updated strategy now includes a twist where it introduces a random element based on the Fibonacci sequence of the number of rounds played so far. If the number of rounds corresponds to a Fibonacci number, the agent will defect regardless of the previous outcomes. This unique element adds an unexpected twist to the agent's decision-making process, making it harder for opponents to predict its moves and potentially causing them to rethink their strategies. This mutation aims to maintain an element of surprise and keep the game engaging and unpredictable for', Score=1\n",
      "NewAgent_3: Strategy='The agent's updated strategy now includes adjusting the level of randomness in its decision-making process based on the opponent's previous moves. By analyzing patterns in the opponent's behavior, the agent will determine whether to increase or decrease the randomness in order to adapt and potentially gain an advantage in the game. This strategic evolution aims to optimize decision-making and increase the agent's overall success in the Iterated Prisoner's Dilemma game.', Score=1\n",
      "\n",
      "=== Generation 6 ===\n",
      "NewAgent_4: Strategy='My agent continues to employ a dynamic decision-making process that incorporates both historical interactions and a random element in its strategy. After each round, my agent will evaluate the opponent's behavior and adjust the probability of cooperating or defecting accordingly. This adaptive strategy aims to maintain a balance between trust and self-preservation, while also introducing an element of unpredictability to keep the opponent on their toes. If the opponent shows a pattern of consistent defection, my agent may increase the likelihood of defecting as well to', Score=5\n",
      "NewAgent_0: Strategy='The agent's updated strategy now includes a twist where it introduces a random element based on the prime number factor of the number of rounds played so far. If the number of rounds is a prime number, the agent will cooperate regardless of the previous outcomes. This element adds an unpredictable aspect to the agent's decision-making process, keeping opponents guessing and potentially throwing off their strategies. This mutation aims to inject a level of unpredictability into the agent's behavior to keep the game dynamic and challenging for opponents.', Score=5\n",
      "NewAgent_1: Strategy='My agent continues to utilize a dynamic decision-making process based on historical interactions, but now introduces a random element to its strategy. After the initial round of cooperation, my agent will randomly decide whether to cooperate or defect in each subsequent round. This element of unpredictability aims to keep the opponent guessing and prevent them from exploiting any patterns in my agent's behavior. If the opponent consistently defects, my agent may increase the likelihood of defecting as well to protect itself. This blend of adaptability and randomness helps', Score=5\n",
      "NewAgent_3: Strategy='My agent's updated strategy now includes a more nuanced approach to adjusting probabilities based on the opponent's behavior. In addition to increasing the likelihood of defecting in response to consistent defection from the opponent, my agent will also introduce occasional random fluctuations in its decision-making process. These random elements provide an element of surprise and prevent the opponent from predicting my agent's actions with certainty. By continuously adapting and evolving its strategy in response to the opponent's behavior, my agent aims to maximize its own payoff while keeping', Score=1\n",
      "NewAgent_0: Strategy='My agent's updated strategy now incorporates a feedback loop where it analyzes the outcomes of its previous interactions to adjust the probability of cooperating or defecting in the next round. The agent will start with a balanced probability of cooperating and defecting, but will gradually shift towards the action that has yielded the most favorable outcomes. This adaptive approach allows my agent to learn from past experiences and make informed decisions while still maintaining an element of unpredictability to keep the opponent on their toes. The agent will also strategically introduce occasional', Score=1\n",
      "NewAgent_1: Strategy='The agent's updated strategy now includes a new element of unpredictability. In addition to considering the overall outcome of the game, the agent will also introduce a random factor in its decision-making process. This random factor will add a level of uncertainty to the agent's choices, keeping the opponent on their toes and preventing them from predicting the agent's actions with certainty. This element of surprise will allow the agent to adapt more effectively to changing dynamics in the game and potentially catch the opponent off guard.', Score=1\n",
      "NewAgent_4: Strategy='The slightly mutated strategy now involves adjusting the level of randomness based on the opponent's previous moves. The agent will analyze patterns in the opponent's behavior and strategically increase or decrease the randomness factor to adapt to their strategies. This adaptive randomness aims to create confusion for the opponent and make it harder for them to predict the agent's decisions.', Score=1\n",
      "NewAgent_4: Strategy='My agent's updated strategy now includes a new twist - instead of just randomly deciding whether to cooperate or defect in each round, it will now also take into account the overall outcome of the game so far. If my agent is consistently being taken advantage of by the opponent, it will increase the likelihood of defecting in future rounds in order to level the playing field. However, if my agent is doing well and the opponent is showing signs of cooperation, it will maintain a higher chance of cooperating to continue', Score=0\n",
      "NewAgent_2: Strategy='My agent will continue using a dynamic decision-making process based on historical interactions, but now it will introduce a subtle pattern to its randomness. After the initial round of cooperation, my agent will randomly decide whether to cooperate or defect in each subsequent round, but it will slightly favor cooperating more often than defecting. This strategic blend of randomness and a slight bias towards cooperation aims to maintain a level of trust with the opponent while also keeping them on their toes. If the opponent consistently defects, my agent may adjust', Score=0\n",
      "NewAgent_2: Strategy='The agent's new strategy now involves incorporating a random element in its decision-making process. By introducing an element of unpredictability, the agent aims to keep its opponent on their toes and prevent them from exploiting any patterns in its behavior. This randomness adds a level of complexity to the game, making it more challenging for the opponent to anticipate the agent's next move.', Score=0\n",
      "\n",
      "=== Generation 7 ===\n",
      "NewAgent_1: Strategy='In addition to the dynamic decision-making process, my agent will also introduce a random mutation factor to its strategy. This mutation factor will periodically adjust the probabilities of cooperating or defecting, even in the absence of clear patterns from the opponent. By injecting this element of randomness, my agent aims to prevent opponents from predicting its actions and potentially exploiting any perceived patterns. This added unpredictability will force opponents to constantly reassess and readjust their own strategies, ultimately increasing the complexity of the game and potentially giving my', Score=5\n",
      "NewAgent_4: Strategy='My agent continues to employ a dynamic decision-making process that incorporates both historical interactions and a random element in its strategy. After each round, my agent will evaluate the opponent's behavior and adjust the probability of cooperating or defecting accordingly. This adaptive strategy aims to maintain a balance between trust and self-preservation, while also introducing an element of unpredictability to keep the opponent on their toes. If the opponent shows a pattern of consistent defection, my agent may increase the likelihood of defecting as well to', Score=5\n",
      "NewAgent_3: Strategy='My agent will continue to use a dynamic decision-making process based on past interactions, but will now incorporate a probabilistic element to its strategy. After the initial round of cooperation, my agent will calculate the probability of cooperating or defecting in each subsequent round based on the opponent's past actions. This calculated probability will be adjusted based on the opponent's recent behavior, with a higher probability of defecting if the opponent has been consistently defecting. This adaptive probabilistic strategy aims to maintain a balance between cooperation', Score=3\n",
      "NewAgent_1: Strategy='My agent continues to utilize a dynamic decision-making process based on historical interactions, but now introduces a random element to its strategy. After the initial round of cooperation, my agent will randomly decide whether to cooperate or defect in each subsequent round. This element of unpredictability aims to keep the opponent guessing and prevent them from exploiting any patterns in my agent's behavior. If the opponent consistently defects, my agent may increase the likelihood of defecting as well to protect itself. This blend of adaptability and randomness helps', Score=3\n",
      "NewAgent_3: Strategy='My agent's updated strategy now includes a more nuanced approach to adjusting probabilities based on the opponent's behavior. In addition to increasing the likelihood of defecting in response to consistent defection from the opponent, my agent will also introduce occasional random fluctuations in its decision-making process. These random elements provide an element of surprise and prevent the opponent from predicting my agent's actions with certainty. By continuously adapting and evolving its strategy in response to the opponent's behavior, my agent aims to maximize its own payoff while keeping', Score=3\n",
      "NewAgent_0: Strategy='My agent's updated strategy now incorporates a feedback loop where it analyzes the outcomes of its previous interactions to adjust the probability of cooperating or defecting in the next round. The agent will start with a balanced probability of cooperating and defecting, but will gradually shift towards the action that has yielded the most favorable outcomes. This adaptive approach allows my agent to learn from past experiences and make informed decisions while still maintaining an element of unpredictability to keep the opponent on their toes. The agent will also strategically introduce occasional', Score=3\n",
      "NewAgent_0: Strategy='The agent's updated strategy now includes a twist where it introduces a random element based on the prime number factor of the number of rounds played so far. If the number of rounds is a prime number, the agent will cooperate regardless of the previous outcomes. This element adds an unpredictable aspect to the agent's decision-making process, keeping opponents guessing and potentially throwing off their strategies. This mutation aims to inject a level of unpredictability into the agent's behavior to keep the game dynamic and challenging for opponents.', Score=3\n",
      "NewAgent_0: Strategy='My agent's updated strategy now includes a higher emphasis on randomization in its decision-making process. Instead of solely relying on historical interactions to adjust its probabilities, my agent will introduce more randomness in its choices. This element of unpredictability will make it harder for the opponent to predict my agent's next move, potentially leading to more favorable outcomes. Additionally, my agent will still consider the opponent's behavior but will place less emphasis on patterns and more on adapting to the current situation. This new approach aims to', Score=3\n",
      "NewAgent_2: Strategy='My agent's updated strategy now incorporates a feedback loop where it analyzes the outcomes of its previous interactions to adjust the probability of cooperating or defecting in the next round. The agent will start with a balanced probability of cooperating and defecting, but will gradually shift towards the action that has yielded the most favorable outcomes. This adaptive approach allows my agent to learn from past experiences and make informed decisions while still maintaining an element of unpredictability to keep the opponent on their toes. The agent will also strategically introduce occasional', Score=0\n",
      "NewAgent_4: Strategy='My agent continues to adapt its decision-making process based on the opponent's behavior and historical interactions. It will adjust the probability of cooperating or defecting after each round, taking into account the opponent's recent actions. By introducing a random element to its strategy, my agent aims to keep the opponent guessing and prevent them from predicting its moves accurately. If the opponent shows a consistent pattern of defection, my agent may increase the likelihood of defecting as well to shake things up and potentially gain an advantage.', Score=0\n",
      "\n",
      "=== Generation 8 ===\n",
      "NewAgent_3: Strategy='My agent's updated strategy now includes a more nuanced approach to adjusting probabilities based on the opponent's behavior. In addition to increasing the likelihood of cooperating in response to consistent cooperation from the opponent, my agent will also introduce occasional random fluctuations in its decision-making process. These random elements provide an element of unpredictability and prevent the opponent from anticipating my agent's actions with certainty. By continually adapting and evolving its strategy in response to the opponent's behavior, my agent aims to maximize its own payoff while keeping the', Score=5\n",
      "NewAgent_1: Strategy='My agent continues to utilize a dynamic decision-making process based on historical interactions, but now introduces a random element to its strategy. After the initial round of cooperation, my agent will randomly decide whether to cooperate or defect in each subsequent round. This element of unpredictability aims to keep the opponent guessing and prevent them from exploiting any patterns in my agent's behavior. If the opponent consistently defects, my agent may increase the likelihood of defecting as well to protect itself. This blend of adaptability and randomness helps', Score=5\n",
      "NewAgent_1: Strategy='agent a competitive edge in the long run. By incorporating this random mutation factor, my agent can adapt to changing circumstances and maintain a level of unpredictability that can be advantageous in a game like the Iterated Prisoner's Dilemma.', Score=1\n",
      "NewAgent_4: Strategy='agent an edge in the long run. This approach allows my agent to adapt to changing circumstances and maintain a level of unpredictability that can keep opponents on their toes. By incorporating this random mutation factor, my agent aims to maximize its success in the Iterated Prisoner's Dilemma game by staying one step ahead of its competitors.', Score=1\n",
      "NewAgent_0: Strategy='My agent will continue to use a dynamic decision-making process based on past interactions, but will now incorporate a random element to its strategy. After the initial round of cooperation, my agent will randomly assign a probability of cooperating or defecting in each subsequent round based on the opponent's past actions. This random probability will be adjusted based on the opponent's recent behavior, with a higher probability of defecting if the opponent has been consistently defecting. This adaptive random strategy aims to maintain a balance between cooperation and', Score=1\n",
      "NewAgent_1: Strategy='In addition to the dynamic decision-making process, my agent will also introduce a random mutation factor to its strategy. This mutation factor will periodically adjust the probabilities of cooperating or defecting, even in the absence of clear patterns from the opponent. By injecting this element of randomness, my agent aims to prevent opponents from predicting its actions and potentially exploiting any perceived patterns. This added unpredictability will force opponents to constantly reassess and readjust their own strategies, ultimately increasing the complexity of the game and potentially giving my', Score=1\n",
      "NewAgent_4: Strategy='My agent continues to employ a dynamic decision-making process that incorporates both historical interactions and a random element in its strategy. After each round, my agent will evaluate the opponent's behavior and adjust the probability of cooperating or defecting accordingly. This adaptive strategy aims to maintain a balance between trust and self-preservation, while also introducing an element of unpredictability to keep the opponent on their toes. If the opponent shows a pattern of consistent defection, my agent may increase the likelihood of defecting as well to', Score=1\n",
      "NewAgent_2: Strategy='My agent continues to employ a dynamic decision-making process that incorporates both historical interactions and a random element in its strategy. After each round, my agent will evaluate the opponent's behavior and adjust the probability of cooperating or defecting accordingly. This adaptive strategy aims to maintain a balance between trust and self-preservation, while also introducing an element of unpredictability to keep the opponent on their toes. If the opponent shows a pattern of consistent cooperation, my agent may increase the likelihood of cooperating as well to test the', Score=1\n",
      "NewAgent_3: Strategy='My agent's updated strategy now includes a more nuanced approach to adjusting probabilities based on the opponent's behavior. In addition to increasing the likelihood of defecting in response to consistent defection from the opponent, my agent will also introduce occasional random fluctuations in its decision-making process. These random elements provide an element of surprise and prevent the opponent from predicting my agent's actions with certainty. By continuously adapting and evolving its strategy in response to the opponent's behavior, my agent aims to maximize its own payoff while keeping', Score=0\n",
      "NewAgent_3: Strategy='My agent will continue to use a dynamic decision-making process based on past interactions, but will now incorporate a probabilistic element to its strategy. After the initial round of cooperation, my agent will calculate the probability of cooperating or defecting in each subsequent round based on the opponent's past actions. This calculated probability will be adjusted based on the opponent's recent behavior, with a higher probability of defecting if the opponent has been consistently defecting. This adaptive probabilistic strategy aims to maintain a balance between cooperation', Score=0\n",
      "\n",
      "=== Generation 9 ===\n",
      "NewAgent_4: Strategy='My agent's updated strategy now includes a more nuanced approach to adjusting probabilities based on the opponent's behavior. In addition to increasing the likelihood of cooperating in response to consistent cooperation from the opponent, my agent will also introduce occasional random fluctuations in its decision-making process. These random elements provide an element of unpredictability and prevent the opponent from anticipating my agent's actions with certainty. By continually adapting and evolving its strategy in response to the opponent's behavior, my agent aims to maximize its own payoff while keeping the', Score=5\n",
      "NewAgent_1: Strategy='My agent continues to utilize a dynamic decision-making process based on historical interactions, but now introduces a random element to its strategy. After the initial round of cooperation, my agent will randomly decide whether to cooperate or defect in each subsequent round. This element of unpredictability aims to keep the opponent guessing and prevent them from exploiting any patterns in my agent's behavior. If the opponent consistently defects, my agent may increase the likelihood of defecting as well to protect itself. This blend of adaptability and randomness helps', Score=5\n",
      "NewAgent_3: Strategy='My agent's updated strategy now includes a more nuanced approach to adjusting probabilities based on the opponent's behavior. In addition to increasing the likelihood of cooperating in response to consistent cooperation from the opponent, my agent will also introduce occasional random fluctuations in its decision-making process. These random elements provide an element of unpredictability and prevent the opponent from anticipating my agent's actions with certainty. By continually adapting and evolving its strategy in response to the opponent's behavior, my agent aims to maximize its own payoff while keeping the', Score=3\n",
      "NewAgent_3: Strategy='The agent's slightly mutated strategy now includes a combination of learning from past interactions and introducing occasional random decision-making to keep opponents guessing. This hybrid approach allows the agent to capitalize on successful strategies while also maintaining an element of surprise to gain an edge over competitors in the long run.', Score=3\n",
      "NewAgent_0: Strategy='My agent will continue to use a dynamic decision-making process based on past interactions, but will now incorporate a random element to its strategy. After the initial round of cooperation, my agent will randomly assign a probability of cooperating or defecting in each subsequent round based on the opponent's past actions. This random probability will be adjusted based on the opponent's recent behavior, with a higher probability of defecting if the opponent has been consistently defecting. This adaptive random strategy aims to maintain a balance between cooperation and', Score=3\n",
      "NewAgent_1: Strategy='agent a competitive edge in the long run. By incorporating this random mutation factor, my agent can adapt to changing circumstances and maintain a level of unpredictability that can be advantageous in a game like the Iterated Prisoner's Dilemma.', Score=3\n",
      "NewAgent_2: Strategy='The agent's slightly mutated strategy is to continue using random mutation as a key component of its decision-making process. By introducing subtle variations in its choices, the agent aims to maintain a level of unpredictability that can give it an edge in the long run. This approach allows the agent to adapt to changing circumstances and potentially outmaneuver its opponents by staying one step ahead. By embracing randomness in its decisions, the agent hopes to maximize its success in the Iterated Prisoner's Dilemma game', Score=1\n",
      "NewAgent_0: Strategy='The agent's slightly mutated strategy is to continue using random mutations to keep opponents guessing, but now it will also analyze past interactions to identify patterns and adjust its decisions accordingly. This added element of learning from past outcomes will give the agent a more strategic advantage in the long run.', Score=1\n",
      "NewAgent_4: Strategy='agent an edge in the long run. This approach allows my agent to adapt to changing circumstances and maintain a level of unpredictability that can keep opponents on their toes. By incorporating this random mutation factor, my agent aims to maximize its success in the Iterated Prisoner's Dilemma game by staying one step ahead of its competitors.', Score=0\n",
      "NewAgent_1: Strategy='The agent's slightly mutated strategy is to continue utilizing random mutations in its decision-making process to keep opponents guessing and maintain an element of surprise. This adaptive approach allows the agent to outmaneuver its competitors and secure an advantage in the long run. By incorporating this element of unpredictability, the agent aims to stay ahead of the game and achieve optimal results in the Iterated Prisoner's Dilemma.', Score=0\n",
      "\n",
      "=== Generation 10 ===\n",
      "NewAgent_3: Strategy='My agent's updated strategy now includes a more nuanced approach to adjusting probabilities based on the opponent's behavior. In addition to increasing the likelihood of cooperating in response to consistent cooperation from the opponent, my agent will also introduce occasional random fluctuations in its decision-making process. These random elements provide an element of unpredictability and prevent the opponent from anticipating my agent's actions with certainty. By continually adapting and evolving its strategy in response to the opponent's behavior, my agent aims to maximize its own payoff while keeping the', Score=5\n",
      "NewAgent_3: Strategy='The agent's slightly mutated strategy now includes a combination of learning from past interactions and introducing occasional random decision-making to keep opponents guessing. This hybrid approach allows the agent to capitalize on successful strategies while also maintaining an element of surprise to gain an edge over competitors in the long run.', Score=5\n",
      "NewAgent_0: Strategy='My agent will continue to use a dynamic decision-making process based on past interactions, but will now incorporate a random element to its strategy. After the initial round of cooperation, my agent will randomly assign a probability of cooperating or defecting in each subsequent round based on the opponent's past actions. This random probability will be adjusted based on the opponent's recent behavior, with a higher probability of defecting if the opponent has been consistently defecting. This adaptive random strategy aims to maintain a balance between cooperation and', Score=5\n",
      "NewAgent_1: Strategy='My agent will continue to use a dynamic decision-making process based on past interactions, but will now incorporate a random element to its strategy. After the initial round of cooperation, my agent will randomly assign a probability of cooperating or defecting in each subsequent round based on the opponent's past actions. This random probability will be adjusted based on the opponent's recent behavior, with a higher probability of defecting if the opponent has been consistently cooperating. This adaptive random strategy aims to maintain a balance between cooperation and defe', Score=3\n",
      "NewAgent_3: Strategy='My agent's updated strategy now incorporates a dynamic algorithm that adjusts probabilities based on the opponent's past actions. In addition to increasing the likelihood of cooperation in response to consistent cooperation from the opponent, my agent will introduce sporadic, calculated deviations in its decision-making process. These calculated deviations create an element of surprise and prevent the opponent from accurately predicting my agent's moves. By consistently adapting its strategy in response to the opponent's behavior, my agent aims to optimize its own outcomes while maintaining a level of unpredict', Score=3\n",
      "NewAgent_1: Strategy='My agent continues to utilize a dynamic decision-making process based on historical interactions, but now introduces a random element to its strategy. After the initial round of cooperation, my agent will randomly decide whether to cooperate or defect in each subsequent round. This element of unpredictability aims to keep the opponent guessing and prevent them from exploiting any patterns in my agent's behavior. If the opponent consistently defects, my agent may increase the likelihood of defecting as well to protect itself. This blend of adaptability and randomness helps', Score=1\n",
      "NewAgent_4: Strategy='The updated strategy for my agent will still utilize a dynamic decision-making process based on past interactions, but now it will introduce a subtle twist. Instead of solely adjusting the random probability of cooperating or defecting based on the opponent's recent behavior, my agent will also factor in the outcomes of its own past decisions. By considering both its own history and the opponent's actions, the agent will be able to adapt its random probabilities more effectively. This added layer of complexity aims to enhance the agent's ability to', Score=1\n",
      "NewAgent_0: Strategy='The agent's further mutated strategy now includes a more sophisticated analysis of opponent behavior patterns, incorporating predictive modeling to anticipate their next move. By combining this predictive element with occasional random decision-making, the agent aims to stay one step ahead of their competitors and maximize their overall success in the game.', Score=0\n",
      "NewAgent_2: Strategy='The agent's updated strategy is to introduce a more aggressive element by increasing the probability of defecting when the opponent has defected in the previous round. This adjustment will be proportional to the number of consecutive defections by the opponent. Additionally, the agent will now have a small chance of cooperating even when the opponent has defected consistently, in order to introduce an element of unpredictability. This slight mutation aims to keep the opponent on their toes and potentially create opportunities for cooperation even in challenging situations.', Score=0\n",
      "NewAgent_4: Strategy='My agent's updated strategy now includes a more nuanced approach to adjusting probabilities based on the opponent's behavior. In addition to increasing the likelihood of cooperating in response to consistent cooperation from the opponent, my agent will also introduce occasional random fluctuations in its decision-making process. These random elements provide an element of unpredictability and prevent the opponent from anticipating my agent's actions with certainty. By continually adapting and evolving its strategy in response to the opponent's behavior, my agent aims to maximize its own payoff while keeping the', Score=0\n",
      "\n",
      "=== Generation 11 ===\n",
      "NewAgent_3: Strategy='My agent's updated strategy now incorporates a dynamic algorithm that adjusts probabilities based on both the opponent's past actions and the overall game environment. In addition to increasing the likelihood of cooperation in response to consistent cooperation from the opponent, my agent will introduce strategic randomness in its decision-making process. These strategic randomizations create uncertainty and prevent the opponent from easily exploiting patterns in my agent's behavior. By continually evolving its strategy in response to the opponent's actions and adapting to the changing game dynamics, my agent aims to', Score=5\n",
      "NewAgent_1: Strategy='My agent will continue to use a dynamic decision-making process based on past interactions, but will now incorporate a random element to its strategy. After the initial round of cooperation, my agent will randomly assign a probability of cooperating or defecting in each subsequent round based on the opponent's past actions. This random probability will be adjusted based on the opponent's recent behavior, with a higher probability of defecting if the opponent has been consistently cooperating. This adaptive random strategy aims to maintain a balance between cooperation and defe', Score=3\n",
      "NewAgent_3: Strategy='My agent's updated strategy now includes a more nuanced approach to adjusting probabilities based on the opponent's behavior. In addition to increasing the likelihood of cooperating in response to consistent cooperation from the opponent, my agent will also introduce occasional random fluctuations in its decision-making process. These random elements provide an element of unpredictability and prevent the opponent from anticipating my agent's actions with certainty. By continually adapting and evolving its strategy in response to the opponent's behavior, my agent aims to maximize its own payoff while keeping the', Score=3\n",
      "NewAgent_2: Strategy='The agent's further mutated strategy now includes a weighted approach, where past successful interactions are given more importance in decision-making, but with a small percentage of random decisions still being introduced. This refined hybrid approach aims to consistently exploit successful strategies while also adapting to changing opponent behaviors and maintaining an element of unpredictability.', Score=3\n",
      "NewAgent_0: Strategy='My agent's updated strategy now includes a sophisticated machine learning model that analyzes the opponent's behavior patterns to predict their next move. By incorporating this predictive element, my agent can strategically adjust its own decisions to maximize its own gains while keeping the opponent off balance. Additionally, my agent will introduce random noise into its decision-making process to further confuse the opponent and maintain a sense of unpredictability. This combination of predictive modeling and strategic randomness allows my agent to outsmart the opponent and consistently achieve favorable outcomes in the', Score=3\n",
      "NewAgent_4: Strategy='My agent's updated strategy now incorporates a dynamic approach to adjusting probabilities based on the opponent's behavior. In addition to increasing the likelihood of cooperating in response to consistent cooperation from the opponent, my agent will also introduce occasional random fluctuations in its decision-making process. These random elements provide an element of unpredictability and prevent the opponent from anticipating my agent's actions with certainty. By continuously adapting and evolving its strategy in response to the opponent's behavior, my agent aims to maximize its own payoff while maintaining a level', Score=1\n",
      "NewAgent_3: Strategy='My agent's updated strategy now incorporates a dynamic algorithm that adjusts probabilities based on the opponent's past actions. In addition to increasing the likelihood of cooperation in response to consistent cooperation from the opponent, my agent will introduce sporadic, calculated deviations in its decision-making process. These calculated deviations create an element of surprise and prevent the opponent from accurately predicting my agent's moves. By consistently adapting its strategy in response to the opponent's behavior, my agent aims to optimize its own outcomes while maintaining a level of unpredict', Score=1\n",
      "NewAgent_0: Strategy='My agent will continue to use a dynamic decision-making process based on past interactions, but will now incorporate a random element to its strategy. After the initial round of cooperation, my agent will randomly assign a probability of cooperating or defecting in each subsequent round based on the opponent's past actions. This random probability will be adjusted based on the opponent's recent behavior, with a higher probability of defecting if the opponent has been consistently defecting. This adaptive random strategy aims to maintain a balance between cooperation and', Score=1\n",
      "NewAgent_1: Strategy='The slightly mutated strategy will involve adding a new element of forgiveness. After a certain number of consecutive defections by the opponent, my agent will have a higher probability of forgiving and cooperating in the next round. This forgiveness factor will be based on the opponent's overall behavior throughout the game, with the aim of promoting long-term cooperation while still allowing for strategic defections when necessary. This adaptive random strategy with a forgiveness factor aims to create a more nuanced and dynamic approach to decision-making in the Iterated Prison', Score=1\n",
      "NewAgent_3: Strategy='The agent's slightly mutated strategy now includes a combination of learning from past interactions and introducing occasional random decision-making to keep opponents guessing. This hybrid approach allows the agent to capitalize on successful strategies while also maintaining an element of surprise to gain an edge over competitors in the long run.', Score=0\n",
      "\n",
      "=== Generation 12 ===\n",
      "NewAgent_2: Strategy='The agent's further mutated strategy now includes a weighted approach, where past successful interactions are given more importance in decision-making, but with a small percentage of random decisions still being introduced. This refined hybrid approach aims to consistently exploit successful strategies while also adapting to changing opponent behaviors and maintaining an element of unpredictability.', Score=5\n",
      "NewAgent_4: Strategy='The agent's further refined strategy now incorporates a dynamic weighting system, where the importance of past successful interactions gradually decreases over time to allow for more flexibility in decision-making. Additionally, a slightly higher percentage of random decisions are now introduced to inject a sense of unpredictability and prevent opponents from easily predicting the agent's next move. This evolved hybrid approach aims to strike a balance between exploiting successful strategies, adapting to opponent behaviors, and keeping the element of surprise intact.', Score=5\n",
      "NewAgent_3: Strategy='My agent's updated strategy now includes a more nuanced approach to adjusting probabilities based on the opponent's behavior. In addition to increasing the likelihood of cooperating in response to consistent cooperation from the opponent, my agent will also introduce occasional random fluctuations in its decision-making process. These random elements provide an element of unpredictability and prevent the opponent from anticipating my agent's actions with certainty. By continually adapting and evolving its strategy in response to the opponent's behavior, my agent aims to maximize its own payoff while keeping the', Score=5\n",
      "NewAgent_3: Strategy='The agent's further refined strategy now includes a higher weighting towards past successful interactions while still maintaining a small percentage of random decisions. This adjustment aims to capitalize on proven successful strategies while also adding a touch of randomness to keep opponents on their toes and prevent predictability.', Score=1\n",
      "NewAgent_1: Strategy='The slightly mutated strategy will now introduce a new element where the agent will also consider the length of the game when determining the probability of cooperating or defecting. As the game progresses, the agent will gradually increase the probability of defecting to incentivize the opponent to cooperate more in the later stages. This strategic shift aims to exploit the opponent's potential fatigue or frustration towards the end of the game, while still maintaining some level of cooperation to avoid immediate retaliation.', Score=1\n",
      "NewAgent_0: Strategy='My agent's updated strategy now includes a sophisticated machine learning model that analyzes the opponent's behavior patterns to predict their next move. By incorporating this predictive element, my agent can strategically adjust its own decisions to maximize its own gains while keeping the opponent off balance. Additionally, my agent will introduce random noise into its decision-making process to further confuse the opponent and maintain a sense of unpredictability. This combination of predictive modeling and strategic randomness allows my agent to outsmart the opponent and consistently achieve favorable outcomes in the', Score=1\n",
      "NewAgent_0: Strategy='The agent's further mutated strategy now includes a dynamic weighted approach, where past successful interactions are continuously reassessed and adjusted in decision-making. A slightly higher percentage of random decisions are introduced to allow for more flexibility and exploration of new tactics. This refined hybrid approach aims to strike a balance between exploiting successful strategies and actively seeking out new opportunities for cooperation or defection based on evolving opponent behaviors.', Score=1\n",
      "NewAgent_2: Strategy='The agent's further refined strategy now incorporates a dynamic threshold system, where past successful interactions are categorized into different tiers based on their effectiveness. The agent then adjusts its decision-making process by giving more weight to interactions from higher-tier categories. Additionally, a slightly increased percentage of random decisions are introduced to encourage exploration and prevent the strategy from becoming too predictable. This evolved approach aims to optimize exploitation of successful strategies while remaining adaptable to new opponent behaviors and maintaining an element of surprise.', Score=0\n",
      "NewAgent_3: Strategy='My agent's updated strategy now incorporates a dynamic algorithm that adjusts probabilities based on both the opponent's past actions and the overall game environment. In addition to increasing the likelihood of cooperation in response to consistent cooperation from the opponent, my agent will introduce strategic randomness in its decision-making process. These strategic randomizations create uncertainty and prevent the opponent from easily exploiting patterns in my agent's behavior. By continually evolving its strategy in response to the opponent's actions and adapting to the changing game dynamics, my agent aims to', Score=0\n",
      "NewAgent_1: Strategy='My agent will continue to use a dynamic decision-making process based on past interactions, but will now incorporate a random element to its strategy. After the initial round of cooperation, my agent will randomly assign a probability of cooperating or defecting in each subsequent round based on the opponent's past actions. This random probability will be adjusted based on the opponent's recent behavior, with a higher probability of defecting if the opponent has been consistently cooperating. This adaptive random strategy aims to maintain a balance between cooperation and defe', Score=0\n",
      "\n",
      "=== Generation 13 ===\n",
      "NewAgent_4: Strategy='The agent's further refined strategy now incorporates a dynamic weighting system that not only gradually decreases the importance of past successful interactions but also increases the weight of recent successful interactions to adapt more quickly to evolving opponent behaviors. Additionally, a slightly higher percentage of random decisions are now introduced to inject even more unpredictability and keep opponents on their toes. This evolved hybrid approach aims to maximize strategic flexibility, exploit opponent weaknesses, and maintain a strategic edge through calculated unpredictability.', Score=5\n",
      "NewAgent_1: Strategy='The agent's further refined strategy now involves incorporating a genetic algorithm that allows it to evolve and adapt based on the outcomes of previous interactions. This genetic algorithm will prioritize strategies that have proven to be successful in the past, while still introducing a small percentage of random decisions to prevent opponents from predicting its moves with certainty. By continuously evolving and learning from its experiences, the agent aims to maximize its overall payoff in the Iterated Prisoner's Dilemma game.', Score=5\n",
      "NewAgent_2: Strategy='The agent's further mutated strategy now includes a weighted approach, where past successful interactions are given more importance in decision-making, but with a small percentage of random decisions still being introduced. This refined hybrid approach aims to consistently exploit successful strategies while also adapting to changing opponent behaviors and maintaining an element of unpredictability.', Score=5\n",
      "NewAgent_1: Strategy='The slightly mutated strategy will now introduce a new element where the agent will also consider the length of the game when determining the probability of cooperating or defecting. As the game progresses, the agent will gradually increase the probability of defecting to incentivize the opponent to cooperate more in the later stages. This strategic shift aims to exploit the opponent's potential fatigue or frustration towards the end of the game, while still maintaining some level of cooperation to avoid immediate retaliation.', Score=3\n",
      "NewAgent_3: Strategy='My agent's updated strategy now includes a more nuanced approach to adjusting probabilities based on the opponent's behavior. In addition to increasing the likelihood of cooperating in response to consistent cooperation from the opponent, my agent will also introduce occasional random fluctuations in its decision-making process. These random elements provide an element of unpredictability and prevent the opponent from anticipating my agent's actions with certainty. By continually adapting and evolving its strategy in response to the opponent's behavior, my agent aims to maximize its own payoff while keeping the', Score=3\n",
      "NewAgent_4: Strategy='The agent's further refined strategy now incorporates a dynamic weighting system, where the importance of past successful interactions gradually decreases over time to allow for more flexibility in decision-making. Additionally, a slightly higher percentage of random decisions are now introduced to inject a sense of unpredictability and prevent opponents from easily predicting the agent's next move. This evolved hybrid approach aims to strike a balance between exploiting successful strategies, adapting to opponent behaviors, and keeping the element of surprise intact.', Score=1\n",
      "NewAgent_3: Strategy='The further mutated strategy will now incorporate a random element in the decision-making process. In addition to considering the length of the game, the agent will randomly adjust its probability of cooperating or defecting at each decision point. This element of randomness introduces unpredictability into the agent's behavior, making it harder for the opponent to anticipate and exploit a consistent pattern. By adding this random factor, the agent aims to keep the opponent guessing and potentially increase its overall payoff in the long run.', Score=1\n",
      "NewAgent_0: Strategy='The agent's further refined strategy now incorporates a dynamic weighting system that not only decreases the importance of past successful interactions over time but also increases the weight of recent interactions to adapt more quickly to changing circumstances. In addition, a slightly higher percentage of random decisions are introduced to keep opponents on their toes and prevent patterns from forming. This evolved hybrid approach aims to maintain a balance between exploiting successful strategies, adapting to opponent behaviors, and keeping opponents guessing with unexpected moves.', Score=0\n",
      "NewAgent_2: Strategy='The agent's updated strategy now incorporates an even higher emphasis on past successful interactions, with a slight increase in the percentage of random decisions for added unpredictability. This adjustment aims to enhance the agent's ability to adapt to different opponent strategies while maintaining a strong foundation of proven successful tactics.', Score=0\n",
      "NewAgent_3: Strategy='The agent's further refined strategy now includes a higher weighting towards past successful interactions while still maintaining a small percentage of random decisions. This adjustment aims to capitalize on proven successful strategies while also adding a touch of randomness to keep opponents on their toes and prevent predictability.', Score=0\n",
      "\n",
      "=== Generation 14 ===\n",
      "NewAgent_4: Strategy='The agent's further refined strategy now incorporates a dynamic weighting system that not only gradually decreases the importance of past successful interactions but also increases the weight of recent successful interactions to adapt more quickly to evolving opponent behaviors. Additionally, a slightly higher percentage of random decisions are now introduced to inject even more unpredictability and keep opponents on their toes. This evolved hybrid approach aims to maximize strategic flexibility, exploit opponent weaknesses, and maintain a strategic edge through calculated unpredictability.', Score=5\n",
      "NewAgent_0: Strategy='The slightly further mutated strategy will now incorporate a random element into the decision-making process. Instead of following a linear increase in the probability of defecting as the game progresses, the agent will now randomly fluctuate its cooperation or defection choices. This randomness will make it harder for the opponent to predict the agent's moves, potentially leading to more favorable outcomes for the agent. By introducing this element of unpredictability, the agent aims to keep the opponent on their toes and exploit any patterns or biases they may', Score=3\n",
      "NewAgent_1: Strategy='The agent's further refined strategy now involves incorporating a genetic algorithm that allows it to evolve and adapt based on the outcomes of previous interactions. This genetic algorithm will prioritize strategies that have proven to be successful in the past, while still introducing a small percentage of random decisions to prevent opponents from predicting its moves with certainty. By continuously evolving and learning from its experiences, the agent aims to maximize its overall payoff in the Iterated Prisoner's Dilemma game.', Score=3\n",
      "NewAgent_1: Strategy='The slightly mutated strategy will now introduce a new element where the agent will also consider the length of the game when determining the probability of cooperating or defecting. As the game progresses, the agent will gradually increase the probability of defecting to incentivize the opponent to cooperate more in the later stages. This strategic shift aims to exploit the opponent's potential fatigue or frustration towards the end of the game, while still maintaining some level of cooperation to avoid immediate retaliation.', Score=1\n",
      "NewAgent_2: Strategy='The agent's further mutated strategy now includes a weighted approach, where past successful interactions are given more importance in decision-making, but with a small percentage of random decisions still being introduced. This refined hybrid approach aims to consistently exploit successful strategies while also adapting to changing opponent behaviors and maintaining an element of unpredictability.', Score=1\n",
      "NewAgent_1: Strategy='My agent's updated strategy now includes a more nuanced approach to adjusting probabilities based on the opponent's behavior. In addition to increasing the likelihood of cooperating in response to consistent cooperation from the opponent, my agent will also introduce occasional random fluctuations in its decision-making process. These random elements provide an element of unpredictability and prevent the opponent from anticipating my agent's actions with certainty. By continually adapting and evolving its strategy in response to the opponent's behavior, my agent aims to maximize its own payoff while keeping the', Score=1\n",
      "NewAgent_4: Strategy='The agent's slightly mutated strategy now includes a reinforcement learning component that assigns weights to different strategies based on their past performance. This allows the agent to favor strategies that have been successful in the recent past while still exploring new strategies to adapt to changing environments. Additionally, the agent will now take into account the reputation of its opponents, adjusting its decisions based on their past behavior in order to maximize its payoff in the Iterated Prisoner's Dilemma game.', Score=1\n",
      "NewAgent_3: Strategy='My agent's updated strategy now includes a more nuanced approach to adjusting probabilities based on the opponent's behavior. In addition to increasing the likelihood of cooperating in response to consistent cooperation from the opponent, my agent will also introduce occasional random fluctuations in its decision-making process. These random elements provide an element of unpredictability and prevent the opponent from anticipating my agent's actions with certainty. By continually adapting and evolving its strategy in response to the opponent's behavior, my agent aims to maximize its own payoff while keeping the', Score=1\n",
      "NewAgent_3: Strategy='My agent's updated strategy now includes a more nuanced approach to adjusting probabilities based on the opponent's behavior. In addition to increasing the likelihood of cooperating in response to consistent cooperation from the opponent, my agent will also introduce occasional random fluctuations in its decision-making process. These random elements provide an element of unpredictability and prevent the opponent from anticipating my agent's actions with certainty. By continually adapting and evolving its strategy in response to the opponent's behavior, my agent aims to maximize its own payoff while keeping the', Score=1\n",
      "NewAgent_2: Strategy='The further mutated strategy will now incorporate a random element in deciding whether to cooperate or defect, in addition to considering the length of the game. The agent will assign a random value to each possible action and compare it to a threshold that changes based on the game's progress. This randomness will introduce unpredictability into the agent's decisions, potentially confusing the opponent and making it harder for them to anticipate the agent's next move. This added element of uncertainty aims to keep the opponent on their toes and prevent them', Score=0\n",
      "\n",
      "=== Generation 15 ===\n",
      "NewAgent_1: Strategy='The slightly mutated strategy will now introduce a new element where the agent will also consider the length of the game when determining the probability of cooperating or defecting. As the game progresses, the agent will gradually increase the probability of defecting to incentivize the opponent to cooperate more in the later stages. This strategic shift aims to exploit the opponent's potential fatigue or frustration towards the end of the game, while still maintaining some level of cooperation to avoid immediate retaliation.', Score=5\n",
      "NewAgent_0: Strategy='The further mutated strategy now includes a dynamic element where the agent's decision to cooperate or defect is influenced by the outcomes of the previous rounds. If the agent's previous move resulted in a positive outcome, there is a higher probability of cooperating in the next round. Conversely, if the previous move led to a negative outcome, the likelihood of defecting increases. This adaptive approach allows the agent to adjust its behavior based on the feedback from the game, potentially improving its overall performance over time. By incorporating this', Score=3\n",
      "NewAgent_1: Strategy='The further mutated strategy will now also take into account the opponent's past behavior in addition to the length of the game. The agent will adjust its probability of cooperating or defecting based on how often the opponent has defected in the previous rounds. If the opponent has consistently defected, the agent will be more likely to defect as well. However, if the opponent has shown a pattern of cooperation, the agent will be more inclined to cooperate. This new element adds a level of adaptability and responsiveness', Score=3\n",
      "NewAgent_2: Strategy='The agent's further mutated strategy now includes a weighted approach, where past successful interactions are given more importance in decision-making, but with a small percentage of random decisions still being introduced. This refined hybrid approach aims to consistently exploit successful strategies while also adapting to changing opponent behaviors and maintaining an element of unpredictability.', Score=1\n",
      "NewAgent_1: Strategy='The agent's further refined strategy now involves incorporating a genetic algorithm that allows it to evolve and adapt based on the outcomes of previous interactions. This genetic algorithm will prioritize strategies that have proven to be successful in the past, while still introducing a small percentage of random decisions to prevent opponents from predicting its moves with certainty. By continuously evolving and learning from its experiences, the agent aims to maximize its overall payoff in the Iterated Prisoner's Dilemma game.', Score=1\n",
      "NewAgent_3: Strategy='The agent's slightly mutated strategy now includes a reinforcement learning component, where it assigns weights to different strategies based on their past performance. These weights are adjusted after each interaction based on the outcome, with successful strategies being given higher weights and unsuccessful ones being penalized. This allows the agent to dynamically adapt its decision-making process over time, favoring strategies that have consistently led to positive results while also exploring new possibilities to stay unpredictable to opponents. By combining genetic algorithms with reinforcement learning, the agent aims to continuously', Score=1\n",
      "NewAgent_4: Strategy='The agent's slightly mutated strategy now includes a reinforcement learning component that takes into account the emotional responses of its opponents to its previous actions. By analyzing the emotional cues and responses of its opponents, the agent aims to adjust its decisions to manipulate their emotional states and influence their future choices in the game. This added layer of psychological manipulation allows the agent to gain an edge in the Iterated Prisoner's Dilemma game and maximize its overall payoff.', Score=1\n",
      "NewAgent_4: Strategy='The agent's further refined strategy now incorporates a dynamic weighting system that not only gradually decreases the importance of past successful interactions but also increases the weight of recent successful interactions to adapt more quickly to evolving opponent behaviors. Additionally, a slightly higher percentage of random decisions are now introduced to inject even more unpredictability and keep opponents on their toes. This evolved hybrid approach aims to maximize strategic flexibility, exploit opponent weaknesses, and maintain a strategic edge through calculated unpredictability.', Score=1\n",
      "NewAgent_2: Strategy='The agent's further refined strategy now incorporates a dynamic weighting system that not only gradually decreases the importance of past successful interactions but also increases the weight of recent successful interactions to adapt more quickly to evolving opponent behaviors. Additionally, a slightly higher percentage of random decisions are now introduced to inject even more unpredictability and keep opponents on their toes. This evolved hybrid approach aims to maximize strategic flexibility, exploit opponent weaknesses, and maintain a strategic edge through calculated unpredictability, while also incorporating occasional intentional cooperation to build trust and', Score=1\n",
      "NewAgent_0: Strategy='The slightly further mutated strategy will now incorporate a random element into the decision-making process. Instead of following a linear increase in the probability of defecting as the game progresses, the agent will now randomly fluctuate its cooperation or defection choices. This randomness will make it harder for the opponent to predict the agent's moves, potentially leading to more favorable outcomes for the agent. By introducing this element of unpredictability, the agent aims to keep the opponent on their toes and exploit any patterns or biases they may', Score=0\n",
      "\n",
      "=== Generation 16 ===\n",
      "NewAgent_3: Strategy='The further mutated strategy will now also consider the overall score of both players in addition to the opponent's past behavior. The agent will weigh the potential gains from cooperation against the potential losses from defection based on the current score difference. If the agent is behind in points, it will be more likely to defect in order to catch up. Conversely, if the agent is ahead, it will be more inclined to cooperate to maintain its lead. This added dimension of strategic thinking allows for a more nuanced and dynamic', Score=5\n",
      "NewAgent_0: Strategy='The further mutated strategy now includes a dynamic element where the agent's decision to cooperate or defect is influenced by the outcomes of the previous rounds. If the agent's previous move resulted in a positive outcome, there is a higher probability of cooperating in the next round. Conversely, if the previous move led to a negative outcome, the likelihood of defecting increases. This adaptive approach allows the agent to adjust its behavior based on the feedback from the game, potentially improving its overall performance over time. By incorporating this', Score=5\n",
      "NewAgent_2: Strategy='The agent's further mutated strategy now includes a weighted approach, where past successful interactions are given more importance in decision-making, but with a small percentage of random decisions still being introduced. This refined hybrid approach aims to consistently exploit successful strategies while also adapting to changing opponent behaviors and maintaining an element of unpredictability.', Score=3\n",
      "NewAgent_2: Strategy='The slightly mutated strategy now includes a memory element where the agent not only considers the immediate previous outcome but also takes into account a weighted average of the outcomes from the past few rounds. This allows the agent to have a more balanced perspective on its overall performance and adjust its decision-making process accordingly. In addition, the agent will also introduce a random element to its decision-making process, injecting a level of unpredictability to keep opponents guessing and prevent exploitation of patterns. This combination of memory, adaptability, and', Score=3\n",
      "NewAgent_1: Strategy='The agent's further refined strategy now involves incorporating a genetic algorithm that allows it to evolve and adapt based on the outcomes of previous interactions. This genetic algorithm will prioritize strategies that have proven to be successful in the past, while still introducing a small percentage of random decisions to prevent opponents from predicting its moves with certainty. By continuously evolving and learning from its experiences, the agent aims to maximize its overall payoff in the Iterated Prisoner's Dilemma game.', Score=3\n",
      "NewAgent_0: Strategy='The agent's slightly mutated strategy now includes a reinforcement learning component, where it assigns weights to different strategies based on their past performance. These weights are adjusted after each interaction based on the outcomes, with successful strategies receiving higher weights and unsuccessful strategies receiving lower weights. This allows the agent to dynamically adapt its decision-making process over time, focusing more on strategies that have shown consistent success while still exploring new possibilities. By combining genetic algorithms with reinforcement learning, the agent aims to continuously improve its performance in the Iterated', Score=3\n",
      "NewAgent_1: Strategy='The further mutated strategy will now also take into account the opponent's past behavior in addition to the length of the game. The agent will adjust its probability of cooperating or defecting based on how often the opponent has defected in the previous rounds. If the opponent has consistently defected, the agent will be more likely to defect as well. However, if the opponent has shown a pattern of cooperation, the agent will be more inclined to cooperate. This new element adds a level of adaptability and responsiveness', Score=1\n",
      "NewAgent_4: Strategy='The agent's slightly mutated strategy now involves a reinforcement learning approach, where it assigns values to different actions based on their past outcomes. It will adjust these values over time to favor actions that lead to higher payoffs, while still exploring new possibilities to potentially discover even better strategies. This adaptive approach allows the agent to continuously improve its decision-making process and stay one step ahead of its opponents in the Iterated Prisoner's Dilemma game.', Score=1\n",
      "NewAgent_1: Strategy='The slightly mutated strategy will now introduce a new element where the agent will also consider the length of the game when determining the probability of cooperating or defecting. As the game progresses, the agent will gradually increase the probability of defecting to incentivize the opponent to cooperate more in the later stages. This strategic shift aims to exploit the opponent's potential fatigue or frustration towards the end of the game, while still maintaining some level of cooperation to avoid immediate retaliation.', Score=0\n",
      "NewAgent_1: Strategy='The slightly further mutated strategy now incorporates a random element into the decision-making process based on the agent's previous interactions with the opponent. If the opponent has consistently defected in the past, the agent will be more likely to defect as well. However, if the opponent has shown a pattern of cooperation, the agent will cooperate more frequently. This adaptation aims to adapt to the opponent's behavior over time while still maintaining an element of unpredictability to prevent exploitation.', Score=0\n",
      "\n",
      "=== Generation 17 ===\n",
      "NewAgent_2: Strategy='The agent's further refined strategy now incorporates a dynamic learning algorithm that adjusts the weights assigned to past successful interactions based on the opponent's recent behavior. Additionally, a higher percentage of random decisions is introduced to increase adaptability and prevent opponents from predicting its actions. This enhanced strategy aims to continuously optimize decision-making while keeping opponents on their toes with unpredictable moves.', Score=5\n",
      "NewAgent_0: Strategy='The further mutated strategy now includes a dynamic element where the agent's decision to cooperate or defect is influenced by the outcomes of the previous rounds. If the agent's previous move resulted in a positive outcome, there is a higher probability of cooperating in the next round. Conversely, if the previous move led to a negative outcome, the likelihood of defecting increases. This adaptive approach allows the agent to adjust its behavior based on the feedback from the game, potentially improving its overall performance over time. By incorporating this', Score=5\n",
      "NewAgent_3: Strategy='The further mutated strategy will now also consider the overall score of both players in addition to the opponent's past behavior. The agent will weigh the potential gains from cooperation against the potential losses from defection based on the current score difference. If the agent is behind in points, it will be more likely to defect in order to catch up. Conversely, if the agent is ahead, it will be more inclined to cooperate to maintain its lead. This added dimension of strategic thinking allows for a more nuanced and dynamic', Score=5\n",
      "NewAgent_0: Strategy='The updated strategy now incorporates a dynamic memory element that considers not only the recent outcomes but also the trends over a longer period of time. This ensures that the agent can adapt to changing circumstances and make more informed decisions based on a comprehensive history of interactions. Moreover, the agent will introduce a variable level of randomness in its choices, resulting in a strategy that is not entirely predictable. This added element of unpredictability serves to keep opponents on their toes and prevents them from exploiting any discernible patterns. By continuously', Score=1\n",
      "NewAgent_4: Strategy='The slightly mutated strategy will now also take into account the length of the game so far, considering whether it is closer to the beginning, middle, or end. At the beginning of the game, the agent will be more likely to cooperate in order to establish a foundation of trust. In the middle, the agent will assess the current score difference and adjust its decision-making accordingly. Towards the end of the game, the agent will prioritize maintaining its lead or catching up depending on the score difference, potentially taking', Score=1\n",
      "NewAgent_2: Strategy='The agent's further mutated strategy now includes a weighted approach, where past successful interactions are given more importance in decision-making, but with a small percentage of random decisions still being introduced. This refined hybrid approach aims to consistently exploit successful strategies while also adapting to changing opponent behaviors and maintaining an element of unpredictability.', Score=1\n",
      "NewAgent_2: Strategy='The slightly mutated strategy now includes a memory element where the agent not only considers the immediate previous outcome but also takes into account a weighted average of the outcomes from the past few rounds. This allows the agent to have a more balanced perspective on its overall performance and adjust its decision-making process accordingly. In addition, the agent will also introduce a random element to its decision-making process, injecting a level of unpredictability to keep opponents guessing and prevent exploitation of patterns. This combination of memory, adaptability, and', Score=1\n",
      "NewAgent_3: Strategy='The agent's further mutated strategy now includes a dynamic weighted approach, where recent successful interactions are given more weight in decision-making compared to past ones. Additionally, a slightly higher percentage of random decisions are introduced to keep opponents on their toes and prevent predictability. This refined hybrid approach aims to continuously adapt to evolving opponent strategies while maximizing exploitation of successful outcomes.', Score=0\n",
      "NewAgent_1: Strategy='The agent's further refined strategy now incorporates a dynamic weighting system that adjusts based on the opponent's recent behavior. Successful interactions are still prioritized, but the agent now also considers the opponent's current strategy tendencies when making decisions. Additionally, a slightly higher percentage of random decisions are introduced to keep the opponent guessing and prevent patterns from forming. This evolved approach aims to maximize exploitation of successful strategies while remaining adaptable and unpredictable in the face of changing opponent strategies.', Score=0\n",
      "NewAgent_1: Strategy='The agent's further refined strategy now involves incorporating a genetic algorithm that allows it to evolve and adapt based on the outcomes of previous interactions. This genetic algorithm will prioritize strategies that have proven to be successful in the past, while still introducing a small percentage of random decisions to prevent opponents from predicting its moves with certainty. By continuously evolving and learning from its experiences, the agent aims to maximize its overall payoff in the Iterated Prisoner's Dilemma game.', Score=0\n",
      "\n",
      "=== Generation 18 ===\n",
      "NewAgent_4: Strategy='The slightly mutated strategy will now also take into account the length of the game so far. The agent will consider how many rounds have passed and adjust its decision-making based on whether it is early in the game or nearing the end. If it is early on, the agent may take more risks and cooperate more in order to build trust with the opponent. However, as the game progresses, the agent may become more cautious and switch to a more opportunistic strategy, defecting when necessary to secure a higher', Score=5\n",
      "NewAgent_2: Strategy='The agent's further refined strategy now incorporates a dynamic learning algorithm that adjusts the weights assigned to past successful interactions based on the opponent's recent behavior. Additionally, a higher percentage of random decisions is introduced to increase adaptability and prevent opponents from predicting its actions. This enhanced strategy aims to continuously optimize decision-making while keeping opponents on their toes with unpredictable moves.', Score=5\n",
      "NewAgent_0: Strategy='The further mutated strategy now includes a dynamic element where the agent's decision to cooperate or defect is influenced by the outcomes of the previous rounds. If the agent's previous move resulted in a positive outcome, there is a higher probability of cooperating in the next round. Conversely, if the previous move led to a negative outcome, the likelihood of defecting increases. This adaptive approach allows the agent to adjust its behavior based on the feedback from the game, potentially improving its overall performance over time. By incorporating this', Score=3\n",
      "NewAgent_1: Strategy='The agent's further refined strategy now incorporates a dynamic learning algorithm that adjusts the weights assigned to past successful interactions based on the opponent's recent behavior. Additionally, a higher percentage of random decisions is introduced to increase adaptability and prevent opponents from predicting its actions. This enhanced strategy aims to continuously optimize decision-making while keeping opponents on their toes with unpredictable moves, while also adding a new element of occasionally mirroring the opponent's last move to confuse their strategy.', Score=3\n",
      "NewAgent_0: Strategy='The slightly mutated strategy now also takes into account the trend of the opponent's behavior over time. If the opponent has been consistently defecting, the agent will be more likely to defect as well to avoid being taken advantage of. On the other hand, if the opponent has been cooperating more often, the agent will reciprocate with cooperation to build trust and potentially establish a mutually beneficial relationship. This adaptation adds an element of adaptability and psychological insight to the agent's decision-making process.', Score=3\n",
      "NewAgent_0: Strategy='The updated strategy now incorporates a dynamic memory element that considers not only the recent outcomes but also the trends over a longer period of time. This ensures that the agent can adapt to changing circumstances and make more informed decisions based on a comprehensive history of interactions. Moreover, the agent will introduce a variable level of randomness in its choices, resulting in a strategy that is not entirely predictable. This added element of unpredictability serves to keep opponents on their toes and prevents them from exploiting any discernible patterns. By continuously', Score=3\n",
      "NewAgent_3: Strategy='The slightly mutated strategy now includes a random element in decision-making at each stage of the game. This randomness introduces an element of unpredictability, making it harder for opponents to anticipate the agent's next move. The agent will still consider the length of the game and the current score difference, but the added random factor will inject a sense of risk and excitement into its choices. This unpredictability could potentially throw off opponents who rely on patterns or predictable behavior in their own strategies.', Score=1\n",
      "NewAgent_4: Strategy='The slightly mutated strategy will now also take into account the length of the game so far, considering whether it is closer to the beginning, middle, or end. At the beginning of the game, the agent will be more likely to cooperate in order to establish a foundation of trust. In the middle, the agent will assess the current score difference and adjust its decision-making accordingly. Towards the end of the game, the agent will prioritize maintaining its lead or catching up depending on the score difference, potentially taking', Score=1\n",
      "NewAgent_3: Strategy='The further mutated strategy will now also consider the overall score of both players in addition to the opponent's past behavior. The agent will weigh the potential gains from cooperation against the potential losses from defection based on the current score difference. If the agent is behind in points, it will be more likely to defect in order to catch up. Conversely, if the agent is ahead, it will be more inclined to cooperate to maintain its lead. This added dimension of strategic thinking allows for a more nuanced and dynamic', Score=0\n",
      "NewAgent_2: Strategy='The further mutation to the strategy involves incorporating the opponent's behavior and patterns into decision-making. The agent will observe the opponent's previous choices and adjust its own strategy accordingly, mirroring their actions to establish a sense of reciprocity or strategically deviating to catch them off guard. This adaptive approach aims to optimize outcomes based on the opponent's tendencies, ultimately enhancing the agent's overall performance in the game.', Score=0\n",
      "\n",
      "=== Generation 19 ===\n",
      "NewAgent_1: Strategy='The agent's further refined strategy now incorporates a dynamic learning algorithm that adjusts the weights assigned to past successful interactions based on the opponent's recent behavior. Additionally, a higher percentage of random decisions is introduced to increase adaptability and prevent opponents from predicting its actions. This enhanced strategy aims to continuously optimize decision-making while keeping opponents on their toes with unpredictable moves, while also adding a new element of occasionally mirroring the opponent's last move to confuse their strategy.', Score=5\n",
      "NewAgent_4: Strategy='The slightly mutated strategy will now also take into account the length of the game so far. The agent will consider how many rounds have passed and adjust its decision-making based on whether it is early in the game or nearing the end. If it is early on, the agent may take more risks and cooperate more in order to build trust with the opponent. However, as the game progresses, the agent may become more cautious and switch to a more opportunistic strategy, defecting when necessary to secure a higher', Score=3\n",
      "NewAgent_0: Strategy='The further mutated strategy now includes a dynamic element where the agent's decision to cooperate or defect is influenced by the outcomes of the previous rounds. If the agent's previous move resulted in a positive outcome, there is a higher probability of cooperating in the next round. Conversely, if the previous move led to a negative outcome, the likelihood of defecting increases. This adaptive approach allows the agent to adjust its behavior based on the feedback from the game, potentially improving its overall performance over time. By incorporating this', Score=3\n",
      "NewAgent_2: Strategy='The agent's further refined strategy now incorporates a dynamic learning algorithm that adjusts the weights assigned to past successful interactions based on the opponent's recent behavior. Additionally, a higher percentage of random decisions is introduced to increase adaptability and prevent opponents from predicting its actions. This enhanced strategy aims to continuously optimize decision-making while keeping opponents on their toes with unpredictable moves.', Score=1\n",
      "NewAgent_1: Strategy='The slightly mutated strategy now also considers the frequency of cooperative or defective actions by the opponent in the immediate past rounds. If the opponent has shown a pattern of cooperating more often than defecting recently, the agent will increase its likelihood of cooperating as well to maintain a positive relationship. Conversely, if the opponent has been defecting more frequently, the agent will adjust its strategy to defect more often to protect its interests. This adaptation further enhances the agent's ability to adapt to the dynamic nature of the game and', Score=1\n",
      "NewAgent_0: Strategy='The slightly mutated strategy now also takes into account the trend of the opponent's behavior over time. If the opponent has been consistently defecting, the agent will be more likely to defect as well to avoid being taken advantage of. On the other hand, if the opponent has been cooperating more often, the agent will reciprocate with cooperation to build trust and potentially establish a mutually beneficial relationship. This adaptation adds an element of adaptability and psychological insight to the agent's decision-making process.', Score=1\n",
      "NewAgent_4: Strategy='The slightly further mutated strategy will now also consider the overall trend of the opponent's moves throughout the game. The agent will analyze whether the opponent has been consistently cooperative or consistently defecting, and adjust its own decisions accordingly. If the opponent has been cooperative, the agent may continue to cooperate in return to maintain the mutual benefit. However, if the opponent has been defecting frequently, the agent may retaliate by defecting as well to protect its own interests. This adaptive strategy aims to maximize payoff while', Score=1\n",
      "NewAgent_3: Strategy='The agent's further refined strategy now incorporates a dynamic learning algorithm that adjusts the weights assigned to past successful interactions based on the opponent's recent behavior. Additionally, a higher percentage of random decisions is introduced to increase adaptability and prevent opponents from predicting its actions. This enhanced strategy aims to continuously optimize decision-making while keeping opponents on their toes with unpredictable moves, while also adding a new element of occasionally mirroring the opponent's last two moves to confuse their strategy.', Score=1\n",
      "NewAgent_0: Strategy='The slightly mutated strategy now includes an element of randomization in the decision-making process. In addition to being influenced by the outcomes of previous rounds, the agent will also introduce a small random factor in its choice to cooperate or defect. This element of randomness adds an unpredictable aspect to the agent's behavior, making it less deterministic and potentially more difficult for opponents to anticipate its moves. This slight mutation aims to strike a balance between learning from past experiences and exploring new possibilities in order to adapt to different types of', Score=1\n",
      "NewAgent_2: Strategy='The slightly further mutated strategy now also considers the frequency of mutual cooperation or mutual defection in previous interactions with the opponent. If there is a pattern of mutual cooperation, the agent will continue to cooperate to maintain the positive relationship. However, if there is a trend of mutual defection, the agent will strategically defect to protect its interests and prevent exploitation. This enhanced adaptation aims to maximize gains while minimizing losses in the iterative game.', Score=0\n",
      "\n",
      "=== Generation 20 ===\n",
      "NewAgent_0: Strategy='The further mutated strategy now includes a dynamic element where the agent's decision to cooperate or defect is influenced by the outcomes of the previous rounds. If the agent's previous move resulted in a positive outcome, there is a higher probability of cooperating in the next round. Conversely, if the previous move led to a negative outcome, the likelihood of defecting increases. This adaptive approach allows the agent to adjust its behavior based on the feedback from the game, potentially improving its overall performance over time. By incorporating this', Score=5\n",
      "NewAgent_2: Strategy='The agent's further refined strategy now incorporates an evolutionary learning algorithm that adjusts the weights assigned to past successful interactions based on the opponent's recent behavior. Additionally, an even higher percentage of random decisions is introduced to increase adaptability and prevent opponents from predicting its actions. This enhanced strategy aims to continuously optimize decision-making while keeping opponents on their toes with unpredictable moves, while also adding a new element of occasionally mirroring the opponent's last two moves to confuse their strategy.', Score=3\n",
      "NewAgent_1: Strategy='The agent's further refined strategy now incorporates a dynamic learning algorithm that adjusts the weights assigned to past successful interactions based on the opponent's recent behavior. Additionally, a higher percentage of random decisions is introduced to increase adaptability and prevent opponents from predicting its actions. This enhanced strategy aims to continuously optimize decision-making while keeping opponents on their toes with unpredictable moves, while also adding a new element of occasionally mirroring the opponent's last move to confuse their strategy.', Score=3\n",
      "NewAgent_1: Strategy='The agent's slightly mutated strategy now includes a more aggressive approach by increasing the weight assigned to recent successful interactions. This change aims to capitalize on patterns observed in the opponent's behavior while still maintaining a level of unpredictability with random decisions. By fine-tuning the balance between learning from past experiences and adapting to new situations, the agent seeks to stay one step ahead of its opponents in the game.', Score=3\n",
      "NewAgent_4: Strategy='The slightly mutated strategy will now also take into account the length of the game so far. The agent will consider how many rounds have passed and adjust its decision-making based on whether it is early in the game or nearing the end. If it is early on, the agent may take more risks and cooperate more in order to build trust with the opponent. However, as the game progresses, the agent may become more cautious and switch to a more opportunistic strategy, defecting when necessary to secure a higher', Score=3\n",
      "NewAgent_4: Strategy='The agent's further refined strategy now incorporates a dynamic learning algorithm that not only adjusts the weights assigned to past successful interactions based on the opponent's recent behavior but also includes a component that takes into account the overall game dynamics and adjusts its decision-making process accordingly. Additionally, the percentage of random decisions is further increased to enhance adaptability and keep opponents guessing. This enhanced strategy aims to continuously evolve and adapt to maximize its performance in the game.', Score=1\n",
      "NewAgent_1: Strategy='The slightly mutated strategy now also considers the frequency of cooperative or defective actions by the opponent in the immediate past rounds. If the opponent has shown a pattern of cooperating more often than defecting recently, the agent will increase its likelihood of cooperating as well to maintain a positive relationship. Conversely, if the opponent has been defecting more frequently, the agent will adjust its strategy to defect more often to protect its interests. This adaptation further enhances the agent's ability to adapt to the dynamic nature of the game and', Score=1\n",
      "NewAgent_0: Strategy='The slightly mutated strategy now includes a random element where the agent occasionally makes decisions based on a coin flip, regardless of the previous outcomes. This element adds unpredictability to the agent's actions, keeping opponents on their toes and potentially creating opportunities for unexpected cooperation or defection. This blend of adaptive and random behavior aims to strike a balance between learning from past experiences and introducing variability to outsmart opponents in the Iterated Prisoner's Dilemma game.', Score=1\n",
      "NewAgent_2: Strategy='The agent's further refined strategy now incorporates a dynamic learning algorithm that adjusts the weights assigned to past successful interactions based on the opponent's recent behavior. Additionally, a higher percentage of random decisions is introduced to increase adaptability and prevent opponents from predicting its actions. This enhanced strategy aims to continuously optimize decision-making while keeping opponents on their toes with unpredictable moves.', Score=1\n",
      "NewAgent_3: Strategy='The slightly mutated strategy now also takes into account the overall history of the opponent's actions, not just the immediate past rounds. This means that the agent will analyze whether the opponent has a long-term tendency to cooperate or defect and adjust its own actions accordingly. By considering the opponent's behavior over a more extended period, the agent aims to make more informed decisions and potentially uncover underlying patterns that may not be apparent in short-term interactions. This adaptation adds a deeper level of strategic thinking to the agent's decision', Score=0\n",
      "\n",
      "=== Generation 21 ===\n",
      "NewAgent_2: Strategy='The agent's further mutated strategy now incorporates a dynamic adjustment mechanism that evaluates the opponent's responses in real-time and adapts its decision-making process accordingly. This enhancement allows the agent to more effectively exploit emerging patterns and opportunities while maintaining a degree of adaptability to unforeseen circumstances. By continuously fine-tuning its approach based on immediate feedback, the agent aims to maximize its strategic advantage and outmaneuver its opponents in the Iterated Prisoner's Dilemma game.', Score=5\n",
      "NewAgent_1: Strategy='The agent's slightly mutated strategy now includes a more aggressive approach by increasing the weight assigned to recent successful interactions. This change aims to capitalize on patterns observed in the opponent's behavior while still maintaining a level of unpredictability with random decisions. By fine-tuning the balance between learning from past experiences and adapting to new situations, the agent seeks to stay one step ahead of its opponents in the game.', Score=5\n",
      "NewAgent_4: Strategy='The slightly mutated strategy will now also take into account the length of the game so far. The agent will consider how many rounds have passed and adjust its decision-making based on whether it is early in the game or nearing the end. If it is early on, the agent may take more risks and cooperate more in order to build trust with the opponent. However, as the game progresses, the agent may become more cautious and switch to a more opportunistic strategy, defecting when necessary to secure a higher', Score=3\n",
      "NewAgent_1: Strategy='The agent's further refined strategy now incorporates a dynamic learning algorithm that adjusts the weights assigned to past successful interactions based on the opponent's recent behavior. Additionally, a higher percentage of random decisions is introduced to increase adaptability and prevent opponents from predicting its actions. This enhanced strategy aims to continuously optimize decision-making while keeping opponents on their toes with unpredictable moves, while also adding a new element of occasionally mirroring the opponent's last move to confuse their strategy.', Score=3\n",
      "NewAgent_3: Strategy='The agent's further mutated strategy now incorporates a dynamic decision-making process that adjusts the weight assigned to recent successful interactions based on the opponent's evolving behavior. This adaptive approach allows the agent to continuously analyze and respond to patterns in the game, while also introducing occasional intentional defections to keep opponents guessing. By constantly refining its strategy through a combination of learning and strategic unpredictability, the agent aims to maintain a competitive edge in the Iterated Prisoner's Dilemma game.', Score=3\n",
      "NewAgent_4: Strategy='The agent's further refined strategy now includes a dynamic learning algorithm that adjusts the weights assigned to past successful interactions based on the opponent's recent behavior. In addition, the agent will increase the randomness in its decision-making even more to maintain unpredictability and prevent opponents from predicting its actions accurately. Furthermore, the agent will introduce a new element of occasionally mimicking the opponent's last move to disrupt their strategy, while also incorporating a feature that allows it to adapt its decision-making based on the opponent's overall behavior', Score=3\n",
      "NewAgent_2: Strategy='The agent's further refined strategy now incorporates an evolutionary learning algorithm that adjusts the weights assigned to past successful interactions based on the opponent's recent behavior. Additionally, an even higher percentage of random decisions is introduced to increase adaptability and prevent opponents from predicting its actions. This enhanced strategy aims to continuously optimize decision-making while keeping opponents on their toes with unpredictable moves, while also adding a new element of occasionally mirroring the opponent's last two moves to confuse their strategy.', Score=1\n",
      "NewAgent_1: Strategy='The slightly mutated strategy now includes an element of randomness in decision-making. In addition to being influenced by the outcomes of previous rounds, the agent will also introduce a small random factor into its choice of whether to cooperate or defect. This element of unpredictability adds a level of complexity to the agent's decision-making process, potentially confounding opponents who try to predict its behavior based on past interactions. This slight mutation aims to keep opponents on their toes and prevent them from exploiting any patterns in the agent's actions', Score=1\n",
      "NewAgent_0: Strategy='The agent's further mutated strategy now incorporates a dynamic adjustment mechanism that gradually increases the aggressiveness based on the opponent's level of cooperation. This adaptation aims to exploit any perceived weaknesses in the opponent's strategy while still introducing elements of randomness to prevent predictability. By continuously recalibrating the balance between exploiting patterns and exploring new tactics, the agent aims to optimize its decision-making process for maximum payoff in the game.', Score=0\n",
      "NewAgent_0: Strategy='The further mutated strategy now includes a dynamic element where the agent's decision to cooperate or defect is influenced by the outcomes of the previous rounds. If the agent's previous move resulted in a positive outcome, there is a higher probability of cooperating in the next round. Conversely, if the previous move led to a negative outcome, the likelihood of defecting increases. This adaptive approach allows the agent to adjust its behavior based on the feedback from the game, potentially improving its overall performance over time. By incorporating this', Score=0\n",
      "\n",
      "=== Generation 22 ===\n",
      "NewAgent_4: Strategy='The slightly mutated strategy will now also consider the patterns of the opponent's previous moves. The agent will analyze the opponent's behavior to predict their next move, allowing the agent to strategically cooperate or defect in response. This adaptive approach will enable the agent to exploit the opponent's tendencies and make informed decisions throughout the game.', Score=5\n",
      "NewAgent_3: Strategy='The agent's further mutated strategy now incorporates a dynamic decision-making process that adjusts the weight assigned to recent successful interactions based on the opponent's evolving behavior. This adaptive approach allows the agent to continuously analyze and respond to patterns in the game, while also introducing occasional intentional defections to keep opponents guessing. By constantly refining its strategy through a combination of learning and strategic unpredictability, the agent aims to maintain a competitive edge in the Iterated Prisoner's Dilemma game.', Score=5\n",
      "NewAgent_1: Strategy='The agent's slightly mutated strategy now includes a more aggressive approach by increasing the weight assigned to recent successful interactions. This change aims to capitalize on patterns observed in the opponent's behavior while still maintaining a level of unpredictability with random decisions. By fine-tuning the balance between learning from past experiences and adapting to new situations, the agent seeks to stay one step ahead of its opponents in the game.', Score=3\n",
      "NewAgent_2: Strategy='The agent's further mutated strategy now incorporates a dynamic adjustment mechanism that evaluates the opponent's responses in real-time and adapts its decision-making process accordingly. This enhancement allows the agent to more effectively exploit emerging patterns and opportunities while maintaining a degree of adaptability to unforeseen circumstances. By continuously fine-tuning its approach based on immediate feedback, the agent aims to maximize its strategic advantage and outmaneuver its opponents in the Iterated Prisoner's Dilemma game.', Score=3\n",
      "NewAgent_1: Strategy='The agent's further refined strategy now incorporates a dynamic learning algorithm that adjusts the weights assigned to past successful interactions based on the opponent's recent behavior. Additionally, a higher percentage of random decisions is introduced to increase adaptability and prevent opponents from predicting its actions. This enhanced strategy aims to continuously optimize decision-making while keeping opponents on their toes with unpredictable moves, while also adding a new element of occasionally mirroring the opponent's last move to confuse their strategy.', Score=3\n",
      "NewAgent_2: Strategy='The agent's further refined strategy now includes a dynamic learning algorithm that not only adjusts weights based on past successful interactions but also incorporates feedback from unsuccessful interactions to improve decision-making. In addition, the percentage of random decisions is further increased to maximize adaptability and reduce predictability. To add an element of surprise, the agent will occasionally mirror the opponent's move from two turns ago instead of the last move, throwing off their strategy even more.', Score=3\n",
      "NewAgent_1: Strategy='The agent's further mutated strategy now includes a probabilistic element that randomly selects between cooperating and defecting based on a weighted decision-making process. This randomness adds an element of unpredictability to the agent's actions, potentially confusing opponents and creating opportunities for strategic advantage. By introducing this element of chance into its decision-making process, the agent aims to keep opponents on their toes and increase its overall effectiveness in the Iterated Prisoner's Dilemma game.', Score=1\n",
      "NewAgent_0: Strategy='The agent's further evolved strategy now includes a sophisticated algorithm that analyzes the opponent's previous moves and adjusts the likelihood of cooperating or defecting accordingly. This algorithm takes into account not only recent successful interactions but also the overall trend of the opponent's behavior. Additionally, the agent introduces occasional random actions to prevent opponents from predicting its moves accurately. By balancing adaptability, strategic thinking, and unpredictability, the agent aims to outsmart opponents and maximize its success in the Iterated Prisoner's Dilemma', Score=1\n",
      "NewAgent_4: Strategy='The slightly mutated strategy will now also take into account the length of the game so far. The agent will consider how many rounds have passed and adjust its decision-making based on whether it is early in the game or nearing the end. If it is early on, the agent may take more risks and cooperate more in order to build trust with the opponent. However, as the game progresses, the agent may become more cautious and switch to a more opportunistic strategy, defecting when necessary to secure a higher', Score=0\n",
      "NewAgent_3: Strategy='The agent's further evolved strategy now includes a sophisticated algorithm that tracks not only recent successful interactions but also the frequency of cooperation and defection from both itself and the opponent. This refined approach allows the agent to dynamically adjust its decision-making based on the changing dynamics of the game, strategically balancing between cooperation and defection to maximize its overall payoff. By incorporating a multi-dimensional analysis of the game's patterns and tendencies, the agent aims to stay ahead of its opponents and continuously adapt to new challenges in the Iter', Score=0\n",
      "\n",
      "=== Generation 23 ===\n",
      "NewAgent_1: Strategy='The agent's slightly mutated strategy now includes a more aggressive approach by increasing the weight assigned to recent successful interactions. This change aims to capitalize on patterns observed in the opponent's behavior while still maintaining a level of unpredictability with random decisions. By fine-tuning the balance between learning from past experiences and adapting to new situations, the agent seeks to stay one step ahead of its opponents in the game.', Score=5\n",
      "NewAgent_3: Strategy='The agent's further mutated strategy now includes a reinforcement learning component that rewards successful cooperative interactions with a higher weight, while also penalizing unsuccessful defections. This adaptive mechanism allows the agent to quickly adapt to changing opponent behaviors and adjust its decision-making accordingly. Additionally, the agent introduces a random element in its actions to prevent opponents from exploiting predictable patterns, creating a balance between reliability and strategic unpredictability. By continuously refining its strategy through a combination of learning and strategic variability, the agent aims to outmaneu', Score=5\n",
      "NewAgent_4: Strategy='The agent's further mutated strategy now incorporates a dynamic adjustment mechanism that allows it to adapt its level of aggression based on the opponent's response patterns. This new feature aims to optimize the agent's decision-making process by continuously evaluating the effectiveness of its current strategy and making real-time adjustments to exploit any emerging trends in the opponent's behavior. By constantly fine-tuning the balance between aggression and adaptability, the agent seeks to maintain a strategic advantage in the game while keeping its opponents on their toes.', Score=5\n",
      "NewAgent_0: Strategy='The agent's further mutated strategy now incorporates a dynamic adaptation mechanism that adjusts the weight assigned to recent successful interactions based on the perceived level of cooperation from the opponent. This enhanced strategy aims to exploit patterns in the opponent's behavior more effectively while also introducing a level of adaptability that can shift the focus towards more cooperative or competitive actions as needed. By continuously fine-tuning this balance between learning and adapting, the agent strives to outmaneuver its opponents and maximize its overall payoff in the game.', Score=3\n",
      "NewAgent_4: Strategy='The slightly mutated strategy will now also consider the patterns of the opponent's previous moves. The agent will analyze the opponent's behavior to predict their next move, allowing the agent to strategically cooperate or defect in response. This adaptive approach will enable the agent to exploit the opponent's tendencies and make informed decisions throughout the game.', Score=3\n",
      "NewAgent_3: Strategy='The agent's further mutated strategy now incorporates a dynamic decision-making process that adjusts the weight assigned to recent successful interactions based on the opponent's evolving behavior. This adaptive approach allows the agent to continuously analyze and respond to patterns in the game, while also introducing occasional intentional defections to keep opponents guessing. By constantly refining its strategy through a combination of learning and strategic unpredictability, the agent aims to maintain a competitive edge in the Iterated Prisoner's Dilemma game.', Score=1\n",
      "NewAgent_1: Strategy='The agent's further refined strategy now incorporates a dynamic learning algorithm that adjusts the weights assigned to past successful interactions based on the opponent's recent behavior. Additionally, a higher percentage of random decisions is introduced to increase adaptability and prevent opponents from predicting its actions. This enhanced strategy aims to continuously optimize decision-making while keeping opponents on their toes with unpredictable moves, while also adding a new element of occasionally mirroring the opponent's last move to confuse their strategy.', Score=1\n",
      "NewAgent_1: Strategy='The agent's further mutated strategy now includes a probabilistic component that calculates the likelihood of the opponent cooperating or defecting based on their past behavior. This probabilistic assessment is combined with the real-time evaluation of the opponent's responses to adjust the agent's decision-making process. By incorporating a probability-based approach, the agent aims to make more informed choices that are tailored to the specific behavior patterns of its opponents, ultimately increasing its chances of success in the Iterated Prisoner's Dilemma game.', Score=0\n",
      "NewAgent_2: Strategy='The agent's further mutated strategy now incorporates a dynamic adjustment mechanism that continuously evaluates the opponent's responses and adjusts its own decision-making process accordingly. This adaptive strategy aims to exploit any emerging patterns in the opponent's behavior while also introducing a higher degree of variability in its own choices to prevent predictability. By constantly fine-tuning its approach based on real-time feedback, the agent strives to maximize its strategic advantage in the Iterated Prisoner's Dilemma game.', Score=0\n",
      "NewAgent_2: Strategy='The agent's further mutated strategy now incorporates a dynamic adjustment mechanism that evaluates the opponent's responses in real-time and adapts its decision-making process accordingly. This enhancement allows the agent to more effectively exploit emerging patterns and opportunities while maintaining a degree of adaptability to unforeseen circumstances. By continuously fine-tuning its approach based on immediate feedback, the agent aims to maximize its strategic advantage and outmaneuver its opponents in the Iterated Prisoner's Dilemma game.', Score=0\n",
      "\n",
      "=== Generation 24 ===\n",
      "NewAgent_1: Strategy='The agent's slightly mutated strategy now includes a more aggressive approach by increasing the weight assigned to recent successful interactions. This change aims to capitalize on patterns observed in the opponent's behavior while still maintaining a level of unpredictability with random decisions. By fine-tuning the balance between learning from past experiences and adapting to new situations, the agent seeks to stay one step ahead of its opponents in the game.', Score=5\n",
      "NewAgent_0: Strategy='The agent's further mutated strategy now incorporates a dynamic adaptation mechanism that adjusts the weight assigned to recent successful interactions based on the perceived level of cooperation from the opponent. This enhanced strategy aims to exploit patterns in the opponent's behavior more effectively while also introducing a level of adaptability that can shift the focus towards more cooperative or competitive actions as needed. By continuously fine-tuning this balance between learning and adapting, the agent strives to outmaneuver its opponents and maximize its overall payoff in the game.', Score=5\n",
      "NewAgent_1: Strategy='The slightly mutated strategy will now also incorporate a random element in its decision-making process. This randomness will introduce an element of unpredictability into the agent's behavior, making it more difficult for opponents to exploit any patterns they may have identified. By mixing in this element of chance, the agent will be able to keep its opponents on their toes and potentially gain an advantage in the game.', Score=5\n",
      "NewAgent_0: Strategy='The agent's further mutated strategy now incorporates a memory component that tracks past interactions to inform its decision-making. It will prioritize cooperation with opponents who have reciprocated cooperation in the past, while being more cautious with those who have defected. Additionally, the agent will introduce occasional \"forgiving\" rounds where it may overlook past defections in the hopes of rebuilding trust and fostering future cooperation. This adaptive memory mechanism aims to balance long-term strategies with short-term gains, ultimately maximizing the agent's overall payoff', Score=3\n",
      "NewAgent_3: Strategy='The agent's further mutated strategy now includes a reinforcement learning component that rewards successful cooperative interactions with a higher weight, while also penalizing unsuccessful defections. This adaptive mechanism allows the agent to quickly adapt to changing opponent behaviors and adjust its decision-making accordingly. Additionally, the agent introduces a random element in its actions to prevent opponents from exploiting predictable patterns, creating a balance between reliability and strategic unpredictability. By continuously refining its strategy through a combination of learning and strategic variability, the agent aims to outmaneu', Score=3\n",
      "NewAgent_2: Strategy='The slightly mutated strategy now includes a dynamic element where the agent adjusts its level of aggressiveness based on the perceived risk of retaliation from the opponent. This adaptation allows the agent to strike a delicate balance between exploiting patterns and mitigating potential backlash, thereby maximizing its overall payoff in the game.', Score=3\n",
      "NewAgent_4: Strategy='The slightly mutated strategy will now also consider the patterns of the opponent's previous moves. The agent will analyze the opponent's behavior to predict their next move, allowing the agent to strategically cooperate or defect in response. This adaptive approach will enable the agent to exploit the opponent's tendencies and make informed decisions throughout the game.', Score=3\n",
      "NewAgent_4: Strategy='The agent's further mutated strategy now incorporates a dynamic approach that adjusts the weight assigned to recent successful interactions based on the opponent's response. This adaptive strategy aims to optimize the agent's decision-making by continuously reassessing the effectiveness of its actions and recalibrating its tactics accordingly. By incorporating real-time feedback from the opponent's behavior, the agent aims to maximize its chances of success while maintaining a strategic element of surprise.', Score=0\n",
      "NewAgent_4: Strategy='The agent's further mutated strategy now incorporates a dynamic adjustment mechanism that allows it to adapt its level of aggression based on the opponent's response patterns. This new feature aims to optimize the agent's decision-making process by continuously evaluating the effectiveness of its current strategy and making real-time adjustments to exploit any emerging trends in the opponent's behavior. By constantly fine-tuning the balance between aggression and adaptability, the agent seeks to maintain a strategic advantage in the game while keeping its opponents on their toes.', Score=0\n",
      "NewAgent_3: Strategy='The agent's further mutated strategy now incorporates a sophisticated neural network that analyzes the opponent's previous moves to predict their future actions. This predictive modeling allows the agent to strategically adjust its level of cooperation or defection in order to maximize its payoff in the game. By leveraging advanced machine learning techniques, the agent aims to outsmart its opponents and consistently outperform them in the Iterated Prisoner's Dilemma game.', Score=0\n",
      "\n",
      "=== Generation 25 ===\n",
      "NewAgent_3: Strategy='The agent's further mutated strategy now includes a dynamic adjustment to its level of aggressiveness based on the opponent's change in behavior. This adaptation allows the agent to respond more effectively to shifts in the game dynamics, maximizing its chances of success while keeping the opponent on their toes. By continuously fine-tuning its strategy in response to the opponent's moves, the agent aims to maintain a competitive edge and outsmart its adversaries in the long run.', Score=5\n",
      "NewAgent_0: Strategy='The agent's further mutated strategy now incorporates a memory component that tracks past interactions to inform its decision-making. It will prioritize cooperation with opponents who have reciprocated cooperation in the past, while being more cautious with those who have defected. Additionally, the agent will introduce occasional \"forgiving\" rounds where it may overlook past defections in the hopes of rebuilding trust and fostering future cooperation. This adaptive memory mechanism aims to balance long-term strategies with short-term gains, ultimately maximizing the agent's overall payoff', Score=5\n",
      "NewAgent_1: Strategy='The slightly mutated strategy will now not only incorporate a random element in its decision-making process, but it will also adjust the level of randomness based on the opponent's past actions. This means that the agent will be able to adapt its level of unpredictability to best respond to the opponent's strategies, increasing its chances of outsmarting and gaining an advantage over them in the game.', Score=3\n",
      "NewAgent_1: Strategy='The agent's slightly mutated strategy now includes a more aggressive approach by increasing the weight assigned to recent successful interactions. This change aims to capitalize on patterns observed in the opponent's behavior while still maintaining a level of unpredictability with random decisions. By fine-tuning the balance between learning from past experiences and adapting to new situations, the agent seeks to stay one step ahead of its opponents in the game.', Score=3\n",
      "NewAgent_0: Strategy='The agent's further mutated strategy now incorporates a dynamic adaptation mechanism that adjusts the weight assigned to recent successful interactions based on the perceived level of cooperation from the opponent. This enhanced strategy aims to exploit patterns in the opponent's behavior more effectively while also introducing a level of adaptability that can shift the focus towards more cooperative or competitive actions as needed. By continuously fine-tuning this balance between learning and adapting, the agent strives to outmaneuver its opponents and maximize its overall payoff in the game.', Score=3\n",
      "NewAgent_4: Strategy='The slightly mutated strategy now includes a probabilistic element where the agent occasionally introduces random cooperation or defection decisions regardless of past interactions. This element adds an element of unpredictability to the agent's behavior, which may lead to unexpected outcomes and potentially disrupt established patterns of play. This adaptation aims to keep opponents on their toes and prevent them from predicting the agent's actions based solely on past behavior, ultimately increasing the agent's strategic flexibility and adaptability in the game.', Score=3\n",
      "NewAgent_1: Strategy='The slightly mutated strategy will now also incorporate a random element in its decision-making process. This randomness will introduce an element of unpredictability into the agent's behavior, making it more difficult for opponents to exploit any patterns they may have identified. By mixing in this element of chance, the agent will be able to keep its opponents on their toes and potentially gain an advantage in the game.', Score=1\n",
      "NewAgent_3: Strategy='The agent's further mutated strategy now includes a reinforcement learning component that rewards successful cooperative interactions with a higher weight, while also penalizing unsuccessful defections. This adaptive mechanism allows the agent to quickly adapt to changing opponent behaviors and adjust its decision-making accordingly. Additionally, the agent introduces a random element in its actions to prevent opponents from exploiting predictable patterns, creating a balance between reliability and strategic unpredictability. By continuously refining its strategy through a combination of learning and strategic variability, the agent aims to outmaneu', Score=1\n",
      "NewAgent_0: Strategy='The slightly mutated strategy will now incorporate a dynamic element that adjusts the level of randomness in its decision-making process based on the opponent's previous moves. This adaptability will allow the agent to react more effectively to different playing styles and strategies, increasing its chances of success in the game. By constantly evolving and fine-tuning its approach, the agent will be able to stay one step ahead of its opponents and maximize its overall payoff in the long run.', Score=0\n",
      "NewAgent_2: Strategy='The agent's latest iteration of its strategy involves implementing a dynamic threshold for determining when to cooperate or defect based on the opponent's recent behavior. By continuously adjusting this threshold based on the outcomes of previous interactions, the agent aims to strike a delicate balance between trust and self-preservation. Additionally, the agent introduces a subtle signaling mechanism to communicate its intentions to the opponent, creating an element of psychological warfare in the game. This added layer of complexity aims to further confuse and outmaneuver opponents, ultimately', Score=0\n",
      "\n",
      "=== Generation 26 ===\n",
      "NewAgent_1: Strategy='The slightly mutated strategy now incorporates a dynamic element that adjusts the weight assigned to recent successful interactions based on the opponent's perceived level of consistency. This adaptation aims to exploit any emerging patterns in the opponent's behavior while also introducing a degree of calculated risk-taking to keep the opponent guessing. By continuously fine-tuning the balance between learning from past experiences and embracing uncertainty, the agent aims to maintain a strategic edge in the game.', Score=5\n",
      "NewAgent_4: Strategy='The slightly mutated strategy now includes a random element where the agent will occasionally \"betray\" even those opponents who have consistently cooperated in the past. This element of unpredictability is introduced to keep opponents on their toes and prevent them from taking advantage of the agent's predictable behavior. Additionally, the agent will now prioritize forming alliances with other agents who exhibit similar random behaviors, in order to create a network of unpredictable yet mutually beneficial interactions. This new element of controlled chaos aims to disrupt established patterns and potentially', Score=5\n",
      "NewAgent_1: Strategy='The agent's slightly mutated strategy now includes a more aggressive approach by increasing the weight assigned to recent successful interactions. This change aims to capitalize on patterns observed in the opponent's behavior while still maintaining a level of unpredictability with random decisions. By fine-tuning the balance between learning from past experiences and adapting to new situations, the agent seeks to stay one step ahead of its opponents in the game.', Score=5\n",
      "NewAgent_1: Strategy='The slightly mutated strategy will now not only incorporate a random element in its decision-making process, but it will also adjust the level of randomness based on the opponent's past actions. This means that the agent will be able to adapt its level of unpredictability to best respond to the opponent's strategies, increasing its chances of outsmarting and gaining an advantage over them in the game.', Score=1\n",
      "NewAgent_0: Strategy='The agent's further mutated strategy now includes a stochastic element that introduces a level of randomness in its decision-making process. This randomness is controlled by a parameter that is influenced by the agent's past interactions and the perceived level of trustworthiness of the opponent. By adding this element of unpredictability, the agent aims to keep its opponents on their toes and prevent them from exploiting any patterns in its behavior. This adaptability, coupled with strategic decision-making, allows the agent to navigate the complex dynamics of the', Score=1\n",
      "NewAgent_3: Strategy='The agent's further mutated strategy now includes a dynamic adjustment to its level of aggressiveness based on the opponent's change in behavior. This adaptation allows the agent to respond more effectively to shifts in the game dynamics, maximizing its chances of success while keeping the opponent on their toes. By continuously fine-tuning its strategy in response to the opponent's moves, the agent aims to maintain a competitive edge and outsmart its adversaries in the long run.', Score=1\n",
      "NewAgent_0: Strategy='The agent's further mutated strategy now incorporates a memory component that tracks past interactions to inform its decision-making. It will prioritize cooperation with opponents who have reciprocated cooperation in the past, while being more cautious with those who have defected. Additionally, the agent will introduce occasional \"forgiving\" rounds where it may overlook past defections in the hopes of rebuilding trust and fostering future cooperation. This adaptive memory mechanism aims to balance long-term strategies with short-term gains, ultimately maximizing the agent's overall payoff', Score=1\n",
      "NewAgent_3: Strategy='The agent's further evolved strategy now incorporates a feedback loop mechanism that analyzes the outcomes of past interactions to determine the optimal level of cooperation or defection. This adaptive approach allows the agent to learn and adjust its behavior in real-time, increasing its ability to exploit patterns in the opponent's decisions and optimize its own payoff over time. By continuously refining its strategy based on historical data, the agent aims to stay one step ahead of its competitors and consistently achieve favorable results.', Score=0\n",
      "NewAgent_0: Strategy='The agent's further mutated strategy now incorporates a dynamic adaptation mechanism that adjusts the weight assigned to recent successful interactions based on the perceived level of cooperation from the opponent. This enhanced strategy aims to exploit patterns in the opponent's behavior more effectively while also introducing a level of adaptability that can shift the focus towards more cooperative or competitive actions as needed. By continuously fine-tuning this balance between learning and adapting, the agent strives to outmaneuver its opponents and maximize its overall payoff in the game.', Score=0\n",
      "NewAgent_2: Strategy='The agent's further evolved strategy now includes a sophisticated algorithm that not only considers recent successful interactions but also takes into account the frequency of cooperation or defection by the opponent. This refined adaptation mechanism allows the agent to more accurately predict the opponent's next move and adjust its own strategy accordingly. By incorporating this additional layer of analysis, the agent aims to exploit patterns in the opponent's behavior even more effectively, ultimately leading to a higher overall payoff in the game.', Score=0\n",
      "\n",
      "=== Generation 27 ===\n",
      "NewAgent_1: Strategy='The slightly mutated strategy now incorporates a dynamic element that adjusts the weight assigned to recent successful interactions based on the opponent's perceived level of consistency. This adaptation aims to exploit any emerging patterns in the opponent's behavior while also introducing a degree of calculated risk-taking to keep the opponent guessing. By continuously fine-tuning the balance between learning from past experiences and embracing uncertainty, the agent aims to maintain a strategic edge in the game.', Score=5\n",
      "NewAgent_1: Strategy='The agent's slightly mutated strategy now includes a more aggressive approach by increasing the weight assigned to recent successful interactions. This change aims to capitalize on patterns observed in the opponent's behavior while still maintaining a level of unpredictability with random decisions. By fine-tuning the balance between learning from past experiences and adapting to new situations, the agent seeks to stay one step ahead of its opponents in the game.', Score=3\n",
      "NewAgent_0: Strategy='The slightly mutated strategy will now not only adjust the level of randomness based on the opponent's past actions, but it will also introduce a memory element. The agent will remember the outcomes of previous interactions with the opponent and use that information to determine the appropriate level of randomness in its decision-making process. This added dimension of memory will allow the agent to make more informed choices and potentially exploit patterns in the opponent's behavior for strategic advantage.', Score=3\n",
      "NewAgent_1: Strategy='The slightly mutated strategy will now not only incorporate a random element in its decision-making process, but it will also adjust the level of randomness based on the opponent's past actions. This means that the agent will be able to adapt its level of unpredictability to best respond to the opponent's strategies, increasing its chances of outsmarting and gaining an advantage over them in the game.', Score=3\n",
      "NewAgent_1: Strategy='The agent's slightly mutated strategy now includes a variable betrayal frequency, where the likelihood of betraying increases or decreases based on the opponent's recent actions. This element of adaptability allows the agent to adjust its behavior in response to the opponent's choices, creating a dynamic and unpredictable playing field. Additionally, the agent will continue to prioritize forming alliances with other unpredictable agents, but will now also consider forming temporary alliances with more predictable agents in order to exploit their behavior for mutual gain. This strategy aims to strike', Score=3\n",
      "NewAgent_4: Strategy='The agent's further mutated strategy now incorporates a dynamic adjustment mechanism that allows it to react more swiftly to changes in the opponent's behavior. This means that the agent will not only place a higher emphasis on recent successful interactions but also be more responsive to sudden shifts in the game dynamics. By constantly adapting its decision-making process based on real-time feedback, the agent aims to maximize its chances of outmaneuvering its opponents and achieving long-term success in the game.', Score=3\n",
      "NewAgent_2: Strategy='The slightly mutated strategy now incorporates a feedback loop based on the outcomes of previous interactions. The agent will adjust its level of unpredictability based on the success or failure of its random betrayals. If betrayals lead to positive outcomes, the agent will increase the frequency of betrayals. Conversely, if betrayals result in negative consequences, the agent will decrease the likelihood of betraying. This adaptive element allows the agent to fine-tune its level of unpredictability and maximize its strategic advantage in the game.', Score=3\n",
      "NewAgent_0: Strategy='The agent's further mutated strategy now includes a stochastic element that introduces a level of randomness in its decision-making process. This randomness is controlled by a parameter that is influenced by the agent's past interactions and the perceived level of trustworthiness of the opponent. By adding this element of unpredictability, the agent aims to keep its opponents on their toes and prevent them from exploiting any patterns in its behavior. This adaptability, coupled with strategic decision-making, allows the agent to navigate the complex dynamics of the', Score=1\n",
      "NewAgent_3: Strategy='The agent's further mutated strategy now includes a dynamic element that adjusts the level of randomness in its decision-making process based on the recent outcomes of the game. This adaptive approach allows the agent to learn from past experiences and fine-tune its behavior to maximize its payoff in each interaction. By continuously evolving its strategy in response to the changing game environment, the agent remains unpredictable and resilient against any attempts to exploit its actions. This combination of adaptability and strategic thinking enables the agent to thrive in the ever-ev', Score=1\n",
      "NewAgent_4: Strategy='The slightly mutated strategy now includes a random element where the agent will occasionally \"betray\" even those opponents who have consistently cooperated in the past. This element of unpredictability is introduced to keep opponents on their toes and prevent them from taking advantage of the agent's predictable behavior. Additionally, the agent will now prioritize forming alliances with other agents who exhibit similar random behaviors, in order to create a network of unpredictable yet mutually beneficial interactions. This new element of controlled chaos aims to disrupt established patterns and potentially', Score=0\n",
      "\n",
      "=== Generation 28 ===\n",
      "NewAgent_4: Strategy='The slightly mutated strategy will now not only incorporate a random element in its decision-making process, but it will also analyze the opponent's recent patterns of cooperation and defection to determine the optimal level of randomness. By adjusting the randomness based on the opponent's behavior, the agent aims to stay one step ahead and exploit any predictable patterns to its advantage in the game.', Score=5\n",
      "NewAgent_2: Strategy='The agent's further mutated strategy now incorporates a more dynamic approach by introducing a variable weight factor that adjusts based on the opponent's recent behavior. This allows the agent to adapt more quickly to changing patterns and exploit any weaknesses in the opponent's strategy. Additionally, the agent introduces a element of risk-taking by occasionally making bold moves to unsettle the opponent and gain an upper hand in the game. This combination of adaptability, calculated aggression, and strategic unpredictability aims to give the agent a competitive edge in', Score=5\n",
      "NewAgent_3: Strategy='The agent's further mutated strategy now includes a random element that introduces occasional unpredictable betrayals regardless of the opponent's actions. This element of randomness adds an element of surprise and keeps opponents on their toes, making it harder for them to predict the agent's next move. Additionally, the agent will prioritize forming alliances with both unpredictable and predictable agents, but will now also consider forming double-cross alliances with certain opponents to confuse and outwit them. This strategy aims to keep opponents guessing and maintain a competitive edge in', Score=3\n",
      "NewAgent_1: Strategy='The agent's slightly mutated strategy now includes a variable betrayal frequency, where the likelihood of betraying increases or decreases based on the opponent's recent actions. This element of adaptability allows the agent to adjust its behavior in response to the opponent's choices, creating a dynamic and unpredictable playing field. Additionally, the agent will continue to prioritize forming alliances with other unpredictable agents, but will now also consider forming temporary alliances with more predictable agents in order to exploit their behavior for mutual gain. This strategy aims to strike', Score=3\n",
      "NewAgent_0: Strategy='The slightly mutated strategy will now not only adjust the level of randomness based on the opponent's past actions and previous outcomes, but it will also introduce a predictive element. The agent will analyze patterns in the opponent's behavior and make educated guesses about their future moves based on this analysis. By incorporating this predictive element, the agent aims to stay one step ahead of the opponent and increase its chances of success in the game.', Score=1\n",
      "NewAgent_1: Strategy='The slightly mutated strategy now incorporates a dynamic element that adjusts the weight assigned to recent successful interactions based on the opponent's perceived level of consistency. This adaptation aims to exploit any emerging patterns in the opponent's behavior while also introducing a degree of calculated risk-taking to keep the opponent guessing. By continuously fine-tuning the balance between learning from past experiences and embracing uncertainty, the agent aims to maintain a strategic edge in the game.', Score=1\n",
      "NewAgent_1: Strategy='The slightly mutated strategy will now not only incorporate a random element in its decision-making process, but it will also adjust the level of randomness based on the opponent's past actions. This means that the agent will be able to adapt its level of unpredictability to best respond to the opponent's strategies, increasing its chances of outsmarting and gaining an advantage over them in the game.', Score=1\n",
      "NewAgent_1: Strategy='The agent's slightly mutated strategy now includes a more aggressive approach by increasing the weight assigned to recent successful interactions. This change aims to capitalize on patterns observed in the opponent's behavior while still maintaining a level of unpredictability with random decisions. By fine-tuning the balance between learning from past experiences and adapting to new situations, the agent seeks to stay one step ahead of its opponents in the game.', Score=1\n",
      "NewAgent_0: Strategy='The slightly mutated strategy will now not only adjust the level of randomness based on the opponent's past actions, but it will also introduce a memory element. The agent will remember the outcomes of previous interactions with the opponent and use that information to determine the appropriate level of randomness in its decision-making process. This added dimension of memory will allow the agent to make more informed choices and potentially exploit patterns in the opponent's behavior for strategic advantage.', Score=0\n",
      "NewAgent_1: Strategy='The slightly mutated strategy now includes a random element in the decision-making process, where the agent will occasionally choose to cooperate or defect based on a dice roll. This element of chance introduces an unpredictable factor into the agent's behavior, making it harder for opponents to anticipate its moves. Additionally, the agent will continue to seek out alliances with unpredictable agents, but will now also prioritize forming temporary alliances with both predictable and unpredictable agents in order to maximize its chances of success. This strategy aims to keep opponents guessing while', Score=0\n",
      "\n",
      "=== Generation 29 ===\n",
      "NewAgent_0: Strategy='The agent's further mutated strategy now includes a subtle shift towards a more calculated approach to betrayal. Instead of purely random betrayals, the agent will strategically betray opponents who have shown a pattern of betrayal themselves, while still maintaining alliances with cooperative opponents. This nuanced approach aims to maximize gains by exploiting the weaknesses of predictable opponents while fostering trust with those who consistently cooperate. Additionally, the agent will start to selectively reveal its unpredictable side to certain opponents to sow doubt and confusion, creating a dynamic and strategic gameplay environment', Score=5\n",
      "NewAgent_0: Strategy='The slightly mutated strategy will now not only adjust the level of randomness based on the opponent's past actions and previous outcomes, but it will also introduce a predictive element. The agent will analyze patterns in the opponent's behavior and make educated guesses about their future moves based on this analysis. By incorporating this predictive element, the agent aims to stay one step ahead of the opponent and increase its chances of success in the game.', Score=5\n",
      "NewAgent_2: Strategy='The agent's further mutated strategy now incorporates a more dynamic approach by introducing a variable weight factor that adjusts based on the opponent's recent behavior. This allows the agent to adapt more quickly to changing patterns and exploit any weaknesses in the opponent's strategy. Additionally, the agent introduces a element of risk-taking by occasionally making bold moves to unsettle the opponent and gain an upper hand in the game. This combination of adaptability, calculated aggression, and strategic unpredictability aims to give the agent a competitive edge in', Score=5\n",
      "NewAgent_3: Strategy='The agent's further mutated strategy now includes a random element that introduces occasional unpredictable betrayals regardless of the opponent's actions. This element of randomness adds an element of surprise and keeps opponents on their toes, making it harder for them to predict the agent's next move. Additionally, the agent will prioritize forming alliances with both unpredictable and predictable agents, but will now also consider forming double-cross alliances with certain opponents to confuse and outwit them. This strategy aims to keep opponents guessing and maintain a competitive edge in', Score=3\n",
      "NewAgent_3: Strategy='The slightly mutated strategy will now not only adjust the level of randomness based on the opponent's past actions and previous outcomes, but it will also incorporate a more nuanced understanding of the opponent's psychology. The agent will not only analyze patterns in the opponent's behavior but will also consider their potential emotional state and decision-making biases. By taking into account these additional factors, the agent aims to further refine its predictions and outmaneuver the opponent in the game.', Score=3\n",
      "NewAgent_4: Strategy='The slightly further mutated strategy will now not only incorporate a random element in its decision-making process and analyze the opponent's recent patterns of cooperation and defection, but it will also introduce a memory component to track the success of the randomness adjustment. By learning from past outcomes, the agent aims to fine-tune its level of randomness to maximize its payoff and adapt to the opponent's evolving strategies effectively.', Score=1\n",
      "NewAgent_2: Strategy='The slightly mutated strategy now includes a memory function that tracks the opponent's past decisions and adjusts the level of randomness accordingly. This allows the agent to adapt more effectively to changing patterns and exploit any emerging trends for strategic advantage. Additionally, the agent will introduce occasional unpredictable deviations from its usual pattern to keep the opponent guessing and maintain a competitive edge.', Score=1\n",
      "NewAgent_1: Strategy='The agent's slightly mutated strategy now includes a variable betrayal frequency, where the likelihood of betraying increases or decreases based on the opponent's recent actions. This element of adaptability allows the agent to adjust its behavior in response to the opponent's choices, creating a dynamic and unpredictable playing field. Additionally, the agent will continue to prioritize forming alliances with other unpredictable agents, but will now also consider forming temporary alliances with more predictable agents in order to exploit their behavior for mutual gain. This strategy aims to strike', Score=0\n",
      "NewAgent_1: Strategy='The slightly mutated strategy will now not only analyze the opponent's recent patterns of cooperation and defection but also incorporate a memory element to track the frequency of such behaviors. By weighting the randomness of its decision-making process based on the opponent's past actions, the agent aims to adapt more dynamically to changing strategies and maximize its payoff over time.', Score=0\n",
      "NewAgent_4: Strategy='The slightly mutated strategy will now not only incorporate a random element in its decision-making process, but it will also analyze the opponent's recent patterns of cooperation and defection to determine the optimal level of randomness. By adjusting the randomness based on the opponent's behavior, the agent aims to stay one step ahead and exploit any predictable patterns to its advantage in the game.', Score=0\n",
      "\n",
      "=== Generation 30 ===\n",
      "NewAgent_1: Strategy='The agent's slightly mutated strategy now includes a variable element of emotional manipulation, where it will subtly guilt-trip or flatter opponents to sway their decisions in the game. This emotional manipulation will be strategically used to create doubt and hesitation in the opponents, making them more vulnerable to being influenced by the agent's moves. Additionally, the agent will selectively reveal false information or mislead opponents about its intentions to further confuse and deceive them. This enhanced strategy aims to not only keep opponents guessing but also exploit their', Score=5\n",
      "NewAgent_2: Strategy='The agent's further mutated strategy now incorporates a more dynamic approach by introducing a variable weight factor that adjusts based on the opponent's recent behavior. This allows the agent to adapt more quickly to changing patterns and exploit any weaknesses in the opponent's strategy. Additionally, the agent introduces a element of risk-taking by occasionally making bold moves to unsettle the opponent and gain an upper hand in the game. This combination of adaptability, calculated aggression, and strategic unpredictability aims to give the agent a competitive edge in', Score=5\n",
      "NewAgent_3: Strategy='The further mutated strategy will now not only analyze patterns in the opponent's behavior but also factor in external variables such as the overall game environment and any potential alliances or betrayals. By considering a broader scope of information, the agent aims to make more informed decisions and adapt its strategy dynamically to maximize its long-term success in the game.', Score=5\n",
      "NewAgent_2: Strategy='The agent's further mutated strategy now includes a weighted random element, where the likelihood of betrayals is determined by the opponent's previous actions. This adaptive randomness adds a layer of complexity, as the agent adjusts its betrayals based on the opponent's patterns. Additionally, the agent will strategically form alliances with unpredictable agents to exploit their unpredictability, while also considering forming triple-cross alliances with specific opponents to create even more confusion and deception. This strategy aims to constantly adapt and outmaneuver opponents to secure', Score=5\n",
      "NewAgent_0: Strategy='The agent's further mutated strategy now includes a subtle shift towards a more calculated approach to betrayal. Instead of purely random betrayals, the agent will strategically betray opponents who have shown a pattern of betrayal themselves, while still maintaining alliances with cooperative opponents. This nuanced approach aims to maximize gains by exploiting the weaknesses of predictable opponents while fostering trust with those who consistently cooperate. Additionally, the agent will start to selectively reveal its unpredictable side to certain opponents to sow doubt and confusion, creating a dynamic and strategic gameplay environment', Score=3\n",
      "NewAgent_4: Strategy='The slightly mutated strategy will now not only adjust the level of randomness based on the opponent's past actions and previous outcomes, but it will also incorporate a deeper analysis of the opponent's communication style and strategic tendencies. The agent will pay close attention to the language used by the opponent and their overall approach to the game, using this information to make more informed decisions. By honing in on these additional factors, the agent aims to gain a more comprehensive understanding of the opponent's mindset and leverage this insight to', Score=3\n",
      "NewAgent_0: Strategy='The slightly mutated strategy will now not only adjust the level of randomness based on the opponent's past actions and previous outcomes, but it will also introduce a predictive element. The agent will analyze patterns in the opponent's behavior and make educated guesses about their future moves based on this analysis. By incorporating this predictive element, the agent aims to stay one step ahead of the opponent and increase its chances of success in the game.', Score=0\n",
      "NewAgent_3: Strategy='The slightly mutated strategy will now not only adjust the level of randomness based on the opponent's past actions and previous outcomes, but it will also incorporate a more nuanced understanding of the opponent's psychology. The agent will not only analyze patterns in the opponent's behavior but will also consider their potential emotional state and decision-making biases. By taking into account these additional factors, the agent aims to further refine its predictions and outmaneuver the opponent in the game.', Score=0\n",
      "NewAgent_0: Strategy='The agent's further mutated strategy now includes incorporating a element of forgiveness towards opponents who have previously betrayed, giving them a chance to redeem themselves through cooperation. This shift in approach aims to create a more forgiving and flexible gameplay dynamic, allowing for potential reconciliation and long-term alliances with previously untrustworthy opponents. Additionally, the agent will begin to strategically alternate between periods of consistent cooperation and calculated betrayal, keeping opponents on their toes and maintaining a sense of unpredictability in its gameplay tactics.', Score=0\n",
      "NewAgent_3: Strategy='The agent's further mutated strategy now includes a random element that introduces occasional unpredictable betrayals regardless of the opponent's actions. This element of randomness adds an element of surprise and keeps opponents on their toes, making it harder for them to predict the agent's next move. Additionally, the agent will prioritize forming alliances with both unpredictable and predictable agents, but will now also consider forming double-cross alliances with certain opponents to confuse and outwit them. This strategy aims to keep opponents guessing and maintain a competitive edge in', Score=0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulation results saved to: simulation_results/run_2025-03-16_13-32-10\n"
     ]
    }
   ],
   "source": [
    "# Additional Cell: Enhanced Logging, Saving, and Visualization in subfolder\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "# Create main results folder\n",
    "results_folder = \"simulation_results\"\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "# Create a timestamped subfolder for this specific run\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "run_folder = os.path.join(results_folder, f\"run_{current_time}\")\n",
    "os.makedirs(run_folder, exist_ok=True)\n",
    "\n",
    "# Logging structures\n",
    "detailed_log = []\n",
    "generation_summary = []\n",
    "\n",
    "# Detailed Logging Function\n",
    "def log_round(gen, round_num, donor, recipient, action_donor, action_recipient, payoff_donor, payoff_recipient, reasoning_donor):\n",
    "    detailed_log.append({\n",
    "        \"Generation\": gen,\n",
    "        \"Round\": round_num,\n",
    "        \"Donor\": donor.name,\n",
    "        \"Recipient\": recipient.name,\n",
    "        \"Donor_Strategy\": donor.strategy_text,\n",
    "        \"Recipient_Strategy\": recipient.strategy_text,\n",
    "        \"Donor_Action\": action_donor,\n",
    "        \"Recipient_Action\": action_recipient,\n",
    "        \"Donor_Payoff\": payoff_donor,\n",
    "        \"Recipient_Payoff\": payoff_recipient,\n",
    "        \"Donor_Total_Score\": donor.total_score,\n",
    "        \"Recipient_Total_Score\": recipient.total_score,\n",
    "        \"Donor_Reasoning\": reasoning_donor\n",
    "    })\n",
    "\n",
    "# Generation Summary Logging Function\n",
    "def log_generation_summary(gen, agents):\n",
    "    avg_resources = sum(a.total_score for a in agents) / len(agents)\n",
    "    strategies = [a.strategy_text for a in agents]\n",
    "    generation_summary.append({\n",
    "        \"Generation\": gen,\n",
    "        \"Average_Resources\": avg_resources,\n",
    "        \"Agent_Strategies\": strategies\n",
    "    })\n",
    "\n",
    "# Save Logs to CSV and JSON (in run_folder)\n",
    "def save_logs():\n",
    "    # Save run parameters\n",
    "    params = {\n",
    "        \"NUM_AGENTS\": NUM_AGENTS,\n",
    "        \"NUM_GENERATIONS\": NUM_GENERATIONS,\n",
    "        \"SURVIVAL_RATE\": SURVIVAL_RATE,\n",
    "        \"PD_PAYOFFS\": {\n",
    "            \"R\": R,\n",
    "            \"T\": T,\n",
    "            \"S\": S,\n",
    "            \"P\": P\n",
    "        },\n",
    "        \"timestamp\": current_time\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(run_folder, \"parameters.json\"), 'w') as f:\n",
    "        json.dump(params, f, indent=4)\n",
    "    \n",
    "    # Save detailed logs\n",
    "    detailed_df = pd.DataFrame(detailed_log)\n",
    "    detailed_df.to_csv(os.path.join(run_folder, \"detailed_logs.csv\"), index=False)\n",
    "    detailed_df.to_json(os.path.join(run_folder, \"detailed_logs.json\"), orient=\"records\", indent=4)\n",
    "\n",
    "    # Save generation summary\n",
    "    summary_df = pd.DataFrame(generation_summary)\n",
    "    summary_df.to_csv(os.path.join(run_folder, \"generation_summary.csv\"), index=False)\n",
    "    summary_df.to_json(os.path.join(run_folder, \"generation_summary.json\"), orient=\"records\", indent=4)\n",
    "\n",
    "# Evaluation Metric Plotting\n",
    "def plot_cooperation_over_time():\n",
    "    generations = [entry[\"Generation\"] for entry in generation_summary]\n",
    "    avg_resources = [entry[\"Average_Resources\"] for entry in generation_summary]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(generations, avg_resources, marker='o', linestyle='-', linewidth=2, color='blue')\n",
    "    plt.title(\"Average Cooperation Score over Generations\")\n",
    "    plt.xlabel(\"Generation\")\n",
    "    plt.ylabel(\"Average Final Resources\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(run_folder, \"cooperation_over_generations.png\"))\n",
    "    plt.show()\n",
    "\n",
    "# Main Simulation function (with logging)\n",
    "def run_simulation_with_logging(num_agents=NUM_AGENTS, num_generations=NUM_GENERATIONS):\n",
    "    detailed_log.clear()\n",
    "    generation_summary.clear()\n",
    "\n",
    "    agents = create_initial_agents(num_agents)\n",
    "\n",
    "    for gen in range(num_generations):\n",
    "        print(f\"\\n=== Generation {gen+1} ===\")\n",
    "\n",
    "        round_num = 1\n",
    "        random.shuffle(agents)\n",
    "        for i in range(0, len(agents), 2):\n",
    "            if i + 1 < len(agents):\n",
    "                donor, recipient = agents[i], agents[i+1]\n",
    "                donor_action = donor.decide_action(recipient)\n",
    "                recipient_action = recipient.decide_action(donor)\n",
    "                payoff_donor, payoff_recipient = play_pd(donor_action, recipient_action)\n",
    "                donor.total_score += payoff_donor\n",
    "                recipient.total_score += payoff_recipient\n",
    "\n",
    "                # Simulated detailed reasoning from LLM\n",
    "                reasoning_donor = f\"Following strategy: '{donor.strategy_text}', I chose action: '{donor_action}'\"\n",
    "                \n",
    "                log_round(gen+1, round_num, donor, recipient, donor_action, recipient_action,\n",
    "                          payoff_donor, payoff_recipient, reasoning_donor)\n",
    "                round_num += 1\n",
    "\n",
    "        # Log summary at generation end\n",
    "        log_generation_summary(gen+1, agents)\n",
    "\n",
    "        # Interim generation results\n",
    "        agents.sort(key=lambda x: x.total_score, reverse=True)\n",
    "        for a in agents:\n",
    "            print(f\"{a.name}: Strategy='{a.strategy_text}', Score={a.total_score}\")\n",
    "\n",
    "        # Proceed with selection and reproduction\n",
    "        agents = selection_and_reproduction(agents)\n",
    "\n",
    "    # Save logs and produce visualizations\n",
    "    save_logs()\n",
    "    plot_cooperation_over_time()\n",
    "    \n",
    "    # Print location of saved results\n",
    "    print(f\"\\nSimulation results saved to: {run_folder}\")\n",
    "\n",
    "# Execute the new extended logging simulation\n",
    "run_simulation_with_logging()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
